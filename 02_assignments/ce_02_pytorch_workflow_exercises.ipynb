{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c516556-8e84-4583-acb9-fbcf2cafeeeb",
   "metadata": {
    "id": "N8LsPXZti9Sw",
    "tags": []
   },
   "source": [
    "<h1><center>Laboratory work 2.</center></h1>\n",
    "<h2><center>PyTorch Workflow Exercise</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97ff57-7b9a-4675-9b32-453f56a4dbd4",
   "metadata": {},
   "source": [
    "**Completed:** Last name and First name\n",
    "\n",
    "**Variant:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b78c9c-bf57-4d9d-81b9-0737e07a8e32",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22795d29-bb8e-4394-adc6-e7f1a489a2d5",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Create a straight line dataset](#2.1)\n",
    "2. [Task 2. Build a PyTorch model](#2.2)\n",
    "3. [Task 3. Create a loss function and an optimizer](#2.3)\n",
    "4. [Task 4. Make predictions with the trained model on the test data](#2.4)\n",
    "5. [Task 5. Save your trained model's state_dict() to file](#2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a51b13-ad99-4116-aff2-4c906cbbb2b7",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a2de1-a295-45b9-866e-edb70f8d2c49",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 1. Create a straight line dataset.</span>\n",
    "\n",
    "[Go back to the content](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e50302-6600-4d01-a288-a93647305c84",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Variant 1:** Develop a dataset based on the function \\(y = 0.25x + 2.0\\) where \\(x\\) spans from -50 to 50 in integer increments. Split the dataset into 80% training and 20% testing. Plot both sets using Matplotlib, ensuring you visually distinguish between training and testing points. This approach trains you to handle data creation and partitioning, a crucial aspect of step 1 in the PyTorch workflow.\n",
    "\n",
    "*Technical note:* Use `torch.linspace` for generating x-values, then derive y-values. Employ `matplotlib.pyplot.scatter` to plot the training and testing splits. Always set a random seed with `torch.manual_seed` for consistent results.\n",
    "\n",
    "---\n",
    "**Variant 2:** Generate a linear dataset described by \\(y = 1.5x - 0.3\\) with 300 data points in total. Randomly shuffle the entire dataset, then allocate 75% of it as training data and 25% as validation data. Plot both subsets to confirm your split. This process reinforces data randomness and reproducibility for better model generalization.\n",
    "\n",
    "*Technical note:* Use `torch.randperm` to shuffle the indices, `torch.split` or manual slicing for partitioning, and `matplotlib.pyplot.scatter` for visualization. Implement the standard PyTorch seed setup.\n",
    "\n",
    "---\n",
    "**Variant 3:** Create a noisy linear dataset where \\(y = 0.4x + 0.8 + \\epsilon\\). Let \\(\\epsilon\\) be normally distributed noise with mean 0 and standard deviation 0.2. Generate 250 points for x in the range -10 to 10. Assign 70% of the data to training and 30% to testing, ensuring a balanced distribution across x-values.\n",
    "\n",
    "*Technical note:* Use `torch.normal` with mean=0, std=0.2 to add noise. Employ the typical slicing approach to form the training and testing sets. Visualize with Matplotlib to observe the noise effect.\n",
    "\n",
    "---\n",
    "**Variant 4:** Construct a dataset derived from \\(y = -0.6x + 3.5\\), but introduce uniform noise in the range \\([-0.3, 0.3]\\). Generate 180 points, ensuring you choose a random range for x-values to reflect real-world conditions. Maintain 65% for training and 35% for testing, then visualize them on a scatter plot.\n",
    "\n",
    "*Technical note:* Use `torch.rand` or `torch.distributions.uniform.Uniform` to create noise. Keep your data in a `torch.FloatTensor`. Plot results with `matplotlib.pyplot.scatter` and set a manual seed.\n",
    "\n",
    "---\n",
    "**Variant 5:** Formulate a piecewise linear dataset in two segments: \\(y = 0.3x + 0.5\\) for \\(x \\leq 10\\), and \\(y = 0.05x + 2.0\\) for \\(x > 10\\). Generate 220 data points spanning x from 0 to 20, then randomly separate 70% as training and 30% as test data. Such piecewise definitions help explore discontinuities and transitions in data.\n",
    "\n",
    "*Technical note:* Use simple conditional checks to generate y-values. Convert arrays to PyTorch tensors for uniform processing. Visualize the break in the line on a scatter plot.\n",
    "\n",
    "---\n",
    "**Variant 6:** Generate a linear dataset with moderate sine-wave perturbations: \\(y = 0.7x + 0.9 + 0.2\\sin(x)\\). Let x range from -5 to 5, and sample 200 points. Partition the data 80%/20% and observe how sinusoidal fluctuations challenge a purely linear model. This introduces a scenario where linear regression may not perfectly fit the data.\n",
    "\n",
    "*Technical note:* Use `torch.sin` for the perturbation, combined with a linear function. Maintain reproducibility with `torch.manual_seed`. Scatter-plot training vs. testing for clarity.\n",
    "\n",
    "---\n",
    "**Variant 7:** Construct a dataset with the formula \\(y = 0.3x + 1.0\\) for x in \\([0, 100]\\). Then apply random integer offsets to x for each data point (within ±5). Aim for 250 data points, dividing them evenly for training and testing. Observe how this offset simulates mild measurement error or sampling shifts.\n",
    "\n",
    "*Technical note:* Generate integer offsets with `torch.randint`, add them to your base x-values, then compute y-values. Use standard indexing to build separate training and testing subsets.\n",
    "\n",
    "---\n",
    "**Variant 8:** Create two linear segments from one function: for the first 100 points, use \\(y = 0.8x + 0.2\\); for the next 100 points, use \\(y = -0.4x + 5.0\\). Merge these into a single dataset of 200 points. Split them 70/30 and plot them as a unified dataset with distinct color coding for each segment.\n",
    "\n",
    "*Technical note:* Manipulate x in separate ranges (e.g., 0–50 and 51–100). Convert results into PyTorch tensors. Display the overall distribution using `matplotlib.pyplot.scatter`.\n",
    "\n",
    "---\n",
    "**Variant 9:** Construct a noisy dataset based on \\(y = 0.2x + 1.0\\), but incorporate small-scale Gaussian noise that changes sign every 20 data points. Generate 240 total samples, with 180 for training and 60 for testing. This variant forces you to consider distribution shifts and the effect of structured noise patterns.\n",
    "\n",
    "*Technical note:* Use a loop to apply noise with flipped signs for each block of 20. Ensure you maintain the usual PyTorch seed. Scatter-plot subsets for final inspection.\n",
    "\n",
    "---\n",
    "**Variant 10:** Design a dataset by selecting x-values from a normal distribution centered around 50, standard deviation 15. Use \\(y = 0.05x - 2.0\\) and ensure 200 total points. Then randomly choose 70% of points as training samples and 30% as test samples. This variant introduces a more realistic, non-uniform distribution of x.\n",
    "\n",
    "*Technical note:* Use `torch.normal(mean=50, std=15, size=(200,))` to sample x, then compute y. Check outliers carefully. Plot data distribution to confirm the normal-like shape.\n",
    "\n",
    "---\n",
    "**Variant 11:** Generate a dataset for \\(y = -0.7x + 2.5\\) with x-values in the range \\(-10\\) to \\(10\\). Introduce outliers by adding a large random offset (±5) to 10 randomly chosen samples. Split the data into train/test, with 160 training points and 40 testing points, allowing you to see how outliers can impact linear modeling.\n",
    "\n",
    "*Technical note:* Use random indexing to pick outlier positions. Add an offset using `torch.randint` or a normal distribution with a higher std. Keep everything in float tensors.\n",
    "\n",
    "---\n",
    "**Variant 12:** Create a dataset from \\(y = 0.9x - 1.2\\), but let x-values come from a uniform distribution spanning -50 to 0 for half the points, and 0 to 50 for the other half. Generate 300 points total, dividing 75%/25% for training and testing. Compare the final scatter plot to see differences in negative and positive x-ranges.\n",
    "\n",
    "*Technical note:* Generate two halves with `torch.rand`, scale them accordingly, and combine. Use standard slicing for train/test splits. Visualize to confirm coverage of negative/positive domains.\n",
    "\n",
    "---\n",
    "**Variant 13:** Model a dataset around \\(y = 0.3x + 2.0\\) with x-values purely from even integers between 0 and 100. Add small Gaussian noise to y. Keep 250 points total, randomly selecting 70% as training and 30% as validation. This variant helps you practice data creation with specific integer constraints.\n",
    "\n",
    "*Technical note:* Use `torch.arange(0,101,2)` for even integers. Expand it to produce 250 samples (repetition or re-sampling). Add noise with `torch.normal`. Visualize subsets with a scatter plot.\n",
    "\n",
    "---\n",
    "**Variant 14:** Build a dataset with the function \\(y = -0.3x + 0.8\\), ensuring each x is an integer from 0 to 200 but only prime numbers. This yields a sparser set of x-values. Add slight random noise and split 80%/20%. This exercise highlights dataset construction with domain-specific filtering.\n",
    "\n",
    "*Technical note:* Generate a list of primes within 0–200, convert to a tensor, add noise, then create y. To get prime numbers, you could write a simple checking function or utilize known prime lists.\n",
    "\n",
    "---\n",
    "**Variant 15:** Form a synthetic linear dataset for \\(y = 2.0 - 0.5x\\) for x from 0 to 10 in steps of 0.1, leading to 100 points. Duplicate this pattern five times with distinct random seeds to form 500 total points. Partition 70/30 for training/testing. This variant focuses on replicability with multiple seeds.\n",
    "\n",
    "*Technical note:* Create x using `torch.arange(0,10.1,0.1)`, replicate data five times, and add slight random noise for variation each iteration. Use a combined scatter plot to confirm the dataset structure.\n",
    "\n",
    "---\n",
    "**Variant 16:** Craft a dataset with a near-vertical slope, \\(y = 10.0x + 0.1\\), spanning x from -1 to 1 with 200 points. This steep slope allows you to visualize training difficulties that can arise when data have large gradients. Keep 70% as training and 30% as testing.\n",
    "\n",
    "*Technical note:* Use `torch.linspace(-1, 1, steps=200)` for x, generate y accordingly, and store them as float tensors. Inspect the scattered distribution to see the vertical nature.\n",
    "\n",
    "---\n",
    "**Variant 17:** Generate a linear dataset across three distinct x-ranges: \\([-10, -5]\\), \\([0, 5]\\), and \\([10, 15]\\), all using the same equation \\(y = 0.2x + 1.0\\). Integrate a uniform noise offset in y. Produce 90 points total, dividing 60/30 for training/testing. Notice the data’s “gapped” distribution on the x-axis.\n",
    "\n",
    "*Technical note:* Create separate segments, each with `torch.linspace`. Combine them and add noise. Visualize to confirm the gaps between segments.\n",
    "\n",
    "---\n",
    "**Variant 18:** Combine two linear trends: for half the points, \\(y = 0.6x + 0.1\\); for the other half, \\(y = -0.6x - 2.0\\). Shuffle them thoroughly to avoid ordering biases, then do a 70/30 split. This scenario tests the model’s ability to handle data with opposing slopes.\n",
    "\n",
    "*Technical note:* Generate each half separately, combine into a single tensor, then shuffle with `torch.randperm`. Use indexing for train/test splits and plot to examine the merged distribution.\n",
    "\n",
    "---\n",
    "**Variant 19:** Construct a dataset from \\(y = 0.05x + 0.2\\), with x-values randomly chosen integers between 0 and 1000, but only keep 200 unique samples. Add a small random offset to y. Shuffle, then split 75/25. The large integer range simulates scenarios with widely spread feature values.\n",
    "\n",
    "*Technical note:* Use `torch.randint` for x, ensure uniqueness by converting to a Python set or by sorting and removing duplicates. Convert the final array to PyTorch tensors and visualize results.\n",
    "\n",
    "---\n",
    "**Variant 20:** Generate a linear dataset where \\(y = -0.1x + 3.0\\), but cluster your x-values into two dense groups, one around 10–15 and another around 90–95. Maintain a total of 300 data points. Observe the model’s potential extrapolation challenges outside these clusters. Split 70%/30% and plot them carefully.\n",
    "\n",
    "*Technical note:* Sample each cluster with `torch.normal` around means 12 and 92, respectively. Apply the linear function, add minimal noise, and ensure the final shape is consistent. Visualization is key to noticing clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26265fc2-d57f-42c4-ba89-84b9d42b8797",
   "metadata": {
    "id": "g7HUhxCxjeBx"
   },
   "source": [
    "Your output of the below cell should look something like:\n",
    "```\n",
    "Number of X samples: 100\n",
    "Number of y samples: 100\n",
    "First 10 X & y samples:\n",
    "X: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
    "        0.0900])\n",
    "y: tensor([0.9000, 0.9030, 0.9060, 0.9090, 0.9120, 0.9150, 0.9180, 0.9210, 0.9240,\n",
    "        0.9270])\n",
    "```\n",
    "\n",
    "Of course the numbers in `X` and `y` may be different, but ideally they're created using the linear regression formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ece9c",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the data parameters\n",
    "\n",
    "\n",
    "# Make X and y using linear regression feature\n",
    "\n",
    "\n",
    "print(f\"Number of X samples: {len(X)}\")\n",
    "print(f\"Number of y samples: {len(y)}\")\n",
    "print(f\"First 10 X & y samples:\\nX: {X[:10]}\\ny: {y[:10]}\")\n",
    "\n",
    "# Split the data into training and testing\n",
    "\n",
    "# Plot the training and testing data \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f6c73-d000-42b2-a1c8-4a9d7d8c933d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71c8e4-03f0-4c88-bb6d-353aeca84100",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 2. Build a PyTorch model.</span>\n",
    "\n",
    "[Go back to the content](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e6c67-9f89-4dc4-a639-bd3a70332a23",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:** Construct a PyTorch model by subclassing `nn.Module` to handle multiple inputs: let it accept two input features and learn a single linear output. Initialize `weights` and `bias` using `nn.Parameter()` and demonstrate how the `forward()` method sums contributions from both features. Print out the model’s `state_dict()` to confirm initialization.\n",
    "\n",
    "*Technical note:* Use `requires_grad=True` for each parameter. Incorporate the standard `super().__init__()` approach within the custom module. Check your final model parameters using `model.state_dict()`.\n",
    "\n",
    "---\n",
    "**Variant 2:** Build a two-layer fully connected network. The first layer has 8 neurons, followed by a ReLU, then a final output layer with 1 neuron. Subclass `nn.Module` and define these layers manually. Present the `forward()` pass to transform inputs sequentially. Conclude by printing `model.state_dict()`.\n",
    "\n",
    "*Technical note:* Use `nn.Linear(…); nn.ReLU()` in `__init__()`. In `forward()`, pass data through both layers. This approach aligns with “2. Building a model” from the workflow notes.\n",
    "\n",
    "---\n",
    "**Variant 3:** Implement a small neural network that includes an activation function other than ReLU, such as `nn.Tanh()`. The model should contain one hidden layer with 4 units, followed by a single-output layer. Your `forward()` method demonstrates how data flows from input → hidden → Tanh → output. Print parameter shapes.\n",
    "\n",
    "*Technical note:* Use `nn.Linear` for both layers, place `nn.Tanh` in the forward pass, and wrap everything under a custom `nn.Module`. Inspect parameter shapes with `[p.shape for p in model.parameters()]`.\n",
    "\n",
    "---\n",
    "**Variant 4:** Create a model that includes a dropout layer to demonstrate how you might regularize during training. The model has two linear layers separated by `nn.Dropout(0.2)`. Show how the dropout probability can be toggled using `model.train()` vs. `model.eval()`. Finally, print the dropout’s parameter setting.\n",
    "\n",
    "*Technical note:* Subclass `nn.Module`, define `self.dropout = nn.Dropout(0.2)` in `__init__()`. In the forward method, pass the input through the first layer, apply dropout, then pass through the second layer.\n",
    "\n",
    "---\n",
    "**Variant 5:** Build a purely linear model that includes learnable bias and weight but in non-traditional shapes—for instance, a matrix shaped `(1, 3)` for the weights if you’re mapping from 3 input features to a single output. Show how reshaping can be done in the forward pass to get the correct result.\n",
    "\n",
    "*Technical note:* Use `nn.Parameter` for the weight matrix `(1, 3)` and `nn.Parameter` for bias `(1,)`. In `forward()`, take an input of shape `(N, 3)` and multiply appropriately, ensuring dimension alignment.\n",
    "\n",
    "---\n",
    "**Variant 6:** Construct a model that implements a piecewise linear function internally. For input x < 0, apply a certain linear transformation; for x ≥ 0, apply another. Use `torch.where` or Python conditionals in the `forward()` method to decide which transformation to apply. Print out your customized model’s state.\n",
    "\n",
    "*Technical note:* You might define separate parameters for negative and positive regimes, e.g., `(w_neg, b_neg)` and `(w_pos, b_pos)`. Demonstrate conditional logic in `forward()` carefully.\n",
    "\n",
    "---\n",
    "**Variant 7:** Build an initialization scheme directly inside the model’s `__init__()`, using `torch.nn.init.xavier_uniform_` for one layer and `torch.nn.init.kaiming_normal_` for another. Then confirm in the forward pass that the dimensionalities match your expectations. Print the mean and variance of each initialized parameter to highlight their different distributions.\n",
    "\n",
    "*Technical note:* Use `nn.Linear` to define layers. After creation, apply something like `torch.nn.init.xavier_uniform_(layer1.weight)` or `torch.nn.init.kaiming_normal_(layer2.weight)`.\n",
    "\n",
    "---\n",
    "**Variant 8:** Implement a class-based PyTorch model that returns multiple outputs, for example, a main regression output plus an auxiliary classification score. Let the hidden layer feed into two different output layers. Print the final dictionary or tuple of outputs. This clarifies how to handle multi-task learning in a single model.\n",
    "\n",
    "*Technical note:* Subclass `nn.Module` and define `self.regressor`, `self.classifier` as separate `nn.Linear` layers. The forward pass returns two distinct outputs, e.g., `return {\"reg_out\": reg, \"class_out\": class_score}`.\n",
    "\n",
    "---\n",
    "**Variant 9:** Construct a custom model that includes a learnable scaling parameter on the input data before it even reaches the hidden layers. This approach can reflect data normalization within the model. Demonstrate how an `nn.Parameter` for `scale` can be applied element-wise in `forward()`. Print out this additional parameter in the state dictionary.\n",
    "\n",
    "*Technical note:* Use `scale = nn.Parameter(torch.ones(1))` in `__init__()`. In `forward(x)`, do `x = x * scale` before passing to subsequent layers.\n",
    "\n",
    "---\n",
    "**Variant 10:** Create a model that explicitly replicates a standard linear regression with a single weight and bias. However, store these as part of a dictionary in `__init__()`, not standard `nn.Parameter` attributes. Show how you can still register them as parameters with `self.register_parameter()`. Confirm with `state_dict()` that they’re recognized.\n",
    "\n",
    "*Technical note:* Use something like:\n",
    "```python\n",
    "self.w = nn.Parameter(torch.randn(1))\n",
    "self.register_parameter(\"bias\", nn.Parameter(torch.zeros(1)))\n",
    "```\n",
    "The forward pass simply does `(x * self.w) + self.bias`.\n",
    "\n",
    "---\n",
    "**Variant 11:** Develop a two-layer neural network that expects 1D input data. In the forward pass, reshape the input to 2D if necessary (e.g., from `(batch_size,)` to `(batch_size, 1)`). The hidden layer can have 8 neurons, followed by an output layer with 1 neuron. Print shapes in the forward pass to verify correctness.\n",
    "\n",
    "*Technical note:* Use `x = x.unsqueeze(1)` or `x.view(-1, 1)` if x is 1D. Then pass through `self.hidden` and `self.output`. Checking shapes with `print(x.shape)` helps ensure no dimension mismatches.\n",
    "\n",
    "---\n",
    "**Variant 12:** Build a model that leverages `nn.Sequential` to stack layers (Linear → ReLU → Linear). Demonstrate how you can quickly define forward behavior without writing a custom `forward()`. Print the final architecture and confirm the parameter count.\n",
    "\n",
    "*Technical note:* Use something like:\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.Linear(in_features=4, out_features=8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=8, out_features=1)\n",
    ")\n",
    "```\n",
    "Calling `print(self.net)` reveals the stacked layout.\n",
    "\n",
    "---\n",
    "**Variant 13:** Create a residual-style model that adds the input directly to the output of a hidden layer: \\( \\text{out} = x + \\text{ReLU}(\\text{Linear}(x)) \\). Subclass `nn.Module`, define a linear layer, and then demonstrate your skip connection in `forward()`. This is a mini-introduction to residual networks.\n",
    "\n",
    "*Technical note:* Something like:\n",
    "```python\n",
    "out = self.linear(x)\n",
    "out = torch.relu(out)\n",
    "return x + out\n",
    "```\n",
    "Check that x has matching dimensions to the linear output. Print `state_dict()` to confirm.\n",
    "\n",
    "---\n",
    "**Variant 14:** Implement a small network that uses parameter slicing. For instance, define a big parameter vector in `__init__()`, then slice it into distinct segments for different computations in `forward()`. Show that each segment can serve a different linear transformation of the input.\n",
    "\n",
    "*Technical note:* Use a single `nn.Parameter(torch.randn(10))`. In `forward()`, treat `[0:5]` as weights for a first input subset and `[5:]` as weights for a second. Confirm the concept with printed shapes.\n",
    "\n",
    "---\n",
    "**Variant 15:** Build a model that uses `nn.Embedding` for an integer input dimension (e.g., 50) and a small embedding size (e.g., 8). Then pass the embedded output into a linear layer that performs a final regression or classification. Print the embedding layer’s parameters and demonstrate how they map integer indices to embeddings.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "self.embed = nn.Embedding(num_embeddings=50, embedding_dim=8)\n",
    "self.fc = nn.Linear(8, 1)\n",
    "```\n",
    "The forward pass calls `embedding_output = self.embed(x)`, then `self.fc(embedding_output)`.\n",
    "\n",
    "---\n",
    "**Variant 16:** Construct a dynamic model in which the number of hidden neurons changes based on an argument passed to the constructor. In `__init__()`, read an integer `hidden_units` parameter and create the linear layer accordingly. Print the architecture to confirm how the shape depends on this user-defined parameter.\n",
    "\n",
    "*Technical note:* Something like:\n",
    "```python\n",
    "def __init__(self, hidden_units=16):\n",
    "    super().__init__()\n",
    "    self.layer = nn.Linear(10, hidden_units)\n",
    "    self.out = nn.Linear(hidden_units, 1)\n",
    "```\n",
    "Show it off by instantiating the model with various hidden_units values.\n",
    "\n",
    "---\n",
    "**Variant 17:** Implement a multi-branch model that processes the input in parallel: for instance, run x through two different `nn.Linear` layers, combine them by addition, and produce the final output. Print the shapes of each branch to illustrate how they merge. This variant helps you explore parallel feed-forward paths in a single network.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "branch1 = self.lin1(x)\n",
    "branch2 = self.lin2(x)\n",
    "combined = branch1 + branch2\n",
    "```\n",
    "Keep in mind dimension consistency. Use `nn.ModuleList` if you prefer dynamic branching.\n",
    "\n",
    "---\n",
    "**Variant 18:** Create a more specialized model that includes a non-trainable parameter used strictly for inference logic. Mark it as a buffer using `self.register_buffer()`. In `forward()`, apply it as a scaling or offset factor. Print the model’s buffers separately from its parameters to demonstrate the difference.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "self.register_buffer(\"my_buffer\", torch.tensor([10.0]))\n",
    "```\n",
    "The buffer won’t appear in `.parameters()`, but you’ll see it in `.buffers()`. This helps illustrate the difference between learnable weights and fixed buffers.\n",
    "\n",
    "---\n",
    "**Variant 19:** Build a class-based PyTorch model that only uses `torch.nn.functional` style functions for activation (e.g., `F.relu`) instead of modules like `nn.ReLU()`. Subclass `nn.Module`, define linear layers, then call something like `x = F.relu(self.fc1(x))` in `forward()`. Show that your model’s function is identical, even though you’re not instantiating layer modules for activation.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "import torch.nn.functional as F\n",
    "```\n",
    "Then in forward:\n",
    "```python\n",
    "x = F.relu(self.fc1(x))\n",
    "x = self.fc2(x)\n",
    "```\n",
    "Parameters are only for `fc1` and `fc2` in this approach.\n",
    "\n",
    "---\n",
    "**Variant 20:** Implement a modular design: define a `LinearBlock(nn.Module)` that holds a linear layer and a chosen activation, then build a main model that stacks multiple `LinearBlock`s. Print out how each block’s parameters appear in the main model’s `state_dict()`. This clarifies how nested modules operate.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, in_f, out_f):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_f, out_f)\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "```\n",
    "Compose them in another module, e.g. `self.block1 = LinearBlock(...)`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fda18-963a-42c8-9b70-1c1bd84bca70",
   "metadata": {
    "id": "qzd__Y5rjtB8"
   },
   "outputs": [],
   "source": [
    "# Create PyTorch linear regression model by subclassing `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38180d31-31cf-46a1-8e93-a1b4e864f0cd",
   "metadata": {
    "id": "5LdcDnmOmyQ2"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and put it to the target device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fe741-0528-46ed-909d-c3acd72273ff",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf146975-5f31-462e-8186-9fee8e7e0dc6",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 3. Create a loss function and an optimizer.</span>\n",
    "\n",
    "[Go back to the content](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a7822-066d-4651-a132-a8b608c8e4bd",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:** Use `nn.MSELoss()` as the loss function combined with `torch.optim.SGD()` with momentum = 0.9. Train for 300 epochs, logging training loss every 25 epochs to track progress. Print final loss and compare the difference in training speed versus a model without momentum.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "```\n",
    "Call `optimizer.zero_grad()`, then `loss.backward()`, `optimizer.step()` in each epoch.\n",
    "\n",
    "---\n",
    "**Variant 2:** Choose `nn.L1Loss()` with an Adam optimizer (`torch.optim.Adam`) at a learning rate of 0.005. Train for 200 epochs and evaluate on your test set every 20 epochs. Notice how L1Loss changes the gradient behavior compared to MSELoss. Log training vs. validation curves.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "```\n",
    "This approach typically yields robust results against outliers.\n",
    "\n",
    "---\n",
    "**Variant 3:** Implement a custom loss function that penalizes errors more if they occur at larger x-values. For instance, define a function `loss = MSE((y_pred - y_true) * x)`. Combine it with RMSprop optimizer at a learning rate of 0.001. Show how to implement custom backward passes if needed.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "def weighted_mse_loss(y_pred, y_true, x):\n",
    "    return ((y_pred - y_true) * x).pow(2).mean()\n",
    "```\n",
    "In training, compute `loss = weighted_mse_loss(...)`, call `.backward()`, then `optimizer.step()`.\n",
    "\n",
    "---\n",
    "**Variant 4:** Adopt a Huber loss (`nn.SmoothL1Loss`) with an SGD optimizer that has a learning rate schedule. Use `torch.optim.lr_scheduler.StepLR` to reduce the learning rate by half every 50 epochs. Run 300 epochs in total. Observe how scheduled learning rates influence convergence.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "```\n",
    "Adjust the schedule each epoch via `scheduler.step()`.\n",
    "\n",
    "---\n",
    "**Variant 5:** Experiment with `nn.MSELoss(reduction='sum')` instead of `'mean'`. Pair this with `torch.optim.AdamW`, a variant of Adam that decouples weight decay. Train for 400 epochs, logging the sum-based loss. Compare how training dynamics differ from the usual mean-based approach.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "```\n",
    "Keep an eye on the scale of the loss.\n",
    "\n",
    "---\n",
    "**Variant 6:** Combine multiple losses: for example, 70% MSE and 30% L1. Let `loss = 0.7 * MSE + 0.3 * L1`. Use an Adagrad optimizer with learning rate 0.01 for 200 epochs. Log training loss to see how the composite objective stabilizes. This variant demonstrates multi-objective blending.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "mse = F.mse_loss(y_pred, y_true)\n",
    "l1 = F.l1_loss(y_pred, y_true)\n",
    "loss = 0.7 * mse + 0.3 * l1\n",
    "```\n",
    "Use `torch.optim.Adagrad` with a given lr. Keep track of partial losses separately.\n",
    "\n",
    "---\n",
    "**Variant 7:** Explore the effect of weight decay (L2 regularization) by setting a moderate decay value (e.g., 0.001) in `torch.optim.SGD`. Use MSE loss for 250 epochs. Evaluate model performance on both training and validation sets, discussing how overfitting might be mitigated.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "```\n",
    "Observing the difference in training vs. validation error after 250 epochs highlights regularization’s effect.\n",
    "\n",
    "---\n",
    "**Variant 8:** Introduce cyclical learning rates with MSE loss. Use `torch.optim.SGD` and `torch.optim.lr_scheduler.CyclicLR`. Train for 200 epochs, letting the learning rate oscillate between 0.001 and 0.01. Compare the final performance to a static learning rate approach.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, base_lr=0.001, max_lr=0.01, step_size_up=100\n",
    ")\n",
    "```\n",
    "Invoke `scheduler.step()` in each epoch. This approach can sometimes accelerate convergence.\n",
    "\n",
    "---\n",
    "**Variant 9:** Implement a negative log-likelihood loss (`nn.NLLLoss`) after applying a `LogSoftmax` output layer for classification tasks. Even though your dataset might be regression-based, you can adapt it to classification by discretizing outputs. Use Adam optimizer and train for 150 epochs. Evaluate accuracy each epoch.\n",
    "\n",
    "*Technical note:* Convert real y-values into discrete classes, e.g., rounding or binning. Then pass final logits through `F.log_softmax`, compute `nn.NLLLoss`. Update parameters with Adam.\n",
    "\n",
    "---\n",
    "**Variant 10:** Explore the usage of `nn.CrossEntropyLoss` for a multi-class linear problem with random synthetic data. The model has an output dimension > 1. Train with RMSprop for 100 epochs, tracking classification accuracy. This variant highlights a classification scenario within the typical PyTorch workflow.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "```\n",
    "Ensure your model’s output dimension matches the number of classes.\n",
    "\n",
    "---\n",
    "**Variant 11:** Create a small dataset for binary classification, then use `nn.BCEWithLogitsLoss()`, which combines a Sigmoid layer with BCE. Try an SGD optimizer at lr=0.02. Run 200 epochs and record the training AUC (area under ROC curve) every 20 epochs to see how well the model separates the two classes.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "```\n",
    "Calculate AUC with scikit-learn functions or a custom routine. Remember to apply a sigmoid to predictions for thresholding.\n",
    "\n",
    "---\n",
    "**Variant 12:** Develop a multi-target regression scenario using `nn.MSELoss()`, but with a 2D output from your model. Use Adam optimizer at lr=0.001 for 300 epochs, comparing each dimension’s error separately. This variant reveals the difference between single-output vs. multi-output training.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "y_pred = model(x)  # shape [batch_size, 2]\n",
    "criterion = nn.MSELoss()\n",
    "```\n",
    "Log dimension-wise error if needed, i.e., MSE on each output.\n",
    "\n",
    "---\n",
    "**Variant 13:** Implement an Adam optimizer with gradient clipping at a maximum norm of 1.0. Use `nn.SmoothL1Loss`. After each backward pass, apply `torch.nn.utils.clip_grad_norm_`. Run for 250 epochs to see how gradient clipping stabilizes training if the model or data cause large updates.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "loss.backward()\n",
    "nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "```\n",
    "This technique is often used in RNN or large models to prevent exploding gradients.\n",
    "\n",
    "---\n",
    "**Variant 14:** Employ a combination of `nn.L1Loss` and a custom penalty on the sum of absolute weights (not just for model parameters, but also for intermediate variables if desired). Train with Adagrad for 300 epochs. Illustrate how adding parameter penalties in the total loss can shape the final model weights.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "param_abs_sum = sum(p.abs().sum() for p in model.parameters())\n",
    "total_loss = l1_loss + 0.001 * param_abs_sum\n",
    "```\n",
    "Demonstrate the concept of model-based regularization within the loss function.\n",
    "\n",
    "---\n",
    "**Variant 15:** Use `nn.CosineEmbeddingLoss` in a scenario where your model outputs vectors intended to be similar or dissimilar based on a label (1 or -1). This departure from typical regression/classification shows how different tasks can be approached with specialized loss functions. Train with Adam for 200 epochs.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.CosineEmbeddingLoss(margin=0.5)\n",
    "```\n",
    "You’ll need pairs of vectors and a “similar/dissimilar” label. This fosters an understanding of embedding-based training.\n",
    "\n",
    "---\n",
    "**Variant 16:** Utilize a polynomial-decay learning rate with `torch.optim.SGD` and an MSE loss. Set an initial lr=0.1, decaying to 0.001 by epoch 300. Implement your own polynomial schedule or adapt `LambdaLR`. Show how to code the polynomial schedule logic in the training loop.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "lambda_lr = lambda epoch: (1 - epoch/300)**2\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "```\n",
    "Check final lr after 300 epochs.\n",
    "\n",
    "---\n",
    "**Variant 17:** Conduct an experiment with the `ASGD` optimizer (Averaged SGD) and a mean absolute error (MAE) loss. Train for 500 epochs, logging how the averaged weights stabilize. Compare final performance to standard SGD. This variant highlights another lesser-used but interesting PyTorch optimizer.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.ASGD(model.parameters(), lr=0.01)\n",
    "```\n",
    "After training, evaluate the final loss/metric on test data.\n",
    "\n",
    "---\n",
    "**Variant 18:** Combine a hinge-loss-like objective for a simplified linear SVM approach in PyTorch. Implement the hinge loss manually, \\( \\text{loss} = \\max(0, 1 - y \\cdot \\hat{y}) \\). Use RMSprop at lr=0.01 for 200 epochs. Observe the classification boundary this yields.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "def hinge_loss(y_pred, y_true):\n",
    "    return torch.clamp(1 - y_pred * y_true, min=0).mean()\n",
    "```\n",
    "Use `loss.backward()`, `optimizer.step()` in your training loop. Keep an eye on signs of y_pred vs. y_true.\n",
    "\n",
    "---\n",
    "**Variant 19:** Perform a multi-loss objective combining classification and reconstruction tasks. For example, a classification cross-entropy plus an MSE autoencoder penalty in a single model. Use Adam at lr=0.001. This variant exemplifies multi-task learning with a combined loss for 300 epochs.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "loss = classification_loss + beta * reconstruction_loss\n",
    "```\n",
    "Where `classification_loss` might be `nn.CrossEntropyLoss()`, and `reconstruction_loss` could be MSE between an output reconstruction and input.\n",
    "\n",
    "---\n",
    "**Variant 20:** Leverage `nn.KLDivLoss` with a log-probability output (using `log_softmax`). Convert a dataset’s real values into probability distributions (via softmax) for a specialized training scenario. Pair it with `torch.optim.Adam` at lr=0.0005 for 200 epochs. This technique is often employed in knowledge distillation or distribution-based tasks.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "y_pred_log_prob = F.log_softmax(model(x), dim=1)\n",
    "loss = nn.KLDivLoss(reduction='batchmean')(y_pred_log_prob, y_true_prob)\n",
    "```\n",
    "Ensure `y_true_prob` is also in probability space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b40429-41e9-4dfd-8cc6-a17c45a66d65",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4577912-969b-4c5e-a695-a80eeac32fac",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 4. Make predictions with the trained model on the test data.</span>\n",
    "\n",
    "[Go back to the content](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018ad0a-5dea-42e3-a5dd-f40c0414c00f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Variant 1:** After training a regression model, generate predictions for the test set and visualize them against the ground-truth labels on a 2D scatter plot. Include a line plot of the ideal function for comparison. Comment on how well the predictions follow the line.\n",
    "\n",
    "*Technical note:* Use `model.eval()` to disable dropout or batch-norm updates. Inference code might be:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    preds = model(x_test)\n",
    "```\n",
    "Then `plt.plot(x_test, preds, 'r-')`.\n",
    "\n",
    "---\n",
    "**Variant 2:** Evaluate a classification model on the test data by computing a confusion matrix. Display it using `matplotlib.pyplot.imshow`. Provide a color bar and relevant axis labels. Discuss how you can interpret misclassifications from the matrix.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "predictions = model(x_test).argmax(dim=1)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "```\n",
    "Plot with `plt.imshow(cm, cmap='Blues')`.\n",
    "\n",
    "---\n",
    "**Variant 3:** Implement a function that calculates R-squared (coefficient of determination) for regression predictions on the test set. Plot predicted vs. actual points. Also highlight a perfect fit line in the same plot for reference.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "ss_res = ((y_test - preds)**2).sum()\n",
    "ss_tot = ((y_test - y_test.mean())**2).sum()\n",
    "r2 = 1 - ss_res/ss_tot\n",
    "```\n",
    "Visualize with `plt.scatter(y_test, preds)` and `plt.plot(..., ..., 'r--')` for y=x line.\n",
    "\n",
    "---\n",
    "**Variant 4:** For multi-output regression, compute the mean absolute error (MAE) per output dimension on the test set. Visualize each error dimension in a bar chart to see which output is hardest for the model. Provide commentary on potential reasons behind such differences.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "abs_errors = (preds - y_test).abs().mean(dim=0)\n",
    "```\n",
    "Use Matplotlib’s bar plot to display the values for each output dimension.\n",
    "\n",
    "---\n",
    "**Variant 5:** Obtain test predictions in a step-by-step manner: feed one sample at a time, store predictions, and measure the inference time per sample. Plot the distribution of inference times. This highlights real-world performance aspects beyond just accuracy.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for sample in x_test:\n",
    "        pred = model(sample.unsqueeze(0))\n",
    "        ...\n",
    "end_time = time.time()\n",
    "```\n",
    "Plot a histogram of sample-level inference times.\n",
    "\n",
    "---\n",
    "**Variant 6:** Implement a custom function to compute a “prediction error distribution” by grouping test samples by x-value or class label. Visualize how the average prediction error changes across these groups. Summarize insights for model improvement.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "error = preds - y_test\n",
    "grouped_error = {}\n",
    "for label in unique_labels:\n",
    "    grouped_error[label] = error[y_test_class == label].mean()\n",
    "```\n",
    "Plot with `plt.bar(grouped_error.keys(), grouped_error.values())`.\n",
    "\n",
    "---\n",
    "**Variant 7:** Introduce a threshold-based check for predictions. For instance, in a regression scenario, highlight the data points where `|pred - y_true| > 0.5`. Then visualize them distinctly on a scatter plot to see where the model struggles. This approach pinpoints outlier predictions.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "large_error_indices = (preds - y_test).abs() > 0.5\n",
    "```\n",
    "Plot them in a different color or marker shape for clarity.\n",
    "\n",
    "---\n",
    "**Variant 8:** Write a function that compares predicted results from two different models on the same test set. Plot side-by-side bar charts or line plots for each model’s predictions to see which model performs better. Summarize potential reasons for differences.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "preds_modelA = modelA(x_test)\n",
    "preds_modelB = modelB(x_test)\n",
    "```\n",
    "Use `plt.subplot` or separate figures to visualize comparisons.\n",
    "\n",
    "---\n",
    "**Variant 9:** Calculate percentile-based errors on the test set. For instance, compute the 50th and 90th percentile of absolute errors. Summarize these in a textual or visual manner to represent how performance might degrade under “worst-case” scenarios.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "absolute_errs = (preds - y_test).abs()\n",
    "p50 = absolute_errs.kthvalue(int(0.5 * len(absolute_errs)))[0]\n",
    "p90 = absolute_errs.kthvalue(int(0.9 * len(absolute_errs)))[0]\n",
    "```\n",
    "Interpret these values carefully.\n",
    "\n",
    "---\n",
    "**Variant 10:** For classification tasks, generate a precision-recall curve on the test set if it’s binary classification. Plot the curve and compute the area under it (AUPRC). Emphasize how this metric can be more revealing than accuracy in imbalanced scenarios.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "precision, recall, _ = precision_recall_curve(y_test, probas)\n",
    "prc_auc = auc(recall, precision)\n",
    "```\n",
    "Convert model logits to probabilities for `probas`.\n",
    "\n",
    "---\n",
    "**Variant 11:** Employ test-time data augmentation. For each test sample, create a few augmented variants (e.g., slight jitter in input). Average the model’s predictions across these augmented samples as the final prediction. Plot how this approach compares to single-shot predictions.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "augmented_preds = []\n",
    "for _ in range(N):\n",
    "    augmented_input = test_sample + random_jitter\n",
    "    augmented_preds.append(model(augmented_input))\n",
    "ensemble_pred = torch.mean(torch.stack(augmented_preds), dim=0)\n",
    "```\n",
    "Document any performance gain.\n",
    "\n",
    "---\n",
    "**Variant 12:** Visualize the difference between `model.train()` and `model.eval()` predictions if your model uses BatchNorm or Dropout. Show how test predictions might be inconsistent if you forget to switch to `eval()`. Provide a short demonstration on a small test batch.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "model.train()\n",
    "train_preds = model(x_test)\n",
    "model.eval()\n",
    "eval_preds = model(x_test)\n",
    "```\n",
    "Plot both outcomes to see differences, especially if the test batch is small.\n",
    "\n",
    "---\n",
    "**Variant 13:** Illustrate how to extract intermediate layer outputs at inference time by modifying the forward pass or using hooks. Compare those intermediate activations for a few test samples. This helps you understand model interpretability and debugging.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "def forward_with_activation(self, x):\n",
    "    x = self.layer1(x)\n",
    "    act = torch.relu(x)\n",
    "    out = self.layer2(act)\n",
    "    return out, act\n",
    "```\n",
    "Use hooks or direct expansions in `forward`.\n",
    "\n",
    "---\n",
    "**Variant 14:** For a multi-class classifier, generate per-class accuracy, precision, and recall metrics on the test set. Summarize them in a tabular format. Visualize them with a horizontal bar chart, labeling each class. This variant digs deeper into evaluation beyond a single metric.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "correct_per_class = ...\n",
    "precision_per_class = ...\n",
    "recall_per_class = ...\n",
    "```\n",
    "Use `precision_score` and `recall_score` from scikit-learn or your own methods.\n",
    "\n",
    "---\n",
    "**Variant 15:** Demonstrate how to evaluate the model in multiple mini-batches instead of all test data at once. Accumulate predictions and labels for each batch, then compute metrics at the end. Plot a line graph showing how partial accuracy might vary across test mini-batches.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "all_preds, all_labels = [], []\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch[\"inputs\"])\n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(batch[\"labels\"])\n",
    "```\n",
    "Aggregate for final metrics.\n",
    "\n",
    "---\n",
    "**Variant 16:** Conduct a temperature scaling experiment on the test set for classification. Manually adjust the logits with a scalar “temperature” and compute calibration error. Plot the model’s predicted probability vs. true frequency to see if temperature scaling improves calibration.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "def temperature_scale(logits, temp):\n",
    "    return logits / temp\n",
    "```\n",
    "Use an appropriate calibration metric like the expected calibration error (ECE).\n",
    "\n",
    "---\n",
    "**Variant 17:** For a regression setting, create an error histogram on the test set, showing how frequently each error magnitude occurs. Provide additional lines for mean error and median error. Summarize your findings on the shape of the error distribution and potential outliers.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "errors = (preds - y_test).view(-1).cpu().numpy()\n",
    "plt.hist(errors, bins=30)\n",
    "plt.axvline(np.mean(errors), color='red')\n",
    "plt.axvline(np.median(errors), color='green')\n",
    "```\n",
    "Remember to set `model.eval()`.\n",
    "\n",
    "---\n",
    "**Variant 18:** Evaluate model predictive uncertainty by using a dropout-based approximation at test time (Monte Carlo dropout). Perform multiple forward passes with `model.train()` (keeping dropout active) for each test sample, then compute mean and standard deviation of predictions. Plot the distribution for a sample subset.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "def mc_dropout_prediction(model, x, runs=50):\n",
    "    model.train()\n",
    "    preds = []\n",
    "    for _ in range(runs):\n",
    "        preds.append(model(x))\n",
    "    return torch.stack(preds).mean(dim=0), torch.stack(preds).std(dim=0)\n",
    "```\n",
    "Compare with the standard single `eval()` pass.\n",
    "\n",
    "---\n",
    "**Variant 19:** Highlight the correlation between input magnitude and prediction error. For each test sample, compute the absolute error and the magnitude of x (or relevant input). Plot a scatter of input magnitude vs. error, then fit a simple trend line. This helps reveal if the model struggles with large or small inputs.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "errors = (preds - y_test).abs()\n",
    "input_magnitudes = x_test.abs()  # or a relevant function of x\n",
    "```\n",
    "Analyze correlation or slope for interpretability.\n",
    "\n",
    "---\n",
    "**Variant 20:** Display the top five highest-error samples from the test set in a table or textual summary, listing input features, true labels, predictions, and error magnitude. This final variant helps in diagnosing what kinds of inputs cause the greatest mistakes and suggests future data improvements.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "errors = (preds - y_test).abs()\n",
    "top5 = errors.topk(5).indices\n",
    "```\n",
    "Print out relevant data for each sample. Combine with domain knowledge to interpret results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac3ed3-3393-41ce-a895-726d9d208791",
   "metadata": {
    "id": "bbMPK5Qjjyx_"
   },
   "outputs": [],
   "source": [
    "# Make predictions with the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758d2d2-26b3-4084-9d9e-9c001364e969",
   "metadata": {
    "id": "K3BdmQaDpFo8"
   },
   "outputs": [],
   "source": [
    "# Plot the predictions (these may need to be on a specific device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31ca4a-4484-4009-8afb-267225331b9d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb619fc-e4d3-4da0-b7a7-77a3273750e2",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 5. Save your trained model's state_dict() to file.</span>\n",
    "\n",
    "[Go back to the content](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae62997-e4f8-428a-9356-cf5d999549ec",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Variant 1:** After training a regression model, save its state dictionary to `\"model_regression.pth\"`. Then create a new model instance of the same architecture, load the weights, and confirm that the test MSE remains identical. Emphasize the importance of matching model architectures when loading state dicts.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "torch.save(model.state_dict(), \"model_regression.pth\")\n",
    "model_new = MyModel()\n",
    "model_new.load_state_dict(torch.load(\"model_regression.pth\"))\n",
    "```\n",
    "Evaluate `model_new` on the test set for confirmation.\n",
    "\n",
    "---\n",
    "**Variant 2:** Train a classification model, then save both `state_dict()` and a dictionary containing class-to-index mappings to a single file using `torch.save(...)`. Show how to load the dictionary, reassign the mapping, and confirm correct interpretation of predicted class indices upon inference.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "checkpoint = {\n",
    "  \"model_state\": model.state_dict(),\n",
    "  \"class_mapping\": class_to_idx\n",
    "}\n",
    "torch.save(checkpoint, \"checkpoint.pth\")\n",
    "```\n",
    "Then load with `checkpoint = torch.load(\"checkpoint.pth\")`.\n",
    "\n",
    "---\n",
    "**Variant 3:** Store the model state alongside optimizer state in one dictionary. Illustrate the difference in training continuity when you reload the optimizer’s parameters vs. ignoring them. This showcases how you can resume training from an exact checkpoint.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "checkpoint = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"epoch\": current_epoch\n",
    "}\n",
    "torch.save(checkpoint, \"model_and_optim.pth\")\n",
    "```\n",
    "Reload them to resume seamlessly.\n",
    "\n",
    "---\n",
    "**Variant 4:** Save the best model checkpoint during training whenever the validation loss improves. Keep track of the “best loss” and only overwrite your saved file if the new validation loss is lower. Present your final model with the best validation performance after training.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    torch.save(model.state_dict(), \"best_model.pth\")\n",
    "```\n",
    "This approach is standard for picking an optimal checkpoint.\n",
    "\n",
    "---\n",
    "**Variant 5:** Create a scenario where you freeze all but the final layer of the network, then save only the final layer’s parameters after partial retraining. Demonstrate how partial parameter freezing can reduce file size if the bulk of the model is large and unchanged.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "for param in model.some_layers.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "After training the final layer, `torch.save(model.final_layer.state_dict(), \"final_layer.pth\")`.\n",
    "\n",
    "---\n",
    "**Variant 6:** Convert your model to TorchScript via `torch.jit.trace` or `torch.jit.script` before saving, resulting in a self-contained file that can be loaded without the original Python code. Show how to load and run inference with the TorchScript model.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"model_scripted.pt\")\n",
    "scripted = torch.jit.load(\"model_scripted.pt\")\n",
    "preds = scripted(x_test)\n",
    "```\n",
    "This approach is beneficial for deployment.\n",
    "\n",
    "---\n",
    "**Variant 7:** Demonstrate saving a multi-head model by saving each head’s state dict separately (for instance, regression_head, classification_head) in multiple files. Then load them individually into a new base model. This variant highlights scenarios where partial components are reused or replaced.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "torch.save(model.regression_head.state_dict(), \"reg_head.pth\")\n",
    "torch.save(model.classification_head.state_dict(), \"class_head.pth\")\n",
    "```\n",
    "Initialize a new base model, load each head accordingly.\n",
    "\n",
    "---\n",
    "**Variant 8:** Illustrate how to save a model on GPU, then safely reload it on CPU with `map_location=torch.device('cpu')`. This step avoids potential device mismatch errors. Evaluate the reloaded model on CPU-based test data.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "torch.save(model.state_dict(), \"gpu_model.pth\")\n",
    "model_cpu = MyModel()\n",
    "model_cpu.load_state_dict(torch.load(\"gpu_model.pth\", map_location=\"cpu\"))\n",
    "```\n",
    "Confirm no device mismatch occurs.\n",
    "\n",
    "---\n",
    "**Variant 9:** Save a minimal reproducible environment by exporting a single `.pth` file that contains the model state dict plus a text file specifying the hyperparameters used (learning rate, batch size, etc.). Demonstrate how to read the hyperparameters from the text file before constructing the new model for loading.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "hyperparams = {\"lr\": 0.01, \"batch_size\": 64, \"epochs\": 100}\n",
    "with open(\"hyperparams.json\", \"w\") as f:\n",
    "    json.dump(hyperparams, f)\n",
    "```\n",
    "Load them with `json.load` before re-initializing your model.\n",
    "\n",
    "---\n",
    "**Variant 10:** Perform a partial save: only store the weights of the first layer in a multi-layer model, ignoring the rest. Show how to partially load them into a new model while re-initializing the remaining layers. This approach can be useful for transfer learning or reusing initial features.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "layer1_weights = model.layer1.state_dict()\n",
    "torch.save(layer1_weights, \"layer1.pth\")\n",
    "model_new.layer1.load_state_dict(torch.load(\"layer1.pth\"))\n",
    "```\n",
    "The other layers remain uninitialized.\n",
    "\n",
    "---\n",
    "**Variant 11:** Demonstrate saving the model’s parameters after each epoch in a loop (e.g., “model_epoch_1.pth”, “model_epoch_2.pth”, etc.). Compare the file sizes and explain how to manage disk space by limiting the number of checkpoints or compressing them.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    ...\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")\n",
    "```\n",
    "Use naming patterns for organization.\n",
    "\n",
    "---\n",
    "**Variant 12:** Save a dictionary that includes the training history (loss curve, accuracy curve) alongside the model weights. After training completes, load this file in a separate script to generate post-training plots, showing how to keep model states and logs together.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "history = {\"train_loss\": train_losses, \"val_loss\": val_losses}\n",
    "checkpoint = {\"state\": model.state_dict(), \"history\": history}\n",
    "torch.save(checkpoint, \"model_plus_history.pth\")\n",
    "```\n",
    "Then load to plot historical curves.\n",
    "\n",
    "---\n",
    "**Variant 13:** Highlight how to rename keys in the `state_dict()` if you refactor the model. Suppose the old model used “fc1.weight” but the new model calls it “layer1.weight.” Show how to adapt the saved dictionary’s keys before loading.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "old_dict = torch.load(\"old_model.pth\")\n",
    "new_dict = {}\n",
    "for k, v in old_dict.items():\n",
    "    new_k = k.replace(\"fc1\", \"layer1\")\n",
    "    new_dict[new_k] = v\n",
    "model_new.load_state_dict(new_dict)\n",
    "```\n",
    "This approach ensures compatibility.\n",
    "\n",
    "---\n",
    "**Variant 14:** Demonstrate saving to a custom directory with versioned subfolders (e.g., `checkpoints/v1/model_best.pth`). Also illustrate how to automatically create the directory if it doesn’t exist. This variant addresses practical file-organization strategies for bigger projects.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "import os\n",
    "os.makedirs(\"checkpoints/v1\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"checkpoints/v1/model_best.pth\")\n",
    "```\n",
    "You can incorporate a date or run number in the folder name.\n",
    "\n",
    "---\n",
    "**Variant 15:** Save the model in multiple formats: first as a plain PyTorch `.pth` file, then also as an ONNX file with `torch.onnx.export`. Compare the size and usage of each approach. ONNX can facilitate cross-platform inference beyond PyTorch.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "torch.onnx.export(model, example_input, \"model.onnx\")\n",
    "```\n",
    "Show how to load the ONNX model in an external environment if desired.\n",
    "\n",
    "---\n",
    "**Variant 16:** Combine model quantization with saving. Perform post-training dynamic quantization on a linear model, then save the quantized model. Show how to load and run inference on the quantized version. This approach can reduce model size significantly.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
    "```\n",
    "Load it similarly, ensuring you replicate the quantized structure.\n",
    "\n",
    "---\n",
    "**Variant 17:** Demonstrate how to save the entire model object (not just state_dict) using `torch.save(model, \"full_model.pth\")`. Warn about the potential pitfalls, like reliance on the exact same Python code structure. Then load with `model = torch.load(\"full_model.pth\")`. Evaluate on test data.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "torch.save(model, \"full_model.pth\")\n",
    "model_loaded = torch.load(\"full_model.pth\")\n",
    "```\n",
    "Explain that this method depends on code environment consistency.\n",
    "\n",
    "---\n",
    "**Variant 18:** Split large model checkpoint files if size constraints exist. Show how to divide the state dict into chunks, storing them in multiple `.pth` files. Then recombine them when loading. This is relevant for extremely large models.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "sd = model.state_dict()\n",
    "keys = list(sd.keys())\n",
    "half = len(keys)//2\n",
    "sd_part1 = {k: sd[k] for k in keys[:half]}\n",
    "sd_part2 = {k: sd[k] for k in keys[half:]}\n",
    "torch.save(sd_part1, \"part1.pth\")\n",
    "torch.save(sd_part2, \"part2.pth\")\n",
    "```\n",
    "Reassemble by merging dictionaries.\n",
    "\n",
    "---\n",
    "**Variant 19:** Version your model architecture. Show how to store a “model_version” integer in the checkpoint and include upgrade logic if the version is older. For instance, if the version < 2, rename certain layers or adjust shapes. This ensures backward compatibility in your codebase.\n",
    "\n",
    "*Technical note:* \n",
    "```python\n",
    "checkpoint = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"model_version\": 1\n",
    "}\n",
    "torch.save(checkpoint, \"checkpoint_v1.pth\")\n",
    "```\n",
    "On load, check `model_version` and adapt accordingly.\n",
    "\n",
    "---\n",
    "**Variant 20:** Secure your saved model by encrypting the `.pth` file. (Although PyTorch doesn’t have native encryption, you can wrap the file with standard Python encryption libraries or external tools.) Demonstrate a simple encryption/decryption step before/after the `torch.save` call. Emphasize scenarios where data security is paramount.\n",
    "\n",
    "*Technical note:* \n",
    "- Use a library like `cryptography.fernet` to encrypt the raw checkpoint bytes.  \n",
    "- Decrypt them before calling `torch.load`.  \n",
    "- The model remains unchanged internally.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
