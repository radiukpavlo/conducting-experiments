{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratory work 1.</center></h1>\n",
    "<h2><center>PyTorch Fundamentals Exercises</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Last name and First name\n",
    "\n",
    "**Варіант:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Create a random tensor](#1.1)\n",
    "2. [Task 2. Simple matrix multiplication](#1.2)\n",
    "3. [Task 3. The use of CPU vs. GPU](#1.3)\n",
    "4. [Task 4. Processing a tensor](#1.4)\n",
    "5. [Task 5. Advancing with a tensor](#1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzDBM_v4iMe7"
   },
   "source": [
    "A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness):\n",
    "  * The documentation on [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor).\n",
    "  * The documentation on [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 1. Create a random tensor</span>\n",
    "\n",
    "[Go back to the content](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Variant 1:** Create a random tensor using PyTorch’s built-in functions by generating a tensor of shape `(7, 7)` with values drawn from a standard normal distribution. After generating the tensor using `torch.randn`, calculate its mean and standard deviation to understand the distribution of values. This exercise not only reinforces the basics of tensor creation but also familiarizes you with essential statistical functions in PyTorch. As you work through this task, consider experimenting with reshaping or transposing the tensor to see how operations affect its structure. Ensure that you set a fixed random seed to achieve reproducible results throughout your experiments.\n",
    "\n",
    "*Technical note:* Use `torch.randn` for tensor generation, `torch.mean` and `torch.std` for computing statistics, and `torch.manual_seed` to guarantee reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 2:** Create a random tensor that simulates a grayscale image by creating a tensor of shape `(28, 28)` where each element represents a pixel intensity normalized between 0 and 1. Once the tensor is generated using `torch.rand`, apply a threshold to convert it into a binary image. Finally, use Matplotlib to display both the original and the thresholded images side by side. This task is designed to help you understand how tensor data can represent image information and how basic image processing techniques can be implemented using PyTorch and Python visualization libraries.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to create the tensor, `torch.where` for thresholding, and `matplotlib.pyplot.imshow` to display the images. Don’t forget to set `torch.manual_seed` for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 3:** Create a random tensor that mimics a batch of images intended for a convolutional neural network. Generate a tensor with shape `(32, 3, 64, 64)`, where 32 represents the batch size, 3 the color channels, and `64 x 64` the spatial dimensions of each image. After generating this tensor, calculate the mean and variance over the batch or channel dimensions to simulate data normalization steps. This task is an excellent introduction to handling high-dimensional data and understanding how deep learning models process image data.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation, `torch.mean` and `torch.var` for computing statistics, and `torch.manual_seed` to ensure reproducibility. Optionally, use NumPy for additional statistical comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 4:** Create a random tensor of shape `(10, 5)` representing synthetic feature data for a machine learning experiment. After generating the tensor with `torch.rand`, apply an element-wise exponential function using `torch.exp` to simulate an activation function transformation. Print both the original and transformed tensors to observe the changes. This variant demonstrates not only the creation of random data, but also how element-wise operations can be used to simulate neural network activations.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for generating the tensor, `torch.exp` for the exponential transformation, and `torch.manual_seed` to maintain consistency in results.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 5:** Create a random tensor to simulate sensor data collected over time. Generate a tensor of shape `(100, 4)`, where each column represents readings from a different sensor. Once the tensor is created, normalize each column using min-max scaling so that all values fall within the range of 0 to 1. This task introduces you to both tensor manipulation and data preprocessing techniques, which are vital for preparing data before feeding it into deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation and apply min-max normalization using simple arithmetic operations, along with `torch.min` and `torch.max`. Always set `torch.manual_seed` to ensure reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 6:** Create a random tensor that represents output logits for a classification problem. Create a tensor with shape `(50, 10)`, where each row corresponds to a batch sample’s raw scores for 10 different classes. After the tensor is created using `torch.rand`, apply the softmax function from `torch.nn.functional` to convert these logits into probabilities. This variant provides practical insight into the transformation of raw neural network outputs into probability distributions that are used for making predictions.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation and `torch.nn.functional.softmax` (imported as `F.softmax`) to compute the probabilities. Set the appropriate dimension for softmax and use `torch.manual_seed` for consistent outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 7:** Create a random tensor that serves as the initial weight matrix for a fully connected neural network layer. The tensor should have a shape of `(128, 64)`, with 128 input features and 64 neurons in the layer. Once generated, apply Xavier (Glorot) initialization using `torch.nn.init.xavier_uniform_` to improve the starting point of training. This exercise emphasizes the importance of proper weight initialization in deep learning models and familiarizes you with one of the most common initialization techniques.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for the initial tensor generation and `torch.nn.init.xavier_uniform_` for the initialization. Always set `torch.manual_seed` to ensure that the results are reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 8:** Create a random tensor suitable as input for a recurrent neural network (RNN). Construct a tensor with shape `(15, 20, 50)` where `15` represents the sequence length, `20` the batch size, and `50` the feature size. After generating this tensor using `torch.rand`, reshape or view it if necessary to conform to typical RNN input requirements, and then examine its structure. This task is crafted to enhance your understanding of sequential data formatting and the manipulation of tensors in deep learning applications.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for generating the tensor and functions like `tensor.view()` or `tensor.reshape()` for adjusting dimensions. Ensure reproducibility by setting `torch.manual_seed`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 9:** Create a random tensor for a synthetic regression problem by generating two tensors: one with shape `(200, 1)` to simulate output values and another with shape `(200, 5)` to represent five input features. Concatenate these tensors along the appropriate dimension to form a complete dataset. This exercise not only familiarizes you with basic tensor operations and concatenation using `torch.cat` but also simulates the initial stages of preparing data for regression analysis in a deep learning context.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for generating both tensors and `torch.cat` to concatenate them along dimension 1. Set `torch.manual_seed` to ensure that your results are consistent and reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 10:** Create a random tensor that mimics the output of a convolutional layer in a convolutional neural network (CNN). Create a tensor with shape `(16, 32, 8, 8)` where `16` is the batch size, `32` represents the number of feature maps, and `8x8` denotes the spatial dimensions. After generating the tensor using `torch.rand`, compute the mean activation for each feature map by reducing over the spatial dimensions. This task is designed to provide insight into how convolutional operations transform data and how activation statistics can be analyzed.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation and `torch.mean` with the appropriate dimension parameters to compute the mean activations. Always set `torch.manual_seed` for reproducibility, and consider using Matplotlib if you wish to visualize the activation distributions.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 11:** Create a random tensor of shape `(15, 15)` using PyTorch and then compute the cumulative sum along both the row and column dimensions. This exercise encourages you to explore how tensor operations can be chained to analyze the progression of summed values across different axes. Additionally, compare the results obtained from both directions to deepen your understanding of tensor manipulation. Ensure that you use a fixed random seed for reproducibility and experiment with reshaping the tensor to observe different cumulative behaviors.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to generate the tensor, `torch.cumsum` to compute cumulative sums along specified dimensions, and `torch.manual_seed` for setting the seed.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 12:** Create a random 3-dimensional tensor of shape `(3, 3, 3)` representing a small volumetric dataset. Your task is to perform slicing operations along each dimension and then compute the mean and variance for each slice. This variant is designed to familiarize you with advanced indexing and statistical analysis in PyTorch, as well as to illustrate how multidimensional data can be processed efficiently.\n",
    "\n",
    "*Technical note:* Utilize `torch.rand` for tensor creation, slicing via standard Python indexing (e.g., `tensor[:, 0, :]`), and functions like `torch.mean` and `torch.var` for statistical computation. Set a manual seed using `torch.manual_seed`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 13:** Create a random tensor simulating time-series data with shape `(50, 10)`, where each row represents a time step and each column a different feature. Next, compute the discrete difference along the time dimension to approximate the derivative of the signals. This task helps you understand how temporal data can be manipulated and analyzed, which is essential for sequence modeling and forecasting tasks in deep learning.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to generate the tensor and `torch.diff` (or perform manual subtraction) to compute the differences along the first dimension. Remember to set `torch.manual_seed` for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 14:** Create a random tensor of shape `(100, 300)` to simulate a word embedding matrix, where each row is a 300-dimensional vector representing a word. Normalize each embedding vector using L2 normalization so that each vector has a unit norm. This exercise reinforces the importance of normalization in embedding spaces and provides practical insight into handling high-dimensional data.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to create the tensor, then compute the L2 norm using `torch.norm` and normalize by dividing each vector by its norm. Set a fixed seed with `torch.manual_seed` to ensure consistency.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 15:** Create a random tensor mimicking a mini-batch of grayscale images, with shape `(8, 1, 28, 28)`, similar to the MNIST dataset. Then, flatten each image into a 1D tensor and compute the Euclidean norm of each flattened image. This variant introduces you to the process of flattening multidimensional data and applying vector norms, which is crucial in feature scaling and neural network preprocessing.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for generating the tensor, `tensor.view()` or `tensor.flatten()` for reshaping, and `torch.norm` for computing the Euclidean norm. Ensure reproducibility by setting `torch.manual_seed`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 16:** Create a random tensor representing a weight matrix with shape `(256, 128)` and apply an element-wise sine function to simulate a non-linear transformation of the weights. Afterwards, visualize the distribution of the transformed values by plotting a histogram using Matplotlib. This task demonstrates how custom transformations can be applied to neural network parameters and underscores the importance of visualizing data distributions during model initialization.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to generate the weight matrix, apply `torch.sin` for the transformation, and utilize `matplotlib.pyplot.hist` to create the histogram. Don’t forget to set `torch.manual_seed` to maintain reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 17:** Create a random tensor of shape `(20, 5)` representing raw output scores for a classification problem. Transform these scores into log probabilities using `torch.log_softmax` along the appropriate dimension, and discuss the benefits of using log probabilities for numerical stability in loss computations. This variant deepens your understanding of activation transformations in classification tasks.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation, `torch.log_softmax` (from `torch.nn.functional`) for converting scores, and `torch.manual_seed` to ensure consistent outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 18:** Create a random tensor tailored for transformer models with shape `(4, 16, 64)`, where 4 is the batch size, 16 is the sequence length, and 64 is the embedding dimension. Enhance this tensor by adding a positional encoding computed from sine and cosine functions to incorporate sequential order information. This variant helps you understand how positional encodings enrich transformer models by providing context on the position of tokens within sequences.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to generate the base tensor, then implement positional encoding using mathematical operations (e.g., `torch.sin`, `torch.cos`) and broadcasting. Set the random seed with `torch.manual_seed` for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 19:** Create a random tensor to simulate an adjacency matrix for a small graph with shape `(10, 10)`. Modify the tensor to enforce symmetry and set the diagonal to zero, ensuring that it represents an undirected graph with no self-loops. This exercise provides insight into data preparation for graph neural networks and the importance of maintaining specific structural properties in graph representations.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to generate the initial tensor, then average the tensor with its transpose to enforce symmetry and use `torch.fill_diagonal_` or indexing to zero out the diagonal. Always set `torch.manual_seed` to ensure reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 20:** Create a random tensor representing the raw outputs of a multi-label classification model with shape `(50, 8)`. Apply the sigmoid activation function to convert these outputs into probabilities, and then threshold these probabilities at `0.5` to obtain binary predictions. This variant is designed to illustrate the process of converting continuous outputs into actionable classifications and highlights the significance of thresholding in multi-label scenarios.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for creating the tensor, `torch.sigmoid` for applying the activation function, and conditional operations (e.g., `(tensor > 0.5).float()`) to perform thresholding. Set `torch.manual_seed` to ensure that results are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__iXqqz-ioUJ"
   },
   "source": [
    "<a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pUq9Dc8i2L7"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 2. Simple matrix multiplication</span>\n",
    "\n",
    "[Go back to the content](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-XxvRLfiqkR"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Variant 1:** In this task, you are required to generate two random tensors using PyTorch where the first tensor has a shape of `(7, 5)` and the second tensor has a shape of `(5, 3)`. Your goal is to perform matrix multiplication between these two tensors and then print out both the resulting tensor and its dimensions. This exercise helps reinforce the importance of aligning tensor dimensions correctly for multiplication. Additionally, analyze the output shape to understand how the inner dimensions determine the resulting tensor's shape in linear algebra operations.\n",
    "\n",
    "*Technical note:* Use `torch.rand` or `torch.randn` to generate tensors, and employ either `torch.mm` or the `@` operator for matrix multiplication. Verify dimensions with the `.shape` attribute.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 2:** Set the random seed to `42` to ensure reproducibility while creating two random matrices: one of shape `(8, 4)` and another of shape `(4, 6)`. Multiply these matrices to obtain a new tensor, then use `torch.transpose` on one of the matrices to observe how transposition affects the result. Finally, print the output tensor along with its shape. This variant emphasizes the role of random seeding and tensor transposition in preparing data for deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.manual_seed(42)` for consistency, `torch.transpose` for transposing tensors, and matrix multiplication functions like `torch.matmul` to perform the operation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 3:** Create two square tensors of shape `(10, 10)` using PyTorch’s random functions and perform matrix multiplication between them. After obtaining the product, calculate the trace of the resulting matrix to explore the relationship between matrix multiplication and trace calculation. This exercise serves as a practical demonstration of how linear algebra concepts like trace are applied in neural network operations and regularization techniques.\n",
    "*Technical note:* Use `torch.rand` for tensor creation, `torch.mm` for multiplication, and `torch.trace` to compute the trace of the product.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 4:** Generate two random tensors where the first is of shape `(12, 7)` and the second is of shape `(7, 12)` using PyTorch’s built-in functions. Multiply these tensors to form a new matrix, then compute the determinant of the resulting matrix using `torch.det`. This task helps you understand how matrix multiplication affects properties such as invertibility and volume scaling, which are essential concepts in deep learning and linear algebra.\n",
    "\n",
    "*Technical note:* Utilize `torch.rand` for tensor generation, `torch.mm` for multiplication, and `torch.det` to calculate the determinant.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 5:** In this variant, create two random tensors with shapes `(9, 3)` and `(3, 9)`, respectively. Multiply these tensors to produce a result, and subsequently apply the ReLU activation function to the output matrix to simulate a basic neural network layer operation. This exercise demonstrates how linear transformations and non-linear activations work together in deep learning pipelines.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to create tensors, the `torch.mm` function or `@` operator for multiplication, and `torch.nn.functional.relu` for applying the ReLU function.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 6:** Generate two random tensors with the same shape `(16, 16)` using PyTorch. Perform matrix multiplication on these tensors, and then conduct an element-wise multiplication of the resulting tensor with a tensor filled with ones to simulate the addition of a bias term. This task mimics operations commonly seen in fully connected layers, reinforcing the integration of linear operations and bias adjustments in neural networks.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation, `torch.mm` for matrix multiplication, and element-wise operations with `*` to apply the bias simulation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 7:** Create a random tensor A of shape `(5, 8)` and another tensor B of shape `(8, 5)`. Multiply them to yield a square matrix of shape `(5, 5)`. Afterwards, compute the eigenvalues of the resulting matrix using `torch.linalg.eigvals` to gain insights into its spectral properties. This variant illustrates how matrix multiplication can be extended to more advanced linear algebra concepts that are often relevant in optimization and stability analysis in deep learning.\n",
    "\n",
    "*Technical note:* Generate tensors using `torch.rand`, perform multiplication with `torch.mm`, and calculate eigenvalues using `torch.linalg.eigvals`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 8:** Simulate a mini-batch matrix multiplication scenario by generating a tensor representing a batch of matrices. Create one tensor with shape `(4, 7, 10)`—representing 4 matrices of size `(7, 10)`—and another with shape `(4, 10, 3)` for 4 matrices of size `(10, 3)`. Perform batch matrix multiplication using `torch.bmm` and print both the shape and content of the resulting tensor. This task demonstrates how batch operations are performed in deep learning frameworks when processing multiple samples simultaneously.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation and `torch.bmm` to perform batch matrix multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 9:** Create two small tensors, one of shape `(2, 3)` and another of shape `(3, 2)`, and perform matrix multiplication using the `torch.matmul` function. After obtaining the result, compare it with the output produced by the `torch.mm` function to ensure consistency between these methods. This variant highlights subtle differences and similarities in matrix multiplication functions available in PyTorch, reinforcing their correct usage in deep learning applications.\n",
    "*Technical note:* Utilize `torch.rand` for generating tensors, and perform multiplication with both `torch.matmul` and `torch.mm` to compare outputs.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 10:** Generate two random matrices with different data types: one tensor with data type `torch.float32` and another with `torch.float64`, ensuring their inner dimensions match for multiplication. Before performing matrix multiplication, convert the second tensor to match the first tensor’s data type. Multiply them together and print the resulting tensor along with its data type to verify the successful conversion and operation.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation, type conversion methods such as `.to(dtype)`, and matrix multiplication functions like `torch.mm` or `@`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 11:** Create two random tensors where the first tensor has a shape of `(20, 15)` generated using `torch.randn` and the second tensor has a shape of `(15, 25)` generated using `torch.rand`. Multiply these tensors to obtain a result of shape `(20, 25)`. Then, compute and print basic statistics such as the mean and standard deviation of the resulting tensor to understand the distribution of its values.\n",
    "\n",
    "*Technical note:* Use `torch.randn` and `torch.rand` for tensor creation, `torch.mm` or `torch.matmul` for multiplication, and functions like `torch.mean` and `torch.std` for statistical analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 12:** Set up a simulation of a neural network’s linear layer by creating a weight matrix tensor of shape `(32, 64)` and an input feature tensor of shape `(64, 16)`. Multiply these tensors to simulate the forward pass of a linear layer, and then add a bias tensor of shape `(32, 16)` to the result. Print the final output tensor to verify the complete linear transformation process.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for generating weight and input tensors, the `+` operator for bias addition, and matrix multiplication via `torch.mm` or `torch.matmul`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 13:** Using PyTorch, generate a random tensor representing a small image patch of shape `(3, 3)` and a transformation matrix of shape `(3, 3)`. Multiply these matrices to apply a linear transformation to the image patch. This exercise demonstrates how matrix multiplication can simulate operations such as rotation, scaling, or other affine transformations on image data—a key concept in computer vision and deep learning.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation, and perform multiplication using the `@` operator or `torch.mm` to apply the transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 14:** Create two random matrices with shapes `(6, 4)` and `(4, 6)` respectively, and perform matrix multiplication to yield a square matrix of shape `(6, 6)`. After multiplication, apply the sigmoid activation function to each element of the resulting matrix to map the values between 0 and 1. This operation simulates a typical neural network process where linear transformations are followed by non-linear activations.\n",
    "\n",
    "*Technical note:* Use `torch.rand` to create tensors, `torch.mm` or the `@` operator for multiplication, and `torch.sigmoid` for applying the activation function.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 15:** Generate two random tensors, one with shape `(10, 20)` and another with shape `(20, 5)`, using the `@` operator for multiplication. Compare the resulting tensor with one obtained by using the `torch.matmul` function to demonstrate that both methods yield identical outcomes. This variant reinforces the interchangeability of different matrix multiplication operators in PyTorch.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation and compare the outputs of `@` and `torch.matmul` to ensure consistency.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 16:** Construct a scenario where matrix multiplication is followed by scaling and bias addition. Create two random tensors with shape `(7, 7)` each and multiply them. Then, multiply the product by a constant scaling factor (for instance, `0.5`) and add a bias tensor of the same shape. This sequence simulates the affine transformation commonly used in neural network layers.\n",
    "\n",
    "*Technical note:* Generate tensors using `torch.rand`, perform multiplication with `torch.mm`, scale using the `*` operator, and add bias using the `+` operator.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 17:** Create two random tensors, both of shape `(4, 4)`, and perform their matrix multiplication. Following the multiplication, compute the Frobenius norm of the resulting matrix using `torch.norm` with the appropriate parameters. This variant demonstrates how combining matrix multiplication with norm calculations can be useful for assessing the magnitude and stability of transformations in deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation, `torch.mm` for multiplication, and `torch.norm` with `p='fro'` to compute the Frobenius norm.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 18:** Generate two random tensors using `torch.randn`, where the first has a shape of `(8, 10)` and the second has a shape of `(10, 8)`. Multiply these tensors to obtain a square matrix, and then attempt to compute the inverse of the resulting matrix using `torch.inverse`. This exercise will help you understand the conditions required for a matrix to be invertible and the interplay between matrix multiplication and inversion in linear algebra.\n",
    "\n",
    "*Technical note:* Use `torch.randn` for random tensor generation, `torch.mm` for multiplication, and `torch.inverse` to calculate the inverse of the matrix.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 19:** Simulate a transformation in a neural network by creating a random weight matrix of shape `(64, 128)` and an input activation matrix of shape `(128, 32)`. Multiply these tensors to simulate the linear transformation of features in a hidden layer, and then apply batch normalization to the resulting tensor using PyTorch’s batch normalization functions. Print the final output to observe how matrix multiplication integrates with other deep learning operations.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor creation, `torch.mm` or `torch.matmul` for multiplication, and `torch.nn.BatchNorm1d` for applying batch normalization.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 20:** Using PyTorch, generate two high-dimensional tensors to simulate a simplified attention mechanism. Create a query tensor of shape `(5, 16)` and a key tensor of shape `(16, 5)`, then multiply them to compute attention scores. Afterward, apply the softmax function to the resulting matrix to normalize the scores. This variant bridges the concept of matrix multiplication with the attention mechanism found in transformer models, illustrating a fundamental operation in modern deep learning architectures.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation, the `@` operator or `torch.mm` for matrix multiplication, and `torch.softmax` with an appropriate dimension argument to normalize the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 3. The use of CPU vs. GPU</span>\n",
    "\n",
    "[Go back to the content](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Variant 1:**\n",
    "Create a random tensor on the CPU with shape `(5, 5)`, then transfer it to the GPU to perform an elementwise squaring operation. After processing the tensor on the GPU, transfer the result back to the CPU and print both the original and squared tensors. This exercise demonstrates the process of moving simple tensors between devices and applying basic arithmetic operations on them.\n",
    "\n",
    "*Technical note:* Use `torch.rand((5, 5))` for tensor generation, `.to(device)` for transferring between CPU and GPU, and the `**2` operator or `torch.pow(tensor, 2)` for squaring.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 2:**\n",
    "Create a random tensor with shape `(4, 4)` directly on the GPU, then compute its sum and average using reduction operations. Once the computations are complete, transfer the results back to the CPU and print the values. This exercise emphasizes understanding how simple tensor processing operations differ in speed and performance when executed on the GPU versus the CPU.\n",
    "\n",
    "*Technical note:* Use `torch.rand((4, 4), device='cuda')` for GPU tensor creation, `torch.sum()` and `torch.mean()` for reductions, and `.cpu()` to transfer results back.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 3:**\n",
    "Set a specific random seed for both CPU and GPU computations using `torch.manual_seed` and `torch.cuda.manual_seed_all`. Create identical random tensors on the CPU and GPU with shape `(6, 6)`, then compute the elementwise sine of both tensors. Finally, compare the outputs to verify that operations yield consistent results across devices.\n",
    "\n",
    "*Technical note:* Use `torch.manual_seed(seed)` for CPU and `torch.cuda.manual_seed_all(seed)` for GPU seeding, `torch.sin()` for elementwise sine computation, and `.to(device)` for device transfers.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 4:**\n",
    "Create a random tensor on the CPU and apply an elementwise logarithm transformation. Transfer the tensor to the GPU and compute the elementwise exponential of the logged tensor. Then, move the result back to the CPU to check if the operations are inverses of each other. This task highlights the precision and consistency of simple tensor operations across devices.\n",
    "\n",
    "*Technical note:* Use `torch.log()` and `torch.exp()` for logarithm and exponential operations, `.to('cuda')` for GPU transfers, and `torch.allclose()` to compare the original and processed tensors.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 5:**\n",
    "Create a random tensor on the CPU with shape `(8, 2)` and then transfer it to the GPU to reshape it into a tensor of shape `(4, 4)` using the `.view()` method. After reshaping, compute the mean of the tensor along the rows and transfer the result back to the CPU. This variant teaches you about tensor reshaping and reduction operations while working across devices.\n",
    "\n",
    "*Technical note:* Use `tensor.view(new_shape)` for reshaping, `torch.mean(tensor, dim=1)` for reduction, and `.to(device)` for managing device transitions.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 6:**\n",
    "Create two simple tensors on the CPU with shapes `(3, 3)` and `(3, 3)`. Transfer one tensor to the GPU so that both reside on the same device, and then perform elementwise addition. Finally, transfer the summed tensor back to the CPU for verification. This exercise reinforces the importance of ensuring all operands are on the same device when processing simple tensors.\n",
    "\n",
    "*Technical note:* Use `.to('cuda')` to transfer tensors, the `+` operator for elementwise addition, and `.cpu()` to bring the result back.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 7:**\n",
    "Create a tensor filled with ones on the GPU with shape `(4, 4)`. Subtract a constant value (e.g., `0.5`) from this tensor, then multiply the result elementwise by another randomly generated tensor on the GPU. After processing, transfer the final tensor back to the CPU and display its contents. This task shows how simple arithmetic operations can be combined and processed efficiently on the GPU.\n",
    "\n",
    "*Technical note:* Use `torch.ones()` for generating constant tensors, simple arithmetic operators for subtraction and multiplication, and `.to(device)` for managing device transfers.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 8:**\n",
    "Create a random tensor on the GPU with shape `(10, 1)`, then compute its cumulative sum along the first dimension. Transfer the cumulative sum result back to the CPU and print both the original tensor and the cumulative result. This variant illustrates how reduction operations like cumulative sum can be executed on the GPU for faster computation and then easily retrieved to the CPU for analysis.\n",
    "\n",
    "*Technical note:* Use `torch.cumsum(tensor, dim=0)` for the cumulative sum, and ensure proper device management with `.to('cuda')` and `.cpu()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 9:**\n",
    "Create a random tensor on the CPU with shape `(7, 3)` and multiply it by a constant scalar (e.g., `3.0`). Then, transfer the scaled tensor to the GPU and apply the ReLU activation function elementwise. Finally, bring the activated tensor back to the CPU to compare it with the original. This exercise demonstrates how simple arithmetic and activation operations can be split across devices.\n",
    "\n",
    "*Technical note:* Use the `*` operator for scalar multiplication, `torch.nn.functional.relu()` for the activation, and `.to(device)` for transferring between CPU and GPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 10:**\n",
    "Create a random tensor on the GPU with shape `(5, 5)`, and perform a series of elementwise operations such as division by a constant, addition, and subtraction. Once all operations are completed, transfer the final tensor to the CPU to verify its data type and shape. This variant emphasizes the ease of chaining simple operations on the GPU before moving the result back to the CPU for further processing or analysis.\n",
    "\n",
    "*Technical note:* Use arithmetic operators for elementwise operations, and `.to('cpu')` to bring the tensor back after processing on the GPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 11:**\n",
    "Create a random tensor on the CPU with shape `(6, 2)` and then transfer it to the GPU to compute the elementwise absolute value. Once the absolute values are computed on the GPU, move the result back to the CPU and compare it with the output of an equivalent CPU-only computation. This exercise highlights consistency in simple tensor operations across devices while using functions like `torch.abs()`.\n",
    "\n",
    "*Technical note:* Use `torch.abs()` for computing absolute values and `.to(device)` for transferring tensors between CPU and GPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 12:**\n",
    "Create a random tensor on the GPU with shape `(8, 3)` and use slicing to extract a sub-tensor (for example, the first four rows). Compute the mean of the sliced tensor on the GPU, then transfer this mean value to the CPU for printing. This task reinforces the ability to perform slicing and reduction operations on simple tensors, regardless of whether they are processed on the CPU or GPU.\n",
    "\n",
    "*Technical note:* Use tensor slicing (e.g., `tensor[:4]`), `torch.mean()`, and proper device transfers with `.to('cuda')` and `.cpu()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 13:**\n",
    "Create a simple tensor on the CPU with shape `(4, 4)`, then transfer it to the GPU to simulate broadcasting by adding a one-dimensional tensor (vector) to each row. After performing the broadcast addition on the GPU, transfer the resulting tensor back to the CPU and verify that the operation was applied correctly across all rows.\n",
    "\n",
    "*Technical note:* Use broadcasting rules in PyTorch by ensuring the vector shape is compatible, and use `.to(device)` for moving tensors between CPU and GPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 14:**\n",
    "Create a tensor on the CPU containing integer values, and then convert it to a floating-point tensor before transferring it to the GPU. On the GPU, apply the softmax function along one axis to obtain a probability distribution, then transfer the normalized tensor back to the CPU. This exercise demonstrates both data type conversion and the application of simple activation functions across devices.\n",
    "\n",
    "*Technical note:* Use `tensor.float()` for type conversion, `torch.softmax()` for normalization, and `.to('cuda')`/`.cpu()` for device management.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 15:**\n",
    "Create a tensor on the GPU with shape `(5, 3)` and apply the hyperbolic tangent (tanh) activation function elementwise. Then, transfer the output tensor back to the CPU and compare it with a CPU-only computation of tanh on an equivalent tensor. This variant illustrates the consistency of non-linear activation functions when processing simple tensors across different devices.\n",
    "\n",
    "*Technical note:* Use `torch.tanh()` for the activation function and appropriate device transfer methods such as `.to('cuda')` and `.cpu()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 16:**\n",
    "Create a random tensor on the CPU with shape `(9, 9)`, then transfer it to the GPU to compute the elementwise square root. After computing the square root on the GPU, bring the result back to the CPU and compare it with the output obtained by performing the same operation solely on the CPU. This task highlights how simple mathematical operations are executed across devices with consistent precision.\n",
    "\n",
    "*Technical note:* Use `torch.sqrt()` for computing the square root and verify consistency using `.to('cuda')` and `.cpu()` for device transfers.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 17:**\n",
    "Create a random tensor on the GPU with `requires_grad=True` and shape `(3, 3)`. Multiply this tensor by a constant and then perform a backward pass to compute gradients. Transfer the computed gradients to the CPU and print them out. This variant emphasizes the autograd functionality in PyTorch while processing simple tensors across CPU and GPU, ensuring that gradient computations are correctly tracked regardless of the device.\n",
    "\n",
    "*Technical note:* Use `requires_grad=True` when creating tensors, `tensor.backward()` for gradient computation, and `.cpu()` to transfer gradients.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 18:**\n",
    "Create a random tensor on the CPU with shape `(7, 2)` and then transfer it to the GPU to compute its elementwise reciprocal. After computing the reciprocal on the GPU, transfer the result back to the CPU and compare it with a CPU-computed version to ensure that the operation yields the same result on both devices. This task reinforces the understanding of simple elementwise operations and device consistency.\n",
    "\n",
    "*Technical note:* Use `1 / tensor` for computing the reciprocal, and manage device transfers using `.to('cuda')` and `.cpu()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 19:**\n",
    "Create a random tensor on the CPU with shape `(10, 5)`, then transfer it to the GPU to perform a normalization process. Compute the mean and standard deviation on the GPU, subtract the mean, and divide by the standard deviation to normalize the tensor. Finally, transfer the normalized tensor back to the CPU for further inspection. This exercise illustrates how basic normalization techniques can be applied to simple tensors across different devices.\n",
    "\n",
    "*Technical note:* Use `torch.mean()` and `torch.std()` for statistical calculations, perform elementwise operations for normalization, and transfer tensors using `.to(device)` and `.cpu()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 20:**\n",
    "Create a random tensor on the GPU with shape `(6, 6)` and perform a simple logical operation by checking if each element is greater than a specified threshold (e.g., `0.5`). This will produce a boolean mask tensor. Transfer the boolean mask back to the CPU and print it, demonstrating how logical operations on simple tensors can be executed on the GPU and then analyzed on the CPU.\n",
    "\n",
    "*Technical note:* Use a comparison operator (e.g., `tensor > 0.5`) to create the mask, and manage device transitions with `.to('cuda')` and `.cpu()` for proper processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 4. Processing a tensor</span>\n",
    "\n",
    "[Go back to the content](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Variant 1:**\n",
    "Create a random tensor of shape `(10, 10)` using PyTorch and then process it by calculating its mean, variance, and standard deviation. After computing these statistical metrics, display the results to understand the distribution of tensor values. This exercise helps you appreciate how tensor statistics are vital for tasks like normalization and weight initialization in deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.rand((10, 10))` for tensor generation, `torch.mean()`, `torch.var()`, and `torch.std()` to compute statistics, and ensure reproducibility with `torch.manual_seed()` if needed.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 2:**\n",
    "Create a random tensor of shape `(15, 5)` and perform a sorting operation along both the row and column dimensions. Once sorted, print out the sorted tensors and compare the outputs to understand how PyTorch handles tensor sorting. This variant is particularly useful in understanding data preprocessing techniques used in deep learning when ordering or ranking is required.\n",
    "\n",
    "*Technical note:* Use `torch.sort()` specifying the `dim` parameter, and verify results by checking the sorted tensor values. Optionally, use NumPy's sorting functions for cross-verification.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 3:**\n",
    "Create a random tensor with shape `(8, 8)` and process it by applying thresholding. Set values above a threshold of `0.5` to `1` and those equal to or below `0.5` to `0`. Print the original tensor alongside the thresholded result to illustrate how such operations can be used in segmentation tasks or binary classification preprocessing.\n",
    "\n",
    "*Technical note:* Use `torch.where(tensor > 0.5, torch.tensor(1.0), torch.tensor(0.0))` for thresholding, and ensure that the tensor’s data type is compatible with the operation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 4:**\n",
    "Create a random tensor with shape `(2, 3, 4)` and then reshape it into a two-dimensional tensor with shape `(6, 4)`. After reshaping, flatten the tensor into a one-dimensional vector and print both the reshaped and flattened tensors. This task reinforces the concepts of tensor reshaping and flattening, which are crucial in preparing data for neural network layers.\n",
    "\n",
    "*Technical note:* Use the `.view()` or `.reshape()` methods for reshaping and `.flatten()` for flattening the tensor. Verify the tensor dimensions using the `.shape` attribute.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 5:**\n",
    "Create a random tensor of shape `(5, 5)` and apply several elementwise activation functions such as ReLU, Sigmoid, and Tanh to it. Print the outputs side by side to compare how these non-linear transformations alter the tensor values. This variant demonstrates the importance of activation functions in introducing non-linearity in deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.nn.functional.relu()`, `torch.sigmoid()`, and `torch.tanh()` for activation functions. Ensure the tensor is generated with `torch.rand()` or `torch.randn()` and check results with appropriate plotting if desired.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 6:**\n",
    "Create a random tensor of shape `(10, 10)` and use slicing techniques to extract its central sub-tensor (for example, rows 3 to 7 and columns 3 to 7). After extraction, compute the sum and mean of this sub-tensor to understand localized statistics within the data. This task teaches you the importance of indexing and slicing in tensor processing, which is often used in convolution operations and region-based analyses.\n",
    "\n",
    "*Technical note:* Use standard Python slicing (e.g., `tensor[3:8, 3:8]`) and reduction functions like `torch.sum()` and `torch.mean()` to process the sub-tensor.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 7:**\n",
    "Create a random tensor of shape `(12, 4)` and split it into two tensors along the first dimension. Process each half separately by computing their respective means, then concatenate the results back into a single tensor. This exercise demonstrates how tensor splitting and concatenation can be applied in mini-batch processing and parallel data pipelines in deep learning workflows.\n",
    "\n",
    "*Technical note:* Use `torch.split()` or slicing to divide the tensor, and `torch.cat()` to concatenate the results. Verify dimensions with the `.shape` attribute.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 8:**\n",
    "Create two random tensors of identical shape `(7, 7)` and perform elementwise multiplication on them. Then, flatten both original tensors and compute their dot product. Print all intermediate results to illustrate how basic tensor operations, such as multiplication and dot product, are fundamental in implementing operations like attention mechanisms in neural networks.\n",
    "\n",
    "*Technical note:* Use the `*` operator for elementwise multiplication, `.flatten()` to reshape tensors, and `torch.dot()` for dot product computation. Optionally, verify the result with NumPy’s dot function.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 9:**\n",
    "Create a random tensor of shape `(3, 4)` and compute its transpose. Next, perform matrix multiplication between the original tensor and its transpose to obtain a square matrix. This exercise helps you understand how transposition and matrix multiplication interact, which is a foundational concept in many deep learning algorithms, including covariance matrix computations.\n",
    "\n",
    "*Technical note:* Use `tensor.t()` or `tensor.transpose(0, 1)` for transposition, and `torch.mm()` or the `@` operator for matrix multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 10:**\n",
    "Create a one-dimensional random tensor of length `12` and compute its cumulative sum along its only dimension. Following this, calculate the cumulative product to observe how these reduction operations can capture the progressive aggregation of values. This variant is particularly useful in understanding sequential data processing in recurrent neural networks.\n",
    "\n",
    "*Technical note:* Use `torch.cumsum(tensor, dim=0)` for the cumulative sum and `torch.cumprod(tensor, dim=0)` for the cumulative product. Use `.tolist()` to print results in a readable format if needed.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 11:**\n",
    "Create a random integer tensor with shape `(10, 10)` where the values range between 0 and 5. Process the tensor by counting the frequency of a specific value (for example, count how many times the integer `3` appears). This exercise illustrates the use of simple tensor processing for data analysis, which is often necessary for understanding label distributions in classification tasks.\n",
    "\n",
    "*Technical note:* Use `torch.randint(0, 6, (10, 10))` for tensor generation, and `torch.sum(tensor == 3)` to count occurrences. Compare with NumPy’s `np.count_nonzero()` if needed.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 12:**\n",
    "Create a random tensor of shape `(20,)` and perform normalization by subtracting the tensor’s mean and dividing by its standard deviation. After normalization, print the resulting tensor and verify that its new mean is approximately zero. This variant demonstrates how normalization is essential for stable and efficient training of deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.mean()` and `torch.std()` for computing statistics, and perform elementwise operations to normalize the tensor. Optionally, use `sklearn.preprocessing.StandardScaler` for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 13:**\n",
    "Create a random tensor of shape `(8, 8)` and define a custom Python function that squares each element and then adds a constant (for example, `2`). Apply this function elementwise to the tensor and print the original and processed tensors side by side. This task reinforces the concept of applying custom transformations to tensors, a common practice when designing custom layers or loss functions in deep learning.\n",
    "\n",
    "*Technical note:* Use Python’s `lambda` functions or define a custom function, and apply it using either a loop or vectorized operations (e.g., `tensor ** 2 + 2`). Ensure the operation is efficient using PyTorch’s native capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 14:**\n",
    "Create a random tensor of shape `(6, 6)` and process it by computing the softmax along its rows. Then, compute the log-softmax along its columns and print both outputs. This variant illustrates the difference between these two normalization techniques, which are commonly used in classification tasks and in the computation of probabilities in neural networks.\n",
    "\n",
    "*Technical note:* Use `torch.softmax(tensor, dim=1)` for row-wise softmax and `torch.log_softmax(tensor, dim=0)` for column-wise log-softmax. Compare the numerical stability and interpretability of both results.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 15:**\n",
    "Create a random tensor of shape `(100,)` and compute a histogram of its values to visualize its distribution. Use Matplotlib to plot the histogram and analyze how the distribution might affect the initialization of weights in deep learning models. This exercise helps you understand the underlying data distribution, which is essential for effective model training.\n",
    "\n",
    "*Technical note:* Use `torch.rand((100,))` for tensor generation, convert the tensor to a NumPy array with `.numpy()`, and use `matplotlib.pyplot.hist()` to plot the histogram.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 16:**\n",
    "Create a random tensor of shape `(10, 10)` and extract the unique values present in it. After obtaining the unique elements, sort them and print the sorted list. This task demonstrates the use of simple tensor processing for data preprocessing, which is useful for tasks such as identifying unique classes in a dataset or reducing redundancy.\n",
    "\n",
    "*Technical note:* Use `torch.unique(tensor)` to extract unique values and `torch.sort()` to sort them. Optionally, compare with NumPy’s `np.unique()` for validation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 17:**\n",
    "Create a random tensor of shape `(5, 5)` and compute its elementwise square. Then, calculate the mean squared error (MSE) between the original tensor and the squared tensor as a simple loss metric demonstration. This exercise serves as a foundational example of how loss functions can be computed and used in training neural networks.\n",
    "\n",
    "*Technical note:* Use the `** 2` operator for squaring and compute MSE using `torch.mean((tensor - tensor**2)**2)`. This example also reinforces the concept of error measurement in optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 18:**\n",
    "Create a random tensor of shape `(4, 4)` and compute its L1 norm and L2 norm. Print both norms to compare how they quantify the magnitude of the tensor differently. This variant is particularly useful for understanding regularization techniques, such as L1 and L2 regularization, which are widely used in deep learning to prevent overfitting.\n",
    "\n",
    "*Technical note:* Use `torch.norm(tensor, p=1)` for the L1 norm and `torch.norm(tensor, p=2)` for the L2 norm. Optionally, explore higher-order norms using the `p` parameter.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 19:**\n",
    "Create a random tensor of shape `(10, 10)` and apply a boolean mask to select only those elements that exceed a threshold value (for example, `0.7`). Compute the average of the selected elements and print the result along with the masked tensor. This exercise demonstrates how boolean indexing can be used to process and analyze specific portions of tensor data, a technique often useful in deep learning for handling sparse data.\n",
    "\n",
    "*Technical note:* Use the expression `mask = tensor > 0.7` and then `tensor[mask]` to apply the mask, followed by `torch.mean(tensor[mask])` to compute the average.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 20:**\n",
    "Create a random tensor of shape `(8, 8)` and implement a clipping function that limits the tensor’s values to lie within a specified range, such as between `0.2` and `0.8`. After clipping, round the tensor’s values to two decimal places and print the final processed tensor. This task is designed to illustrate how clipping can prevent extreme values that may lead to exploding gradients during deep learning model training.\n",
    "\n",
    "*Technical note:* Use `torch.clamp(tensor, min=0.2, max=0.8)` for clipping and `torch.round(tensor * 100) / 100` for rounding to two decimal places. This approach is similar to techniques used in gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 5. Advancing with a tensor</span>\n",
    "\n",
    "[Go back to the content](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Variant 1:**\n",
    "Create two random tensors to simulate a fully connected layer’s forward pass. First, create a tensor representing input features of shape `(16, 32)` and another tensor representing weights of shape `(32, 10)`. Multiply these tensors using matrix multiplication, then add a bias tensor of shape `(16, 10)`, and finally apply a ReLU activation function to the result. Print the final output along with its shape to verify the dimensions. This exercise simulates a typical neural network layer operation, reinforcing the importance of matrix operations and non-linear activations in deep learning.\n",
    "\n",
    "*Technical note:* Use `torch.rand` for tensor generation, the `@` operator or `torch.matmul` for matrix multiplication, `torch.add` for bias addition, and `torch.nn.functional.relu` for the activation function. Also, use `torch.manual_seed` for reproducibility if desired.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 2:**\n",
    "Create two random tensors with shapes `(8, 4)` and `(8, 4)` to simulate two sets of predictions. Compute their elementwise difference, then square the difference, and finally compute the mean squared error (MSE) between the two tensors. Print both the intermediate difference tensor and the final MSE value. This variant demonstrates how simple tensor arithmetic can be used to compute a common loss metric in deep learning training routines.\n",
    "\n",
    "*Technical note:* Use elementwise subtraction (`tensor1 - tensor2`), the `** 2` operator for squaring, and `torch.mean()` to compute the MSE.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 3:**\n",
    "Create a random tensor of shape `(3, 4, 5)` to simulate multi-dimensional data. Reshape the tensor into a two-dimensional tensor of shape `(12, 5)` and then compute the sum across the rows to reduce it to a one-dimensional tensor. Print both the reshaped tensor and the resulting reduced tensor. This exercise reinforces the concepts of reshaping and reduction, which are essential when adapting data for fully connected layers in deep neural networks.\n",
    "\n",
    "*Technical note:* Use `tensor.view()` or `tensor.reshape()` for reshaping and `torch.sum(tensor, dim=0)` for the reduction operation.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 4:**\n",
    "Create two random tensors each of shape `(5, 5)` and concatenate them along a new dimension to form a three-dimensional tensor of shape `(2, 5, 5)`. Then, flatten this tensor into a one-dimensional tensor and print both the concatenated tensor and the flattened result. This task teaches you how to combine multiple tensors along a new axis and prepare them for subsequent linear operations, a common requirement in deep learning data pipelines.\n",
    "\n",
    "*Technical note:* Use `torch.cat` with `dim=0` (or use `torch.unsqueeze` to add a new dimension before concatenation) and `torch.flatten()` for flattening the tensor.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 5:**\n",
    "Create a random tensor of shape `(6, 6)` and define a custom function that computes the square of each element and then adds a constant (e.g., `3`). Apply this function elementwise to the tensor and print the original as well as the transformed tensor. This variant highlights how custom elementwise transformations can be implemented, which is often useful for designing novel activation functions or preprocessing steps in neural networks.\n",
    "\n",
    "*Technical note:* Use vectorized operations like `tensor ** 2 + 3` or a lambda function applied directly to the tensor for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 6:**\n",
    "Create a random tensor with shape `(10, 10)` and implement a custom thresholding function that sets all elements below `0.4` to zero while leaving others unchanged. Use tensor indexing or `torch.where` to perform this operation. Print both the original and thresholded tensors to verify that the thresholding has been applied correctly. This exercise is useful for learning how to filter and process data based on conditionals in a deep learning context.\n",
    "\n",
    "*Technical note:* Use `torch.where(tensor < 0.4, torch.tensor(0.0), tensor)` to perform the thresholding efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 7:**\n",
    "Create a random tensor of shape `(8, 3)` representing a batch of samples with three features each. Normalize the tensor along the feature axis so that each column has zero mean and unit variance. After normalization, print the tensor along with its computed means and standard deviations for verification. This operation is critical for preparing data before training deep neural networks, as normalization often leads to improved convergence.\n",
    "\n",
    "*Technical note:* Use `torch.mean(tensor, dim=0)` and `torch.std(tensor, dim=0)` to compute statistics, then apply normalization with `(tensor - mean) / std`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 8:**\n",
    "Create a one-dimensional random tensor of length `12` and compute both its cumulative sum and cumulative product along its sole dimension. Print the original tensor, the cumulative sum, and the cumulative product. This exercise demonstrates how cumulative operations can be applied to tensors to track progressive aggregations, a technique that is useful in sequential data analysis and recurrent neural network applications.\n",
    "\n",
    "*Technical note:* Use `torch.cumsum(tensor, dim=0)` for the cumulative sum and `torch.cumprod(tensor, dim=0)` for the cumulative product.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 9:**\n",
    "Create a random tensor of shape `(10, 10)` and simulate dropout by randomly zeroing out 30% of its elements. Create a dropout mask using a random tensor comparison and multiply the original tensor by this mask. Print the dropout mask and the resulting tensor to observe the regularization effect, which is widely used to prevent overfitting in deep learning models.\n",
    "\n",
    "*Technical note:* Use `mask = (torch.rand(tensor.shape) > 0.3).float()` and then apply the mask with `tensor * mask`.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 10:**\n",
    "Create a random tensor of shape `(7, 3)` and compute its transpose to obtain a tensor of shape `(3, 7)`. Then, perform a matrix multiplication between the original tensor and its transpose to yield a square matrix of shape `(7, 7)`. Print the resulting matrix and discuss how such operations are fundamental in constructing covariance matrices or similarity measures in deep learning.\n",
    "\n",
    "*Technical note:* Use `tensor.t()` or `tensor.transpose(0, 1)` for transposition and `torch.matmul(tensor, tensor.t())` for matrix multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 11:**\n",
    "Create a random tensor of shape `(5, 5)` ensuring all values are positive by using `torch.rand`. Compute the elementwise reciprocal of the tensor and then apply the natural logarithm to the reciprocal. Print the original tensor, its reciprocal, and the log-transformed tensor to observe how these transformations can be useful in tasks such as log-likelihood computations.\n",
    "\n",
    "*Technical note:* Use `1 / tensor` for the reciprocal and `torch.log()` for the logarithm.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 12:**\n",
    "Create a random tensor of shape `(9, 4)` and compute several reduction operations: the total sum, the maximum value, and the index of the maximum element. Print these values along with the original tensor. This variant helps you understand how to extract meaningful summary statistics from tensors, a process that is crucial for monitoring training and debugging deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.sum(tensor)`, `torch.max(tensor)`, and `torch.argmax(tensor)` for these reduction operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 13:**\n",
    "Create a random tensor of shape `(8, 8)` and perform an elementwise comparison against a threshold value (e.g., 0.5) to create a boolean mask. Convert this mask into a binary tensor where `True` becomes `1` and `False` becomes `0`. Print both the boolean mask and the resulting binary tensor. This variant demonstrates how thresholding and binarization are applied in tasks such as image segmentation or anomaly detection in deep learning.\n",
    "\n",
    "*Technical note:* Use `(tensor > 0.5)` to create the boolean mask and `.float()` to convert it to a binary tensor.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 14:**\n",
    "Create a one-dimensional random tensor of length `20` to simulate sequential data and apply a simple moving average filter with a window size of 3. Compute the smoothed tensor by averaging over sliding windows and print both the original and smoothed tensors. This task is particularly useful for preprocessing time-series data before feeding it into recurrent neural networks.\n",
    "\n",
    "*Technical note:* Use a convolutional approach with `torch.nn.functional.conv1d` by reshaping the tensor appropriately, or use slicing and manual averaging, ensuring the window is applied correctly.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 15:**\n",
    "Create a random tensor of shape `(10, 2)` and perform a two-step preprocessing: first, normalize each feature column to have zero mean and unit variance; then, scale the normalized values by a factor of 2 and shift them by adding 1. Print the intermediate normalized tensor and the final scaled tensor. This exercise simulates common data preprocessing steps applied to features before training deep learning models.\n",
    "\n",
    "*Technical note:* Use `torch.mean` and `torch.std` for normalization, followed by elementwise multiplication and addition for scaling and shifting.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 16:**\n",
    "Create a random square tensor of shape `(8, 8)` and extract its diagonal elements using appropriate indexing. Multiply the diagonal elements by a constant factor (e.g., 3) to simulate modifying a weight matrix’s main diagonal, then replace the original diagonal with these scaled values. Print the modified tensor to observe the change. This operation is useful in custom layer designs where altering specific components of a tensor is required.\n",
    "\n",
    "*Technical note:* Use `torch.diag(tensor)` to extract the diagonal and tensor indexing or `torch.diag_embed` to replace the diagonal elements.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 17:**\n",
    "Create a random tensor of shape `(3, 4, 5)` and permute its dimensions to obtain a new shape `(5, 3, 4)`. Then, compute the mean across the first dimension of the permuted tensor to reduce its dimensionality. Print both the permuted tensor and the final mean tensor. This exercise underscores the importance of dimension manipulation and reduction in aligning data formats for various deep learning architectures.\n",
    "\n",
    "*Technical note:* Use `tensor.permute(2, 0, 1)` to rearrange the dimensions and `torch.mean(tensor, dim=0)` to compute the mean along the specified dimension.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 18:**\n",
    "Create a random tensor of shape `(16, 10)` representing a mini-batch of data and manually implement a batch normalization step. Compute the mean and variance along the batch dimension (dim=0), normalize the tensor, and then apply a scaling factor and bias (simulate learned parameters). Print the normalized tensor to observe the effect of batch normalization, a key operation in deep learning to improve training stability.\n",
    "\n",
    "*Technical note:* Use `torch.mean(tensor, dim=0)` and `torch.var(tensor, dim=0)` to compute statistics, and perform the normalization using the formula `(tensor - mean) / sqrt(variance + eps)`, where `eps` is a small constant for numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 19:**\n",
    "Create two random tensors of identical shape `(7, 7)` representing two different feature maps. Compute a weighted average of these tensors by applying weights (e.g., 0.3 for the first tensor and 0.7 for the second tensor). Print the original tensors and the resulting combined tensor, explaining how weighted fusion of features is applied in deep learning architectures such as attention mechanisms or ensemble models.\n",
    "\n",
    "*Technical note:* Use elementwise multiplication and addition, e.g., `result = 0.3 * tensor1 + 0.7 * tensor2`, to perform the weighted averaging.\n",
    "\n",
    "---\n",
    "\n",
    "**Variant 20:**\n",
    "Create two random tensors of shape `(10, 1)`, one simulating predicted outputs and the other simulating ground truth values. Compute the mean squared error (MSE) loss manually by subtracting one tensor from the other, squaring the differences, and averaging the result. Print the computed MSE loss to validate your manual loss computation. This exercise reinforces the fundamentals of loss calculation, a critical step in training deep neural networks.\n",
    "\n",
    "*Technical note:* Use `torch.sub` (or the `-` operator) to compute differences, `**2` for squaring, and `torch.mean()` to average the squared differences."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "00_pytorch_fundamentals_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
