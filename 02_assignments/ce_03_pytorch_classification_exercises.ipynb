{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratory work 3.</center></h1>\n",
    "<h2><center>PyTorch Classification Exercises</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Last name and First name\n",
    "\n",
    "**Варіант:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Dataset creation for binary classification exercises.](#3.1)\n",
    "2. [Task 2. Building a model by subclassing `nn.Module`.](#3.2)\n",
    "3. [Task 3. Setting up loss functions and optimizers.](#3.3)\n",
    "4. [Task 4. Training and testing loops.](#3.4)\n",
    "5. [Task 5. Predictions and plotting decision boundaries.](#3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH7jIZ2SPFee"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 1. Dataset creation for binary classification exercises.</span>\n",
    "\n",
    "[Go back to the content](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:**\n",
    "Create a **dataset of noisy diagonal stripes** in 2D. Specifically, generate 1000 points in the plane where one diagonal band (class 0) is centered around the line $y = x$ with random Gaussian noise, and another diagonal band (class 1) is around $y = x + 5$. Split into train (80%) and test (20%), ensuring balanced classes.\n",
    "\n",
    "*Technical note:*\n",
    "- Use **NumPy** to create an array of x-values and add noise for y-values.\n",
    "- Convert to **PyTorch tensors** with `torch.from_numpy`.\n",
    "- Use `train_test_split` from **scikit-learn**.\n",
    "- The diagonal separation tests the model’s ability to classify offset linear patterns.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Construct a **checkerboard dataset** with 2000 samples. For class 0, sample points where $\\lfloor x \\rfloor + \\lfloor y \\rfloor$ is even, and for class 1, make it odd. Let x and y each span [-10, 10] with random decimal values. Split data 70/30. This dataset emphasizes learning alternating square “tiles.”\n",
    "\n",
    "*Technical note:*\n",
    "- Use integer-floor logic to label classes: if `(floor(x) + floor(y)) % 2 == 0` → class 0, else class 1.\n",
    "- Convert everything into float tensors for training.\n",
    "- This “checkerboard” pattern can challenge simpler linear classifiers.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Generate a **two-spiral dataset** akin to the classic binary spiral problem, but with random scaling. Class 0 forms an inward spiral, class 1 forms an outward spiral. Each spiral has 1000 points. Add random noise to the radial distance. Reserve 80% for training. The spiral shape tests complex decision boundaries.\n",
    "\n",
    "*Technical note:*\n",
    "- Use parametric equations:**\n",
    "$\\theta$ from 0 to $6\\pi$ (or more), radius = $a + b\\theta$.\n",
    "- Add slight random noise to radius and angle.\n",
    "- Convert final arrays to **torch.FloatTensor** and label them carefully.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Build a **two interlocked crescents** dataset (similar to two moons) but scale each crescent differently. Class 0 is a large crescent, class 1 is a smaller one partially overlapping. Generate 1500 points total. Shuffle thoroughly and do a 75/25 split. This setup stresses the classifier’s ability to handle partial overlap.\n",
    "\n",
    "*Technical note:*\n",
    "- Start from scikit-learn’s `make_moons` or your own parametric approach.\n",
    "- Scale one moon by 1.5 and the other by 1.0.\n",
    "- Convert to PyTorch tensors.\n",
    "- Ensure different random seeds for robust variety.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Create a **ring vs. disk** dataset. Class 0 forms a solid disk of radius 3 in 2D, while class 1 forms an annulus (ring) from radius 4 to 6. Sample 1200 points in total, randomly in x-y space, and label by radial distance from origin. This tests radial classification.\n",
    "\n",
    "*Technical note:*\n",
    "- For each point, generate random radius r and angle $\\theta$.\n",
    "- If $r < 3$ → class 0, else if $4 \\leq r \\leq 6$ → class 1.\n",
    "- Convert final data to float, split into train and test sets.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Construct an **imbalanced dataset** of 1000 total points. Use `make_classification` from scikit-learn with 2 classes, but specify weights=[0.9, 0.1]. Let it have 10 features, only 3 of which are informative. Keep 80%/20% split. This emphasizes learning from skewed class distributions.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `make_classification(n_samples=1000, n_features=10, n_informative=3, weights=[0.9, 0.1])`.\n",
    "- Convert X, y to PyTorch tensors with `float()` and `long()`.\n",
    "- Visualize with a basic scatter or dimensionality reduction to see imbalance.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Create a **nonlinear boundary** via the function: if $x^2 + y^2 < 5$, label class 0; else label class 1. Sample 2000 random points in [-5, 5] × [-5, 5]. This forms a circle boundary. Partition 70/30 for train/test. The simple shape is good for novices to test classification.\n",
    "\n",
    "*Technical note:*\n",
    "- Uniformly sample x, y in [-5, 5].\n",
    "- Classify based on radial distance from origin.\n",
    "- Convert to torch tensors.\n",
    "- We can later check if the classifier forms a near-circular decision boundary.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Make a dataset for **XOR logic** in a continuous 2D domain. Sample 2000 points in [-2, 2] × [-2, 2]. Label them 1 if $xy < 0$ (i.e., signs differ) and 0 otherwise. This is the classic XOR pattern but embedded in a continuous space, forcing a non-linear boundary.\n",
    "\n",
    "*Technical note:*\n",
    "- Generate (x, y) by uniform distribution.\n",
    "- Label: class = 1 if x*y < 0, else 0.\n",
    "- Create PyTorch training and test sets.\n",
    "- The XOR pattern is not linearly separable, so a simple linear model struggles.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Construct a **vertical vs. horizontal stripes** dataset. For class 0, draw random points in vertical strips at x ∈ [0,1] and [2,3]. For class 1, draw random points in horizontal strips at y ∈ [1,2] and [3,4]. Create 1000 samples. Mix them, then split 80/20.\n",
    "\n",
    "*Technical note:*\n",
    "- Generate x, y with uniform distributions in designated intervals.\n",
    "- Assign labels by whether point is in the “vertical” or “horizontal” region sets.\n",
    "- Check for possible overlaps if intervals intersect.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Employ **scikit-learn’s `make_blobs`** to form 3 centers but then merge two of them as class 0 and the remaining center as class 1 for a binary classification. Have 1500 samples and some cluster overlap. This merges multi-cluster data into one class vs. a single cluster in the other class.\n",
    "\n",
    "*Technical note:*\n",
    "- `make_blobs(n_samples=1500, centers=3, n_features=2)`.\n",
    "- Suppose cluster 0 & 1 → class 0, cluster 2 → class 1.\n",
    "- Convert arrays to PyTorch float and integer types.\n",
    "- Split 75/25 for training vs. testing.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Generate a **triangular region** for class 0 and an **L-shaped region** for class 1 in the x-y plane. Sample 1200 total points. For the triangle, confine to 0 ≤ x ≤ y ≤ 5. For the L-shape, sample points in a shape covering x from 6 to 8 and y from 0 to 8, plus some margin overlap. Partition 80/20.\n",
    "\n",
    "*Technical note:*\n",
    "- Programmatically filter random x, y to belong to each shape.\n",
    "- Label them accordingly.\n",
    "- Convert to float, form train/test splits.\n",
    "- This geometry-based approach highlights unusual class boundaries.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Create a **Sierpinski-like fractal** region. Label class 0 if the point belongs to one fractal subset, class 1 otherwise. Sample 2000 points in a 2D bounding box. Points close to triangular “holes” get assigned differently than the main region. This fractal complexity challenges the classifier.\n",
    "\n",
    "*Technical note:*\n",
    "- Generate fractal points with iterative Sierpinski triangle logic.\n",
    "- Alternatively, approximate membership with a known formula or repeated transformations.\n",
    "- Convert data to PyTorch and do an 80/20 train/test split.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Use `make_circles` from scikit-learn, but scale each circle differently and shift one circle’s position to ensure partial overlap. Let there be 2000 samples, random noise=0.1. Label one circle class 0, the other class 1. This partial displacement demands more robust classification.\n",
    "\n",
    "*Technical note:*\n",
    "- `make_circles(n_samples=2000, noise=0.1, factor=0.4)` can be a start.\n",
    "- Translate the second circle by shifting x or y coordinates.\n",
    "- Convert to float, label carefully.\n",
    "- Split 70/30 for train/test.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Create **two disjoint sine wave** classes. For class 0, generate points $(x, \\sin(x))$ for x ∈ [0, 6π]. For class 1, do the same but add +1 vertical shift to y. Combine 1500 points with random x sampling, plus small uniform noise in y. The waves remain disjoint, but with near adjacency.\n",
    "\n",
    "*Technical note:*\n",
    "- Use random x ∈ [0, 6π].\n",
    "- Class 0: y=sin(x)+noise, class 1: y=sin(x)+1+noise.\n",
    "- Convert to torch float.\n",
    "- Scatter-plot to check wave offset.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Adopt **3D features** but still keep binary labels from distinct volumes in 3D space. For class 0, points inside a sphere of radius 2. For class 1, points outside that sphere but within radius 3. Generate 2500 points, then flatten into shape [N,3]. This tests multi-dimensional classification skills.\n",
    "\n",
    "*Technical note:*\n",
    "- Randomly sample spherical coordinates or x,y,z in [-3,3], check radius.\n",
    "- Label 0 if radius < 2, else 1 if radius <3.\n",
    "- Convert to float, keep 80/20.\n",
    "- Multi-dimensional data can help illustrate more complex classification boundaries.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Make an **H-shaped** cluster for class 0 (like an uppercase ‘H’ formed by 3 rectangles) and a **U-shaped** cluster for class 1. Each shape has 600 points. Add random noise so the edges are not perfect. This requires a more intricate boundary for the model to learn.\n",
    "\n",
    "*Technical note:*\n",
    "- Programmatically define bounding boxes for the bars of H or U.\n",
    "- Insert random (x, y) in each bar region.\n",
    "- Combine into a single dataset, label, convert to PyTorch.\n",
    "- Shuffle and split as usual.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Construct a **random polygon** in 2D with 8 vertices. Class 0 is inside the polygon, class 1 is outside. Sample 2500 points in the bounding box, label them via a point-in-polygon algorithm (e.g., ray casting). The random shape fosters a more general boundary detection challenge.\n",
    "\n",
    "*Technical note:*\n",
    "- Randomly choose 8 vertices around a circle.\n",
    "- Use a standard “point-in-polygon” check for labeling.\n",
    "- Convert to float, do 75/25 split.\n",
    "- Visualize to confirm correct labeling.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Use **scikit-learn’s `make_classification`** with 20 features but let only 2 be informative, the rest purely noise. Force a 50/50 class distribution but with polynomial or hypercube separation. Let there be 2000 samples. The high number of irrelevant features encourages feature selection or more complex classification methods.\n",
    "\n",
    "*Technical note:*\n",
    "- Example: `make_classification(n_samples=2000, n_informative=2, n_redundant=0, n_features=20, random_state=42)`.\n",
    "- Convert X to float, y to long.\n",
    "- Shuffle and hold out 20% for testing.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Design an **“X-like”** shape: class 0 is points along the lines $y = x$ and $y = -x$ (within ± some thickness). Class 1 is everything else in the bounding box. Generate 2000 total points in [-5,5]². The classifier must identify crossing lines vs. background.\n",
    "\n",
    "*Technical note:*\n",
    "- For each random point, if it’s near y=x or y=-x within some $\\epsilon$, label 0, else 1.\n",
    "- Convert to float, do 80/20 split.\n",
    "- Visualize the crossing lines in the center region.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Create a **stochastic line** separation: choose a random slope m and intercept c each time you generate a batch. For each batch of 300 points, label class 0 if $y < mx + c$, else class 1. Combine multiple random lines into a single dataset of 3000 points. This ensures varied linear boundaries.\n",
    "\n",
    "*Technical note:*\n",
    "- Repeatedly sample (m, c) from uniform or normal distributions.\n",
    "- Generate random x, y in some range, label based on y < mx+c.\n",
    "- Combine all batches into a single big dataset, do train/test.\n",
    "- This fosters generalizing to any linear boundary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMIjxZdzQfPz"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 2. Building a model by subclassing `nn.Module`.</span>\n",
    "\n",
    "[Go back to the content](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:**\n",
    "Subclass `nn.Module` to create a **binary classifier** with two hidden layers. Use **LeakyReLU** in the first hidden layer and **Tanh** in the second. Output layer is a single neuron for a sigmoid-based classification. Emphasize the interplay of different activation functions to see if the model can handle diverse patterns.\n",
    "\n",
    "*Technical note:*\n",
    "- Example module structure: `Linear->LeakyReLU->Linear->Tanh->Linear->Sigmoid`.\n",
    "- Carefully define forward method.\n",
    "- Pass final output through `torch.sigmoid` in forward or apply BCEWithLogitsLoss externally.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Implement an **“all-convolutional”** approach, even for a simple tabular binary classification. Subclass `nn.Module`, add a 1D convolution layer on an expanded input dimension, followed by global average pooling, then a linear output. This unusual approach helps you experiment with convolution on non-image data.\n",
    "\n",
    "*Technical note:*\n",
    "- Reshape inputs to [batch_size, in_channels=1, features].\n",
    "- Use `nn.Conv1d(1, out_channels=4, kernel_size=3)`.\n",
    "- Then an adaptive pooling layer and a final linear layer.\n",
    "- Inspect performance on non-traditional data shape.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Create a **residual fully connected** binary classifier. Subclass `nn.Module` with 3 linear layers, plus a skip connection from the first layer’s input to the final layer’s input. Use ReLU in between. Output dimension = 1. This tests the concept of residual connections for tabular classification.\n",
    "\n",
    "*Technical note:*\n",
    "- The forward pass might do something like:**\n",
    "`x1 = F.relu(self.fc1(x))`\n",
    "`x2 = F.relu(self.fc2(x1) + x)# skip connection`\n",
    "`out = self.fc3(x2)`.\n",
    "- Ensure input dimension matches skip connection.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Design a **model with group normalization** for a small dimensional input. Typically used for images, but you can apply `nn.GroupNorm` to artificially expanded inputs. Subclass `nn.Module`, use a linear layer → group norm → ReLU → linear → output. This encourages exploration of normalization beyond batch norm.\n",
    "\n",
    "*Technical note:*\n",
    "- Use a shape like [batch_size, some_channels, 1] if needed.\n",
    "- `nn.GroupNorm(num_groups=1, num_channels=...)` can approximate layer norm.\n",
    "- Output layer has 1 unit for binary classification.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Implement an **attention-based mechanism** for a simple tabular classification. Subclass `nn.Module` with an attention layer that scores each feature, then aggregates. Use a final linear classification head. This tests implementing a manual attention approach outside of sequences.\n",
    "\n",
    "*Technical note:*\n",
    "- You can learn an attention weight for each feature dimension, e.g., a parameter vector `w_attn`.\n",
    "- Weighted sum of features → pass to a linear layer → sigmoid.\n",
    "- Check dimension consistency in forward pass carefully.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Create a **parallel path** model: Subclass `nn.Module` with two separate linear “arms” that each transform input differently (e.g., different hidden sizes), then concatenate the outputs for a final classification layer. This tests whether combining multiple feature transformations helps classification.\n",
    "\n",
    "*Technical note:*\n",
    "- In `forward`, branch input into two separate linear blocks, then `torch.cat` their outputs along dimension=1.\n",
    "- Apply final classification head.\n",
    "- Mind that each arm output must have the same batch size for concatenation.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Build a **multi-layer perceptron** with **parametric ReLU (PReLU)** instead of standard ReLU. Subclass `nn.Module`. Two hidden layers, each followed by PReLU. The output is a single dimension for binary classification. This **Variant highlights a trainable negative slope for the ReLU.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `nn.PReLU()` after each linear layer.\n",
    "- No skip connections needed, but keep consistent input-output dimension.\n",
    "- Final activation can be a Sigmoid or rely on BCEWithLogitsLoss.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Design a **batch-normalized** binary classifier with dropout. In `__init__`, define two linear layers. After the first linear layer, apply `nn.BatchNorm1d`. Then apply `nn.Dropout(p=0.3)`. Finally, a second linear layer outputs a single logit. The combination addresses internal covariate shift and overfitting.\n",
    "\n",
    "*Technical note:*\n",
    "- The forward pass: `x = self.linear1(x) -> bn -> relu -> dropout -> linear2(x)`.\n",
    "- Use `BatchNorm1d(input_dim)` matching the linear output’s size.\n",
    "- Output a logit for BCEWithLogitsLoss.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Create a **model with gating**. Subclass `nn.Module` that learns a gate from a gating sub-network. Specifically, have a gating linear layer producing a gate between 0 and 1 (via sigmoid), which then multiplies an intermediate hidden representation. This tests gating-based forward logic.\n",
    "\n",
    "*Technical note:*\n",
    "- Let hidden = ReLU(fc1(x)).\n",
    "- gate = Sigmoid(fc_gate(x)).\n",
    "- out = hidden * gate.\n",
    "- Then pass out to a final classification layer.\n",
    "- Carefully define shapes so multiplication is valid.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Subclass `nn.Module` for a **two-branch “ensemble”** inside one model. Each branch is a small feedforward net. Then average both predictions for the final output. This simulates an ensemble approach but in a single neural module. Good for experimentation on combined decision-making.\n",
    "\n",
    "*Technical note:*\n",
    "- Each branch: Linear → ReLU → Linear → output(1).\n",
    "- Final = average of branch1_out + branch2_out.\n",
    "- In practice, you’d likely have logistic output or BCEWithLogitsLoss on the final averaged logit.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Develop a **Gated Recurrent Unit (GRU) classifier** for a binary classification of sequential data. Subclass `nn.Module`. The input is shaped as [batch, seq_len, features]. Use a GRU to produce the final hidden state, then a linear to output a single logit. This shows how to handle sequence inputs for classification.\n",
    "\n",
    "*Technical note:*\n",
    "- In `forward`, pass the input into `nn.GRU`.\n",
    "- Extract the final hidden state, pass to `nn.Linear` for classification.\n",
    "- If dealing with variable lengths, consider padding or pack_sequence.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Construct a **simple Transformer encoder** for binary classification in tabular or short-sequence data. Subclass `nn.Module`. Use `nn.TransformerEncoderLayer` with a specified d_model. Summarize the encoder’s output by taking the [CLS]-like token embedding, then feed to a linear classifier.\n",
    "\n",
    "*Technical note:*\n",
    "- Convert inputs to shape [sequence_length, batch_size, d_model].\n",
    "- Use a single `TransformerEncoderLayer(d_model=...)` or stack them.\n",
    "- Output dimension is 1 for binary classification.\n",
    "- This is a minimal approach to highlight attention-based classification.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Create a **Capsule Network** style classifier for a small 2D input. Subclass `nn.Module`. Extract “capsules” from the input. Even if data is tabular, reshape artificially. Use a dynamic routing or a simplified approach, culminating in two capsule logits for class 0 or 1. This advanced architecture fosters exploration.\n",
    "\n",
    "*Technical note:*\n",
    "- This can be quite complex. You might use a primary capsule layer with matrix transformations.\n",
    "- Then apply a routing algorithm to get final logits.\n",
    "- Usually done for image data, but can be adapted for a small dimension demonstration.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Build a **Neural ODE** binary classifier. Subclass `nn.Module`, implement a small neural ODE block using `torchdiffeq` (if available) or a mock iterative approach. After integration, produce a final hidden state and pass it to a linear head for classification. This is highly experimental for standard classification.\n",
    "\n",
    "*Technical note:*\n",
    "- The ODE function could be a small MLP defining dx/dt.\n",
    "- Use `odeint` from `torchdiffeq`.\n",
    "- Provide an initial state, integrate over some time interval, then decode.\n",
    "- Not standard, but demonstrates advanced PyTorch usage.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Implement a **distillation-friendly** classifier with two submodules: a “teacher” net and a smaller “student” net, both in one class. The forward can output teacher and student logits. The student can be trained with a knowledge-distillation loss. This showcases model distillation in a single PyTorch module.\n",
    "\n",
    "*Technical note:*\n",
    "- The teacher submodule can be bigger.\n",
    "- The student submodule is smaller.\n",
    "- In training, combine standard BCE loss for the teacher with a distillation term (KL divergence) for the student.\n",
    "- This approach is often used to compress models.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Create an **SVM-like** architecture. Subclass `nn.Module` that does a linear transform and outputs a margin. Then in training, apply a hinge loss or your own custom SVM loss. While not purely a typical neural net, it’s a PyTorch module that mimics linear SVM classification.\n",
    "\n",
    "*Technical note:*\n",
    "- Forward pass is `out = self.linear(x)` with no activation.\n",
    "- Use a hinge embedding loss or custom hinge formula: `max(0, 1 - y * out)`.\n",
    "- y is expected to be ±1 in that case.\n",
    "- Evaluate performance accordingly.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Design a **mixture of experts** classifier. Subclass `nn.Module`. Have multiple “expert” linear layers. Also define a gating network to produce weights for each expert’s output. Weighted sum forms the final logit. This pattern can capture different data regimes with separate “expertise.”\n",
    "\n",
    "*Technical note:*\n",
    "- Suppose K experts. Each expert is a linear block.\n",
    "- The gating net outputs K weights (softmax).\n",
    "- Weighted sum of expert outputs → classification logit.\n",
    "- Keep dimension consistency in forward pass.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Build a **dynamic weight sharing** network that uses the same shared hidden layer but different output heads for each class, blending them if needed. Subclass `nn.Module`. You can define a single big hidden transform, then separate output transformations, and combine them with learned gating.\n",
    "\n",
    "*Technical note:*\n",
    "- One large hidden representation, e.g., `h = F.relu(self.shared_fc(x))`.\n",
    "- Then partial heads for class 0 vs. class 1, or a small gating to combine them.\n",
    "- Output final logit.\n",
    "- This encourages interesting multi-branch design.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Create a **graph-based** model with `PyTorch Geometric` or a simple adjacency-based approach. Subclass a GNN layer (like `GCNConv`) for classification of node data in a bipartite or simple graph. The forward pass aggregates neighbor features, then outputs a binary class logit for each node.\n",
    "\n",
    "*Technical note:*\n",
    "- Requires additional library (PyTorch Geometric).\n",
    "- Graph data: define edge_index, node features.\n",
    "- `GCNConv(num_features, hidden_dim)` → ReLU → `GCNConv(hidden_dim, 2)`.\n",
    "- Evaluate classification on node labels.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Implement a **hypernetwork** that generates the weights for a small classifier. Subclass `nn.Module` with a hypernetwork that receives a code vector and outputs the main network’s parameters. Then the main network does the classification. This advanced approach shows meta-learning style.\n",
    "\n",
    "*Technical note:*\n",
    "- The hypernetwork can be a small MLP producing weight/bias for another linear layer.\n",
    "- Reshape these generated parameters and assign them to the child net.\n",
    "- Forward pass: generate parameters → apply them to input → get classification logit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSj97RwyVeFE"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 3. Setting up loss functions and optimizers.</span>\n",
    "\n",
    "[Go back to the content](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:**\n",
    "Use **`nn.BCEWithLogitsLoss`** combined with the **Adam** optimizer. Add a **weight decay** term of 1e-4 to reduce overfitting. This standard approach helps to handle raw logits output for binary classification and ensures partial regularization via weight decay.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.BCEWithLogitsLoss()` expects raw logits from the model’s final layer.\n",
    "- `optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)`.\n",
    "- Weight decay penalizes large weights, improving generalization.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Try **`nn.CrossEntropyLoss`** for binary classification, but treat the output dimension as 2 (logits for class 0 and class 1). Use the **RMSprop** optimizer with lr=0.001. This is an alternative setup that doesn’t require manual sigmoid, since CrossEntropyLoss includes log-softmax internally.\n",
    "\n",
    "*Technical note:*\n",
    "- The model’s final layer should have out_features=2.\n",
    "- `loss_fn = nn.CrossEntropyLoss()` then `y_pred = model(x).argmax(dim=1)` for inference.\n",
    "- RMSprop can help in non-conv tasks if chosen carefully.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Implement a **weighted BCE loss** for imbalanced data. Suppose class 1 is rare, so multiply the positive class with a higher loss weight. Keep the same Adam optimizer but with an initial lr=5e-4. This helps correct for heavily imbalanced classes.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]))` for example.\n",
    "- The pos_weight > 1 penalizes misclassifying positives more.\n",
    "- Use `.to(device)` on that tensor if needed.\n",
    "- Optimize with `Adam`, carefully adjusting learning rate.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Adopt the **hinge loss** for a binary classification scheme with labels ±1. Combine it with the **SGD** optimizer that includes a momentum of 0.9. This replicates a classic SVM-like approach in PyTorch. The hinge measure can sometimes yield different margin-based behavior.\n",
    "\n",
    "*Technical note:*\n",
    "- If your outputs are logits, hinge = mean( max(0, 1 - y*logit) ).\n",
    "- You can implement your own or use `nn.HingeEmbeddingLoss()`.\n",
    "- For momentum: `torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)`.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Experiment with **`nn.SoftMarginLoss`**, which uses a logistic function internally. Set up the output as 1 logit per sample. Employ the **Adamax** optimizer for robust gradient scaling on simpler tasks. This is a neat alternative to BCE for binary classification with certain activation assumptions.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.SoftMarginLoss()`, ensure targets are ±1.\n",
    "- For the optimizer: `torch.optim.Adamax(model.parameters(), lr=1e-3)`.\n",
    "- Check that model outputs raw scores, and labels are ±1.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Use a **custom log-cosh loss** for binary classification, applying a logistic transform to predictions. Combine that with **SGD**. The log-cosh can be more robust to outliers. The custom function might be $\\text{logcosh}(p - t)$ if p is the predicted probability. Keep the code neat and documented.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement log-cosh in PyTorch: `loss = torch.log(torch.cosh(pred - target))`.\n",
    "- Might need to transform model logits to probabilities first.\n",
    "- Summation or mean across the batch.\n",
    "- Manually define or wrap in a custom `nn.Module`.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Employ **smooth L1 loss** for a regression-style approach to classification, forcing the model to output a 0 or 1 target. Then use a threshold to decide class. Combine with the **Adagrad** optimizer. While nonstandard for classification, it demonstrates exploring non-cross-entropy losses.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.SmoothL1Loss()`.\n",
    "- Targets should be float 0.0 or 1.0.\n",
    "- For final classification, threshold p>0.5.\n",
    "- `optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-2)` might adapt well.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Try a **Dice loss** approach for binary classification to handle extremely imbalanced data. This is popular in segmentation but can be adapted. Combine with the **NAdam** optimizer, which is Adam with Nesterov momentum. This fosters quick adaptation in some datasets.\n",
    "\n",
    "*Technical note:*\n",
    "- Dice loss formula ~ $1 - \\frac{2 * \\text{intersection}}{\\text{sum of volumes}}$.\n",
    "- For binary classification, adapt intersection = sum of p*t.\n",
    "- Implement custom forward inside a loss class or as a function.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Adopt a **focal loss** function with $\\alpha = 0.8$ and $\\gamma = 2$ to penalize misclassified or hard samples more. Use the **AdaDelta** optimizer for its dynamic learning rate. This approach is common in object detection but can help general binary classification with class imbalance.\n",
    "\n",
    "*Technical note:*\n",
    "- Focal Loss: $\\text{FL}(p, y) = -\\alpha (1-p)^\\gamma y \\log(p) - (1-\\alpha) p^\\gamma (1-y)\\log(1-p)$.\n",
    "- Implement in PyTorch manually or find a focal loss library.\n",
    "- `optimizer = torch.optim.Adadelta(model.parameters())`.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Implement a custom **KL divergence**-based approach. Transform the model’s output into a distribution over [0,1], and treat the label as a distribution as well (one-hot vs. zero-hot for the single dimension). Then use `F.kl_div` in PyTorch. Use RMSprop with a smaller learning rate for stability.\n",
    "\n",
    "*Technical note:*\n",
    "- For a single binary dimension, interpret probability p vs. (1-p).\n",
    "- Label distribution might be [1,0] for class 0 vs. [0,1] for class 1.\n",
    "- Carefully handle shape mismatch; `F.log_softmax` might help.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Use a **Binary Contrastive Loss** that encourages pairs of samples from the same class to be closer than pairs from different classes. Then define a custom optimizer or standard Adam. This transforms your classification task into a distance-based approach, though you’ll need pairs or triplets of data.\n",
    "\n",
    "*Technical note:*\n",
    "- Contrastive Loss formula: `Y * D^2 + (1-Y) * max(0, margin - D)^2`, where D is distance.\n",
    "- Label Y=1 if same class, 0 if different.\n",
    "- This requires a special data loader or pair sampling.\n",
    "- The final classification can be done via threshold on embedding distance.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Experiment with a **label smoothing** approach in BCEWithLogitsLoss. Instead of 0 or 1, use 0.1 and 0.9, for instance. Combine with the **AdamW** optimizer at lr=3e-4 to handle weight decay. This reduces overconfidence for the model and can improve generalization.\n",
    "\n",
    "*Technical note:*\n",
    "- Manually adjust your targets: if label=1, use 0.9; if label=0, use 0.1.\n",
    "- Or implement a small function to handle that.\n",
    "- `AdamW` is `torch.optim.AdamW(...)` with weight decay.\n",
    "- Check performance on overfitted tasks.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Define a **Triplet loss** for a ranking-based binary classification scenario. You can treat each sample from class 0, a similar sample, and a different sample from class 1. Then use a margin-based triplet objective. Combine with `torch.optim.SGD(lr=1e-3, momentum=0.8)`.\n",
    "\n",
    "*Technical note:*\n",
    "- `nn.TripletMarginLoss` or a custom approach.\n",
    "- Data must come in anchor-positive-negative triplets.\n",
    "- The model outputs embeddings, so it’s a metric learning approach.\n",
    "- Final classification can rely on nearest centroid or threshold distance.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Use **Cosine Embedding Loss** for a similarity-based classification. For each pair of examples, compute a target +1 if same class, -1 if different class. Train the model to produce embeddings with correct cos similarity. Then use the **ASGD** optimizer, an averaged **Variant of SGD.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.CosineEmbeddingLoss()`.\n",
    "- The model must produce embedding vectors, not raw class logits.\n",
    "- Pairs must be labeled accordingly.\n",
    "- `torch.optim.ASGD(model.parameters(), lr=1e-3)` can help on some tasks.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Test a **Binary Cross-Entropy with label weighting** but do so in a **cyclical learning rate** schedule with the Adam optimizer. The cyclical LR might be from `torch.optim.lr_scheduler` or a custom approach that oscillates between lower and higher LR.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss_fn = nn.BCELoss(weight=class_weights)` or `pos_weight` if BCEWithLogits.\n",
    "- Add a cyclical scheduler like `torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-2)`.\n",
    "- Step the scheduler each batch or epoch.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Implement a **Huber-like** classification loss that transitions from L1 to L2 near the boundary. Use a custom function that punishes errors around 0.5 more severely. Pair it with `Adam` for moderate learning rates. Although unusual, it can be instructive to see a smoothed absolute error approach in classification.\n",
    "\n",
    "*Technical note:*\n",
    "- Similar to SmoothL1 but define your own threshold around p=0.5.\n",
    "- For predictions < 0.5 and target=1, or predictions > 0.5 and target=0, define a piecewise function.\n",
    "- Summation or mean across the batch.\n",
    "- Check gradient stability.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Combine **binary cross-entropy** with a secondary **L2 penalty** on hidden features (not just weights). This requires hooking into the forward pass to gather intermediate activations. Then add them as a penalty in your total loss. Use `SGD(lr=1e-3)` for interpretability. This approach is a custom form of regularization.\n",
    "\n",
    "*Technical note:*\n",
    "- E.g., hidden = self.hidden_layer(x).\n",
    "- L2 penalty = `lambda * hidden.pow(2).sum()`.\n",
    "- total_loss = bce_loss + penalty.\n",
    "- Must store the hidden activation or add forward hooks.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Use the **`nn.MultiLabelSoftMarginLoss`** for a scenario where you treat each binary class as multi-label with one dimension. It can still function if you manage the shapes carefully. Pair with the **RAdam** optimizer. This reveals how multi-label logic might apply even with binary tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- If each sample has shape [n_classes], for binary that is 1 dimension.\n",
    "- `MultiLabelSoftMarginLoss` can handle it, but watch input vs. target shape.\n",
    "- RAdam is from `torch.optim` (newer versions) or from external libs.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Implement a **dynamic re-weighting** strategy. For every epoch, measure class error rates. If class 0 accuracy is significantly worse, increase its weight in BCE. This is done by updating the pos_weight each epoch. Combine with the standard Adam optimizer. This is a form of dynamic cost-sensitive learning.\n",
    "\n",
    "*Technical note:*\n",
    "- After each epoch, recalc class error ratio.\n",
    "- Adjust `loss_fn.pos_weight` accordingly.\n",
    "- Must re-instantiate or handle carefully to avoid re-creating the entire module.\n",
    "- Keep logs to see if it balances class performance.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Use a **two-stage** approach: first train with standard BCE, then switch to focal loss mid-way if imbalance is detected. Keep the same Adam or RMSprop. This approach tries a more standard training first, then intensifies on hard examples. Good for advanced experimentation.\n",
    "\n",
    "*Technical note:*\n",
    "- Start with `loss_fn = BCEWithLogitsLoss()`.\n",
    "- After N epochs, measure imbalance or performance. Switch to focal loss.\n",
    "- Ensure that code transitions seamlessly.\n",
    "- Keep same optimizer or re-initialize if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvk4PfNTWUAt"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 4. Training and testing loops.</span>\n",
    "\n",
    "[Go back to the content](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Variant 1:**\n",
    "Implement a **standard training loop** with **early stopping**. If the validation loss does not improve for 5 consecutive epochs, stop. Print training and validation loss every epoch. Use the standard forward-backward-update scheme, but keep track of minimal val loss to decide early stop.\n",
    "\n",
    "*Technical note:*\n",
    "- Track best_val_loss.\n",
    "- If `val_loss < best_val_loss`, update best_val_loss and reset a patience counter. Otherwise increment patience.\n",
    "- Break loop when patience is exceeded.\n",
    "- Evaluate on validation set after each epoch.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Use a **custom learning rate scheduler** that reduces LR by half if the validation accuracy plateaus for 3 epochs. Print training accuracy, validation accuracy, and adjust LR accordingly. This dynamic approach can accelerate training early on but slow down if improvement stalls.\n",
    "\n",
    "*Technical note:*\n",
    "- After each epoch, if `val_acc` doesn’t improve for 3 steps, do `current_lr = current_lr / 2`.\n",
    "- Implement manually or use `torch.optim.lr_scheduler.ReduceLROnPlateau`.\n",
    "- Keep training logs for debugging.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Integrate a **gradient clipping** strategy into your loop. For instance, `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)` after the backward pass. This helps manage exploding gradients. Print a brief log each epoch about the total gradient norm or if clipping was triggered.\n",
    "\n",
    "*Technical note:*\n",
    "- Typical code snippet:**\n",
    "`loss.backward()`\n",
    "`nn.utils.clip_grad_norm_(model.parameters(), 2.0)`\n",
    "`optimizer.step()`.\n",
    "- Watch how it changes stability if your dataset or architecture is complex.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Implement a **multi-phase** training loop. In the first 5 epochs, freeze certain layers (e.g., the first hidden layer). Unfreeze them afterwards for fine-tuning. Print separate losses/accuracies for each phase. This simulates transfer learning or partial layer freezing to speed up initial training.\n",
    "\n",
    "*Technical note:*\n",
    "- E.g., `for param in model.layer1.parameters(): param.requires_grad = False`.\n",
    "- Then after some epochs, set requires_grad back to True.\n",
    "- Evaluate each phase’s metrics, track them.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Use a **two-optimizer** approach. For instance, have one optimizer (SGD) for the first hidden layers, another (Adam) for the final layers. In the loop, call each optimizer’s step separately. This can happen if different parts of the network require different training dynamics.\n",
    "\n",
    "*Technical note:*\n",
    "- `optimizer1 = torch.optim.SGD(model.part1.parameters(), lr=1e-3)`\n",
    "`optimizer2 = torch.optim.Adam(model.part2.parameters(), lr=1e-4)`.\n",
    "- In training:**\n",
    "`loss.backward()`\n",
    "`optimizer1.step(); optimizer2.step()`.\n",
    "- Keep watch for partial updates in each submodule.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Create a **logging hook** with TensorBoard or MLflow. Log training loss, validation loss, accuracy every epoch, plus any custom metrics. Over 50 epochs, examine how they evolve. This synergy with a logging platform can ease debugging. The loop references the logging after each epoch.\n",
    "\n",
    "*Technical note:*\n",
    "- `from torch.utils.tensorboard import SummaryWriter` or MLflow.\n",
    "- `writer.add_scalar('Loss/train', train_loss, epoch)` etc.\n",
    "- Ensure to close writer at the end.\n",
    "- Helps produce real-time graphs of your metrics.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Integrate **automatic mixed precision (AMP)** in the training loop. Use `torch.cuda.amp` with a GradScaler. This can speed up training on GPUs and reduce memory usage. You must wrap forward/backward in an autocast context. Print loss in float for clarity each epoch.\n",
    "\n",
    "*Technical note:*\n",
    "- Typical code snippet:**\n",
    "```\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for inputs, targets in train_loader:\n",
    "with torch.cuda.amp.autocast():\n",
    "outputs = model(inputs)\n",
    "loss = loss_fn(outputs, targets)\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "- Evaluate correctness carefully.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Implement a **cyclical batch size** approach in your loop: start with a smaller batch size, then gradually increase it. This is unusual but might help in some scenarios. For example, double the batch size every 10 epochs. Observe if it speeds up training or affects convergence.\n",
    "\n",
    "*Technical note:*\n",
    "- You may need a flexible DataLoader that can change batch size on the fly or reinitialize the loader.\n",
    "- The rest of the training loop remains standard.\n",
    "- Compare final accuracy with a fixed-batch approach.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Embed a **stochastic weight averaging (SWA)** step in your loop. After your main training finishes, do SWA for the last 10 epochs by averaging model weights. This often improves generalization. Print final test metrics with the SWA model. The loop might freeze the LR or reduce it significantly in SWA phase.\n",
    "\n",
    "*Technical note:*\n",
    "- `torch.optim.swa_utils` has `AveragedModel` and `update_parameters`.\n",
    "- Typically run normal training, then run additional epochs with SWA.\n",
    "- Evaluate the SWA model for final performance.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Do a **k-fold cross-validation** within the training loop. Rather than a single train/test, break your dataset into k folds, train on k-1 folds, validate on 1 fold, repeat. Finally average metrics. Print the per-fold accuracy each time. This extends your loop to multiple sub-trainings.\n",
    "\n",
    "*Technical note:*\n",
    "- Use scikit-learn’s `KFold` or `StratifiedKFold`.\n",
    "- For each fold, build a new model or re-init the same model.\n",
    "- Accumulate average metrics.\n",
    "- This is more time-consuming but robust.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Design a **gradient accumulation** loop. If your GPU can’t handle large batch sizes, accumulate gradients over multiple mini-batches before calling `optimizer.step()`. For example, accumulate for 4 mini-batches to simulate a 4x bigger batch. Print partial losses but step the optimizer only after the 4th mini-batch.\n",
    "\n",
    "*Technical note:*\n",
    "- `accumulate_steps = 4`.\n",
    "- In the loop, do `loss.backward()` each iteration.\n",
    "- After `accumulate_steps` iterations, `optimizer.step()`, `optimizer.zero_grad()`.\n",
    "- Check for floating precision issues or gradient overflow.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Implement a **custom training loop** that monitors the **F1 score** on validation. If the F1 score drops for 3 consecutive epochs, reduce the LR by factor 0.5. Print out the confusion matrix for the validation set every 5 epochs. This approach focuses on classification-specific metrics, not just raw accuracy.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torchmetrics` or a custom F1 function.\n",
    "- Keep track of best F1.\n",
    "- If it worsens 3 times in a row, adjust LR or run a schedule.\n",
    "- Log confusion matrix to console or file for analysis.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Add a **regular evaluation on an external hold-out** (like a test set you never use for early stopping or hyperparameter tuning). Only run it once every 5 epochs or at the very end. This mimics real-world separate validation vs. test usage. Print final test metrics only if the model is stable.\n",
    "\n",
    "*Technical note:*\n",
    "- `for epoch in range(epochs):` do train, do val.\n",
    "- If `epoch % 5 == 0`, run test_inference but do not use it to adjust training.\n",
    "- Helps avoid overfitting the hyperparameters to the validation set.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "In your loop, implement **mixup** data augmentation for tabular or image data. For each pair of samples, produce a weighted combination of inputs and labels. Then compute the loss with the combined label. This can improve generalization for certain tasks. Print a sample of mixup inputs every epoch.\n",
    "\n",
    "*Technical note:*\n",
    "- mixup: $(x1, y1)$, $(x2, y2)$, $\\lambda \\sim Beta(\\alpha, \\alpha)$.\n",
    "- new_x = $\\lambda x1 + (1-\\lambda) x2$.\n",
    "- new_y = $\\lambda y1 + (1-\\lambda) y2$.\n",
    "- Then standard forward/backward with new_x, new_y.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Create a loop for **curriculum learning**. Start with simpler examples (lower noise or smaller input magnitude). After 3 epochs, expand to moderate difficulty, then full difficulty. This organizes the data from easy to hard. Print the portion of “easy” vs. “hard” samples each epoch for transparency.\n",
    "\n",
    "*Technical note:*\n",
    "- Requires your dataset to label sample difficulty.\n",
    "- Sort or group data by difficulty.\n",
    "- Each stage, unfreeze more data.\n",
    "- Evaluate if the model’s final performance improves over a random data order.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Add a **checkpointing** mechanism that saves the model’s state_dict whenever the validation loss is at a new minimum. If training is interrupted, you can resume from the last checkpoint. This is standard for large runs. Print “checkpoint saved!” messages.\n",
    "\n",
    "*Technical note:*\n",
    "- `if val_loss < best_loss: best_loss = val_loss; torch.save(model.state_dict(), 'best_model.pt')`.\n",
    "- To resume, load with `model.load_state_dict(torch.load('best_model.pt'))`.\n",
    "- Also store epoch number, optimizer state if needed.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Build a **multi-GPU** training loop with `DataParallel` or `DistributedDataParallel`. Print the local rank each epoch to show that multiple GPUs are working. This step is advanced but beneficial for large-scale classification tasks. At the end, gather results for the final test metrics.\n",
    "\n",
    "*Technical note:*\n",
    "- For quick usage, `model = nn.DataParallel(model)`.\n",
    "- For more control, `torch.distributed` is used.\n",
    "- The rest of the training loop mostly remains the same, except for data loaders and sync steps.\n",
    "- Confirm that performance scales.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "In your training loop, do a **per-parameter group** schedule. For instance, the first half of the layers use a different learning rate than the latter half. Print each group’s learning rate every epoch. This can help if early layers need smaller adjustments.\n",
    "\n",
    "*Technical note:*\n",
    "- `optimizer = torch.optim.Adam([ {'params': model.layer1.parameters(), 'lr':1e-4}, {'params': model.layer2.parameters(), 'lr':1e-3} ])`.\n",
    "- The loop calls `optimizer.step()` as usual.\n",
    "- Monitor potential differences in layer convergence.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Implement a **teacher forcing** style approach in classification if you have partial label sequences or some iterative model. For each step, feed ground-truth from the previous step with a certain probability. Print the ratio of teacher forcing vs. free running. This is more typical for RNN tasks, but it can appear in multi-step classification.\n",
    "\n",
    "*Technical note:*\n",
    "- Probability p to use ground-truth label as input, else use predicted label.\n",
    "- Often used in sequence tasks with RNN or LSTM.\n",
    "- The training loop must handle random decisions per step.\n",
    "- Helps bridging from fully supervised to learned transitions.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Create a **semi-supervised** loop: you have labeled data and unlabeled data. At each epoch, do a standard supervised pass on the labeled set, then generate pseudo-labels for unlabeled data, optionally filtering out low-confidence predictions. Print how many unlabeled samples get included. This approach can expand the training dataset.\n",
    "\n",
    "*Technical note:*\n",
    "- Train on labeled data with BCE or CE.\n",
    "- For unlabeled, do a forward pass, gather predictions above some confidence threshold.\n",
    "- Add them to the training set with pseudo-label.\n",
    "- This is a form of self-training or pseudo-labeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:37.404716Z",
     "start_time": "2024-03-19T10:44:37.390716Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgnFdlamd2-D",
    "outputId": "627d8c33-071e-4925-f18b-5d5ba6126729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "Pred probs:\n",
      "Pred labels:\n"
     ]
    }
   ],
   "source": [
    "# What's coming out of our model?\n",
    "\n",
    "# logits (raw outputs of model)\n",
    "print(\"Logits:\")\n",
    "## Your code here ##\n",
    "\n",
    "# Prediction probabilities\n",
    "print(\"Pred probs:\")\n",
    "## Your code here ##\n",
    "\n",
    "# Prediction labels\n",
    "print(\"Pred labels:\")\n",
    "## Your code here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:39.053111Z",
     "start_time": "2024-03-19T10:44:37.406716Z"
    },
    "id": "rUSDNHB4euoJ"
   },
   "outputs": [],
   "source": [
    "# Let's calculate the accuracy using accuracy from TorchMetrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "## TODO: Uncomment this code to use the Accuracy function\n",
    "# acc_fn = Accuracy(task=\"multiclass\", num_classes=2).to(device) # send accuracy function to device\n",
    "# acc_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:39.069139Z",
     "start_time": "2024-03-19T10:44:39.054081Z"
    },
    "id": "SHBY3h7XXnxt"
   },
   "outputs": [],
   "source": [
    "## TODO: Uncomment this to set the seed\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Setup epochs\n",
    "\n",
    "\n",
    "# Send data to the device\n",
    "\n",
    "\n",
    "# Loop through the data\n",
    "# for epoch in range(epochs):\n",
    "  ### Training\n",
    "  \n",
    "\n",
    "  # 1. Forward pass (logits output)\n",
    "  \n",
    "  # Turn logits into prediction probabilities\n",
    "  \n",
    "\n",
    "  # Turn prediction probabilities into prediction labels\n",
    "  \n",
    "\n",
    "  # 2. Calculaute the loss\n",
    "  # loss = loss_fn(y_logits, y_train) # loss = compare model raw outputs to desired model outputs\n",
    "\n",
    "  # Calculate the accuracy\n",
    "  # acc = acc_fn(y_pred, y_train.int()) # the accuracy function needs to compare pred labels (not logits) with actual labels\n",
    "\n",
    "  # 3. Zero the gradients\n",
    "  \n",
    "\n",
    "  # 4. Loss backward (perform backpropagation) - https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights.\n",
    "  \n",
    "  # 5. Step the optimizer (gradient descent) - https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression) \n",
    "  \n",
    "\n",
    "  ### Testing\n",
    "  # model_0.eval() \n",
    "  # with torch.inference_mode():\n",
    "    # 1. Forward pass (to get the logits)\n",
    "    \n",
    "    # Turn the test logits into prediction labels\n",
    "    \n",
    "\n",
    "    # 2. Calculate the test loss/acc\n",
    "    \n",
    "\n",
    "  # Print out what's happening every 100 epochs\n",
    "  # if epoch % 100 == 0:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Nwihtomj9JO"
   },
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 5. Predictions and plotting decision boundaries.</span>\n",
    "\n",
    "[Go back to the content](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Variant 1:**\n",
    "After training, generate **predictive probabilities** and visualize them on a 2D mesh grid. Plot the color-coded probability for class 1, from 0 to 1. Then overlay the original data points. This clarifies how confident the model is across the plane.\n",
    "\n",
    "*Technical note:*\n",
    "- Evaluate on a grid of x,y coordinates: `model(torch.from_numpy(grid))`.\n",
    "- Use `torch.sigmoid` on outputs to get probabilities.\n",
    "- Reshape and use `plt.contourf` to display.\n",
    "- A separate color for data points helps interpret mismatch.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Compute a **confusion matrix** after predictions. Plot it with `matplotlib` or `seaborn` as a heatmap, showing true vs. predicted classes. Display the numeric counts in each cell. This step is essential for analyzing misclassifications, especially with imbalances.\n",
    "\n",
    "*Technical note:*\n",
    "- `y_pred = (logits > 0).int()` or argmax if 2-output.\n",
    "- Compare with `y_true` to fill a 2×2 matrix for binary classification.\n",
    "- `seaborn.heatmap(confusion_matrix, annot=True)` helps visual clarity.\n",
    "- Combine with standard accuracy or F1 metrics.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Plot an **ROC curve**. Compute true positive rate vs. false positive rate at multiple thresholds. Then show the **AUC**. This offers deeper insight beyond raw accuracy. Display the points or a smoothed line. Use scikit-learn’s `roc_curve` and `auc` for convenience.\n",
    "\n",
    "*Technical note:*\n",
    "- Get predicted probabilities from model.\n",
    "- `fpr, tpr, thresholds = roc_curve(y_true, y_prob)`\n",
    "- `auc_val = auc(fpr, tpr)`.\n",
    "- Plot with `plt.plot(fpr, tpr)`.\n",
    "- The diagonal line at 0.5 indicates random guessing.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Generate a **Precision-Recall curve** with scikit-learn. This is crucial for highly imbalanced tasks. Plot precision vs. recall across different thresholds. Then compute the area under the curve (average precision). Print that as well.\n",
    "\n",
    "*Technical note:*\n",
    "- `precision, recall, thresholds = precision_recall_curve(y_true, y_prob)`.\n",
    "- `ap_val = average_precision_score(y_true, y_prob)`.\n",
    "- Use `plt.plot(recall, precision)`.\n",
    "- For severely imbalanced data, PR curves can be more informative than ROC.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Visualize the **decision boundary in 3D** if your dataset has 2 features plus you embed the model’s logit as the z-axis. This can produce a 3D surface plot showing how the logit changes across x,y. Then color the plane by sign of the logit. Useful for advanced boundary illustrations.\n",
    "\n",
    "*Technical note:*\n",
    "- Evaluate model(logits) on a 2D mesh.\n",
    "- Reshape for a surface: z = logits.\n",
    "- `ax.plot_surface(X, Y, Z, cmap='viridis')`.\n",
    "- Use the sign or threshold to color class 0 vs. 1.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Implement a **prediction-time data augmentation** for each test sample, e.g., input transformations, and average predictions. Then plot how these augmentations affect the final decision boundary. This approach (Test-Time Augmentation) can improve stability.\n",
    "\n",
    "*Technical note:*\n",
    "- For each point, apply small random noise or shifts K times.\n",
    "- Average the model’s predicted probability.\n",
    "- Then plot the boundary as usual with a mesh approach.\n",
    "- This can smooth out overly sharp boundaries.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "**Plot feature importance** by systematically occluding or zeroing out each input dimension and measuring the drop in classification probability for the correct class. Then display a bar chart of the drop. Combine that with the usual 2D boundary plot if your dataset has 2 features.\n",
    "\n",
    "*Technical note:*\n",
    "- For each feature i, set it to 0 or random baseline for all samples.\n",
    "- Measure average drop in predicted probability for the correct label.\n",
    "- Sort features by that drop.\n",
    "- This is a manual approach to “feature ablation.”\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Use **Grad-CAM** style visualization if you have at least one convolutional layer. For each test image (if it’s an image classification scenario), show where the network focuses. Then overlap that heatmap with the original image. Good for interpretability.\n",
    "\n",
    "*Technical note:*\n",
    "- Hook into the last conv layer, record gradients w.r.t. the predicted class.\n",
    "- Compute the average gradient across channels.\n",
    "- Multiply by feature map, apply ReLU.\n",
    "- Upsample to original image size.\n",
    "- Overlay the heatmap.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Create a **density plot** of the predicted probabilities for each class label. For every sample, get p = model output (sigmoid). Then separate them by class 0 or class 1, plotting the distribution of p. This shows how well-separated the model is in probability space.\n",
    "\n",
    "*Technical note:*\n",
    "- Gather predictions on the entire test set.\n",
    "- For class 0 samples, collect predicted p. For class 1 samples, collect predicted p.\n",
    "- Use something like `sns.kdeplot` or `plt.hist` over them.\n",
    "- Overlap them to see distribution overlap.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Perform a **threshold sweep** plot. For thresholds from 0.0 to 1.0, compute accuracy (or F1) on the test set. Plot threshold vs. metric. This clarifies whether 0.5 is indeed the best threshold, or if a different boundary might yield better performance in certain tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- For each threshold t in np.linspace(0,1,100):**\n",
    "- pred_class = (prob >= t)\n",
    "- measure accuracy or F1.\n",
    "- Plot the curve.\n",
    "- Possibly pick the threshold that maximizes F1.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Use a **decision tree** or a simple rule-based method to approximate the model’s learned boundary. Then overlay the approximate boundary with the real neural net boundary. Compare them visually to see how well a simpler rule set might mimic the net’s decisions.\n",
    "\n",
    "*Technical note:*\n",
    "- Sample many points in the feature space, label them via the net’s predicted class.\n",
    "- Fit a small scikit-learn `DecisionTreeClassifier` to that data.\n",
    "- Plot both boundaries using the same mesh approach.\n",
    "- This is a method for model distillation/interpretation.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Plot the **training trajectory** of a single test sample’s predicted probability over epochs. In each epoch, do an inference pass on that sample, store the predicted probability, and see how it evolves. Then after training ends, show it in a line graph. This highlights how one example is “learned.”\n",
    "\n",
    "*Technical note:*\n",
    "- Within your training loop, after each epoch, do `p = model(x_test_sample)`.\n",
    "- Keep track in a list.\n",
    "- After finishing, do `plt.plot(range(epochs), p_values)`.\n",
    "- Possibly do multiple samples for comparison.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "After obtaining predictions, compute and plot a **calibration curve** (reliability diagram). For predicted probability bins, measure the average predicted probability vs. actual fraction of positives. This reveals whether the model is under/overconfident. Print the expected calibration error (ECE).\n",
    "\n",
    "*Technical note:*\n",
    "- Sort predictions into bins (e.g., 10 bins).\n",
    "- For each bin, compute mean predicted prob and fraction of positives.\n",
    "- Plot them on x-y where x=mean predicted, y=actual fraction.\n",
    "- Perfect calibration is a diagonal line.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Perform a **class separation** visualization in a 2D projection. If data has >2 features, use PCA or t-SNE to reduce to 2D, color by predicted label. Then highlight misclassified points in red circles. This helps see where the net confuses classes in a compressed space.\n",
    "\n",
    "*Technical note:*\n",
    "- `from sklearn.decomposition import PCA`.\n",
    "- Transform your test set X → X2D.\n",
    "- Plot with different markers or colors for predicted classes vs. ground truth.\n",
    "- Mark misclassifications distinctively.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "In an **active learning** scenario, after an initial model is trained, let the model predict on an unlabeled pool. Plot the highest-uncertainty samples (closest to 0.5 probability) in a scatter plot with a special marker. Then you might label them to improve training. The plot reveals uncertain regions.\n",
    "\n",
    "*Technical note:*\n",
    "- Evaluate p = model(x_pool).\n",
    "- Sort by abs(p - 0.5).\n",
    "- Plot top K uncertain points in a different color or shape.\n",
    "- This is a basis for active learning loops.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Generate a **bootstrap interval** for predictions. Train multiple models with different seeds, or do repeated splits. Then for each test point, gather all model predictions and compute the average and variance. Plot the average boundary and a “confidence band.” This requires multiple runs but is instructive.\n",
    "\n",
    "*Technical note:*\n",
    "- For each run, train a fresh model.\n",
    "- For each point on the grid, gather predictions across runs.\n",
    "- Compute mean ± std.\n",
    "- Plot the mean boundary, optionally shade areas with high std to show disagreement among runs.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Implement a **sensitivity vs. specificity** plot for a binary test set. Then show a particular operating point that maximizes Youden’s J statistic (sensitivity + specificity - 1). Mark that threshold on the curve. This is another specialized approach to threshold selection and result plotting.\n",
    "\n",
    "*Technical note:*\n",
    "- For each threshold, compute sensitivity and specificity.\n",
    "- Plot them vs. threshold.\n",
    "- Youden’s J = (sensitivity + specificity - 1).\n",
    "- The threshold that yields the maximum J is often a good compromise.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Build a **predictive speed or latency** chart. For different mini-batch sizes, measure how many inferences per second the model can achieve. Plot batch size on the x-axis, inferences/second on the y-axis. This is about deployment efficiency, not purely accuracy. It’s crucial in real-time tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- For each batch size in [1, 2, 4, 8, ... 128], measure inference time over a fixed set.\n",
    "- Convert to throughput (samples/sec).\n",
    "- Plot the results.\n",
    "- Illustrates how the model scales with batch size.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "**Visualize shap values** or integrated gradients if your input is tabular. This helps to see how each feature influences the final logit. Then produce a summary bar chart or a beeswarm plot. Tools like shap library can produce these visuals. The net result is a strong interpretability approach.\n",
    "\n",
    "*Technical note:*\n",
    "- For shap, `import shap` then pass your model as a black box.\n",
    "- `explainer = shap.DeepExplainer(model, data)`.\n",
    "- `shap_values = explainer.shap_values(test_data)`.\n",
    "- Summarize with `shap.summary_plot()`.\n",
    "- This clarifies per-feature contribution.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Plot a **likelihood “heatmap”** on each feature dimension for predicted class 1 vs. class 0 in a 1D scenario. For example, if you have only 1 or 2 features, you can show how the net’s probability changes across that dimension. Then overlay actual data points to see alignment.\n",
    "\n",
    "*Technical note:*\n",
    "- Evaluate across a range of feature values (0D or 1D input).\n",
    "- Get predicted probability from the net.\n",
    "- Plot it as a function of x.\n",
    "- Mark real data points with their actual label.\n",
    "- Good for simpler 1D or 2D problems to show direct interpretability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, there are lines of code (optional) that you can utilize to resolve your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:39.085139Z",
     "start_time": "2024-03-19T10:44:39.071140Z"
    },
    "id": "0YRzatb8a1P2"
   },
   "outputs": [],
   "source": [
    "# Plot the model predictions\n",
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "  \n",
    "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    # Source - https://madewithml.com/courses/foundations/neural-networks/ \n",
    "    # (with modifications)\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), \n",
    "                         np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    # Make features\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Test for multi-class or binary and adjust logits to prediction labels\n",
    "    if len(torch.unique(y)) > 2:\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # mutli-class\n",
    "    else: \n",
    "        y_pred = torch.round(torch.sigmoid(y_logits)) # binary\n",
    "    \n",
    "    # Reshape preds and plot\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:39.306152Z",
     "start_time": "2024-03-19T10:44:39.150140Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "tU-UNZsKlJls",
    "outputId": "8b7b745a-070d-4ecb-c639-c4ee4d8eae06"
   },
   "outputs": [],
   "source": [
    "# Code for creating a spiral dataset from CS231n\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:44:39.322153Z",
     "start_time": "2024-03-19T10:44:39.308156Z"
    },
    "id": "OWVrmkEyl0VP"
   },
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "import torch\n",
    "X = torch.from_numpy(X).type(torch.float) # features as float32\n",
    "y = torch.from_numpy(y).type(torch.LongTensor) # labels need to be of type long\n",
    "\n",
    "# Create train and test splits\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNloicnciRwCXd2bJo6F2iS",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_pytorch_classification_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
