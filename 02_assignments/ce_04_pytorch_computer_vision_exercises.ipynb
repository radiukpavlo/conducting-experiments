{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratory work 4.</center></h1>\n",
    "<h2><center>PyTorch Computer Vision Exercises</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Completed:** Last name and First name\n",
    "\n",
    "**Variant:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Load the dataset.](#4.1)\n",
    "2. [Task 2. Turn the loaded dataset into `torch.utils.data.DataLoader`.](#4.2)\n",
    "3. [Task 3. Create a CNN model.](#4.3)\n",
    "4. [Task 4. Train the model on the corresponding dataset.](#4.4)\n",
    "5. [Task 5. Make predictions.](#4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.801709Z",
     "start_time": "2024-03-19T11:08:09.248915Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DNwZLMbCzJLk",
    "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Exercises require PyTorch > 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# TODO: Setup device agnostic code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 1. Load the dataset</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvf-3pODxXYI"
   },
   "source": [
    "**Variant 1:**\n",
    "Load the **Stanford Dogs** dataset from a custom directory structure, ensuring each dog breed has its own subfolder. Resize images to 128×128 and normalize them using ImageNet mean and std. Split into train and validation sets by ensuring each breed is well-represented in both.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torchvision.datasets.ImageFolder` pointing to your dataset directory.\n",
    "- Apply `transforms.Resize(128)` and `transforms.Normalize(mean, std)` for preprocessing.\n",
    "- Maintain an 80/20 split for train/val to ensure coverage of all breeds.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Download the **Tiny ImageNet** dataset (200 classes) and store it locally. Use random transformations like random rotation (±15 degrees) and color jitter to boost variation. Separate the dataset into train/val/test sets, ensuring each set has the same class distribution.\n",
    "\n",
    "*Technical note:*\n",
    "- Rely on `transforms.RandomRotation` and `transforms.ColorJitter` for data augmentation.\n",
    "- Keep the image size at 64×64.\n",
    "- Confirm the ratio splits (e.g., 70% train, 15% val, 15% test).\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Prepare a dataset of **flower images** (e.g., from the Oxford 102 Flowers dataset) by manually downloading and organizing them. Convert all images to grayscale to mimic single-channel data. Then create train/val splits with stratified sampling based on flower type.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `PIL.Image.convert(\"L\")` or `transforms.Grayscale(num_output_channels=1)` for conversion.\n",
    "- Keep a record of class names vs. folder structure.\n",
    "- Possibly reduce the image size to 64×64 to handle memory constraints.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Obtain **satellite imagery** from a public dataset like EuroSAT. Use separate channels (e.g., RGB + near-infrared if available) and stack them into a 4-channel input. Resize them consistently to 128×128. Split data by region: training images from certain zones, validation/test from others.\n",
    "\n",
    "*Technical note:*\n",
    "- Custom transform with `transforms.ToTensor()` handling 4 channels if needed.\n",
    "- This ensures real-world “domain-split” for train/val/test.\n",
    "- Normalization can be channel-wise, using means and stds computed from the dataset.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Load a custom **medical imaging** dataset in PNG format (e.g., X-ray or CT scans). Apply histogram equalization as a preprocessing step for better contrast. Classify into normal vs. abnormal. Keep training and testing sets in separate folders to enforce no patient overlap.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement a custom transform that performs histogram equalization via OpenCV or `PIL`.\n",
    "- Store data in `ImageFolder`-like structure for normal/abnormal.\n",
    "- Carefully verify that no patient ID appears in both sets.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Use the **Caltech-256** dataset for image classification. Implement a custom sampler that selects 5 random categories for the training set and 5 different categories for the validation set for a zero-shot-like scenario. Keep the rest for standard testing.\n",
    "\n",
    "*Technical note:*\n",
    "- Programmatically choose which categories go to train vs. validation.\n",
    "- `torch.utils.data.Subset` or a custom approach to filter classes.\n",
    "- Investigate zero-shot performance on the new classes.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Load the **Places365** dataset in a reduced form (e.g., only 30 scene classes). Convert all images to LAB color space instead of RGB to see if it aids classification. Then split into train/validation/test in equal proportions for each scene type.\n",
    "\n",
    "*Technical note:*\n",
    "- Use an image conversion step: `PIL.Image.convert(\"LAB\")` is not standard, so might need custom code.\n",
    "- Alternatively, transform each image with a library that supports LAB.\n",
    "- Keep balanced splits across the 30 selected classes.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Create a **synthetic shape dataset** of polygons rendered on plain backgrounds. For instance, squares, triangles, circles, each in different colors. Randomly generate 10,000 images (64×64) and label them by shape type. Reserve 2,000 for testing. This tests model ability to learn geometric forms.\n",
    "\n",
    "*Technical note:*\n",
    "- Use a Python script (e.g., with PIL or OpenCV) to draw shapes on blank backgrounds.\n",
    "- Each shape class gets random color, position, and orientation.\n",
    "- Save them as PNG, and load via a custom `Dataset` or `ImageFolder`-style structure.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Load the **iNaturalist** dataset for different species identification. Focus on a subset of bird species (say 10 species). Resize images to 224×224. Use an 80/10/10 split for train/val/test. This large and diverse data challenges real biodiversity classification tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- iNaturalist typically has a large number of classes, so specify a subset index or name filter.\n",
    "- Use `transforms.Resize((224, 224))` plus standard normalization.\n",
    "- Aim for robust coverage of each bird species in all splits.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Obtain the **Food-101** dataset and filter out only 20 classes (dishes). Shuffle images to ensure random distribution. Convert them to 3-channel RGB if any are mismatched. Then apply minimal transformations (crop and horizontal flip) before final storage.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `glob` or `os.listdir` to gather only the chosen classes.\n",
    "- Keep `transforms.RandomResizedCrop(224)` plus `transforms.RandomHorizontalFlip()`.\n",
    "- Store final data objects for subsequent tasks.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Use your **own image collection** from a smartphone camera. Label them in 5 categories (e.g., indoors, outdoors, food, pets, vehicles). Resize each to 256×256. Then use a random 70/30 split for train/val. This fosters a personal dataset with unique image properties.\n",
    "\n",
    "*Technical note:*\n",
    "- Manually sort your images into subfolders.\n",
    "- Use `transforms.Resize((256,256))` or adapt the shape as needed.\n",
    "- Keep a stable random seed for reproducible splits.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Download a **GAN-generated** set of faces vs. real faces for a “real vs. fake” classification. Use 5000 total images, half from a generative model, half from real life. Resize to 128×128. Maintain a 75/25 train/test split to see how well the model discerns authenticity.\n",
    "\n",
    "*Technical note:*\n",
    "- Store “real” and “fake” in separate subdirs, each with 2500 images.\n",
    "- `transforms.Resize(128)` for uniform shape.\n",
    "- Potential advanced transformations to handle face alignment or lighting conditions.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Build a **time-lapse dataset** of sky images over a day, each labeled with “sunrise,” “afternoon,” “sunset,” or “night.” Pre-crop them to remove irrelevant parts. Convert them to grayscale for a simpler channel input. Then do an 80/20 split for training and validation.\n",
    "\n",
    "*Technical note:*\n",
    "- Cropping can remove ground features so the model focuses on sky color patterns.\n",
    "- `transforms.Grayscale()` to reduce complexity.\n",
    "- This tests whether the model can learn time-of-day from sky visuals.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Use **UW Faces** dataset for various face angles. Crop images to the central face region, then scale to 96×96. Partition them by subject ID: 80% of IDs in train, 10% in val, 10% in test, ensuring no subject overlap. The goal is face classification or recognition tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- A custom dataset class might handle face cropping with bounding box info if available.\n",
    "- ID-based splitting ensures the model never sees the same person in multiple sets.\n",
    "- This is relevant for recognition or verification.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Collect **microscopy images** of cells (e.g., brightfield vs. fluorescent). Label them with the cell type. Convert each to a 2D single-channel format. Remove images with poor focus (blurry) using a threshold-based filter. Use a random 80/20 train/test approach.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement a “focus measure” (e.g., variance of Laplacian) to drop blurry images.\n",
    "- Convert to single-channel float for accurate intensity representation.\n",
    "- Potentially do minimal augmentation to maintain cell morphology.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Load a **handwritten digit dataset** from multiple languages (e.g., Arabic, Devanagari). Combine them into a single multi-class set. Use uniform resizing to 32×32. Shuffle thoroughly, and keep language distribution the same across splits so each language appears in train/val/test.\n",
    "\n",
    "*Technical note:*\n",
    "- Rely on multiple source folders, each representing a different script.\n",
    "- Merge them, label each digit 0–9 but also track script if needed.\n",
    "- Ensure balanced splits for each digit–script combination.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Utilize the **Flowers-102** dataset from torchvision for image classification. This dataset contains images of 102 different flower categories. Resize images to 128×128 and normalize them using ImageNet mean and std. Split the dataset into training and validation sets, maintaining an 80/20 split and ensuring each flower category is well-represented in both.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torchvision.datasets.Flowers102` to load the dataset.\n",
    "- Apply `transforms.Resize(128)` and `transforms.Normalize(mean, std)` for preprocessing.\n",
    "- Implement a stratified 80/20 split to ensure class balance across train and validation sets.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Create a **meme classification** dataset from various online memes, labeling them by template type (e.g., Drake Meme, Distracted Boyfriend, etc.). Manually gather ~1,000 images per class. Convert them to 224×224 RGB. Keep a 70/15/15 train/val/test distribution.\n",
    "\n",
    "*Technical note:*\n",
    "- Manually scraping requires filtering duplicates.\n",
    "- Use `ImageFolder` structure with each meme template as a folder.\n",
    "- Potential strong data augmentation for text or overlay disclaimers.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Download **road sign images** from different countries (like GTSRB for Germany, plus other local sign datasets). Merge them into a single dataset with sign type labels. Standardize image size to 64×64. Split by country: use one country for validation, another for testing, rest for training, to test cross-domain generalization.\n",
    "\n",
    "*Technical note:*\n",
    "- Ensure that each sign type has consistent labeling across countries.\n",
    "- Possibly keep separate transforms for color normalization if sign color differs.\n",
    "- This tests domain adaptation across different sign designs.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Gather a **multi-lighting set** of the same objects captured in bright, dim, and normal lighting. Label them by object category (10 categories). Each category has ~300 images across lighting conditions. Partition randomly into train/val/test. This explores illumination invariance.\n",
    "\n",
    "*Technical note:*\n",
    "- Acquire or simulate varying light conditions.\n",
    "- Possibly apply gamma correction during load to unify brightness.\n",
    "- Balanced splits help ensure each lighting scenario is seen in train/val/test.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 2. Turn the loaded dataset into `torch.utils.data.DataLoader`</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPDzW0wxhi3"
   },
   "source": [
    "**Variant 1:**\n",
    "Implement **oversampling** in the DataLoader for an imbalanced dataset (e.g., some rare class). Use `WeightedRandomSampler` with higher weights for the underrepresented classes, ensuring more frequent sampling of minority samples.\n",
    "\n",
    "*Technical note:*\n",
    "- Create a sampler that accounts for class distribution.\n",
    "- `DataLoader(..., sampler=weighted_sampler, ...)` overrides the default shuffle.\n",
    "- Validate if oversampling truly improves minority class performance.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Create **multiple DataLoaders** for a hierarchical classification scenario: one loader for coarse categories (e.g., animals vs. vehicles) and one for fine-grained categories (dog, cat, truck, car). The model can train in two phases, using each DataLoader separately.\n",
    "\n",
    "*Technical note:*\n",
    "- Tag each image with both coarse and fine label.\n",
    "- Set up two separate `Dataset` objects or handle one dataset with different label calls.\n",
    "- This approach can be used for multi-level classification tasks.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Use a **Contrastive Dataloader** that outputs pairs of images (similar or dissimilar). Implement a custom `__getitem__` that picks two images with a label indicating same class or not. This is useful for Siamese networks or metric learning.\n",
    "\n",
    "*Technical note:*\n",
    "- Each batch item is (img1, img2, label_is_same).\n",
    "- `collate_fn` might need customization to stack these pairs properly.\n",
    "- Great for tasks that rely on embedding or distance-based classification.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Adopt a **grouped DataLoader** for a large dataset so that each batch has images from exactly 2 classes, making it easier for the model to learn subtle differences. The `collate_fn` can ensure each batch has only classes A and B, then the next batch has C and D, etc.\n",
    "\n",
    "*Technical note:*\n",
    "- Shuffle class pairs, then sample images from those two classes in each batch.\n",
    "- Helps model focus on fewer categories at once.\n",
    "- In practice, might require a custom indexing strategy.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Generate an **online data augmentation pipeline** that randomly rotates, flips, or color-jitters each image during the DataLoader fetch. Set `pin_memory=True` for faster GPU transfers. Confirm that transformations differ each epoch for better model generalization.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `transforms.RandomHorizontalFlip()`, `RandomRotation(degrees=15)`, etc.\n",
    "- Data augmentation is typically done in `transforms.Compose`.\n",
    "- `pin_memory=True` can improve performance if using GPU.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Implement a **curriculum-based DataLoader** that first yields “easiest” samples (e.g., large objects with high contrast) and gradually includes “harder” samples (small objects, occluded). Keep track of difficulty in the dataset and re-sample accordingly over epochs.\n",
    "\n",
    "*Technical note:*\n",
    "- Possibly store a “difficulty score” for each image in the dataset.\n",
    "- Dynamically adjust the sampling distribution each epoch.\n",
    "- This approach can help the model learn from easy to challenging examples.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Build a **semi-supervised DataLoader** with a small labeled set and a large unlabeled set. The labeled set has standard labels, while the unlabeled set only provides data. Return them in separate batches or combined with special labels indicating unlabeled samples.\n",
    "\n",
    "*Technical note:*\n",
    "- Possibly define two datasets, LabeledDataset and UnlabeledDataset, each with their own indexing.\n",
    "- A custom collate function might put a flag for unlabeled items.\n",
    "- This approach is crucial for advanced semi-supervised training strategies.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Use a **stratified sampling** approach so each batch approximates the overall class distribution. For multi-class with big data, this ensures each mini-batch is representative. This can help avoid epochs where some classes are scarcely sampled.\n",
    "\n",
    "*Technical note:*\n",
    "- Compute class frequencies beforehand.\n",
    "- Implement a sampler that picks from each class in proportion to its frequency.\n",
    "- Maintain randomization within class sub-samples.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Using the **iNaturalist** dataset, explore loading multiple images *per instance* to simulate different views or perspectives of the same species.  For each species, instead of loading just one image at a time, design a DataLoader that can load a small set of images (e.g., 2-3 images) associated with the same instance.  The task remains image classification for individual images, but the data loading should demonstrate handling multiple images per sample.\n",
    "\n",
    "*Technical note:*\n",
    "-  Adapt the `ImageFolder` logic or create a custom `Dataset` to group images by instance (species in this case). You might need to restructure or understand how iNaturalist organizes images.\n",
    "-  Instead of a single image tensor per item, each item from the DataLoader will yield a *list* or *tuple* of image tensors (e.g., of length 2 or 3), each of shape [C, H, W].\n",
    "-  For the classification model, you will still process each image individually (e.g., using a CNN), but the data loading step will prepare you for scenarios where multiple views or inputs are available for each instance.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Implement a **“sliding window”** DataLoader for medical imaging volumes (e.g., 3D MRIs). Each sample is a sub-volume chunk of shape [depth, height, width]. Slide through the full volume with overlap. Return sub-volumes plus a label (e.g., tumor or not).\n",
    "\n",
    "*Technical note:*\n",
    "- Use a custom indexing approach that enumerates sub-volume coordinates.\n",
    "- Potentially set a stride for the sliding window.\n",
    "- Returns partial volumes, good for patch-based training.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Create a DataLoader that **balances** the classes at the *batch* level specifically. For example, each mini-batch of 32 images must have exactly 8 images from each of 4 classes. This ensures constant class balance in every batch.\n",
    "\n",
    "*Technical note:*\n",
    "- Possibly implement a custom sampler that cycles through classes in round-robin fashion.\n",
    "- Good for smaller classes that risk underrepresentation in random sampling.\n",
    "- Must handle leftover images carefully.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Use a DataLoader that includes **metadata** per sample, such as bounding box coordinates or textual captions, in addition to the image. The `__getitem__` returns `(image, label, metadata)`. For instance, some tasks might require bounding boxes for classification awareness.\n",
    "\n",
    "*Technical note:*\n",
    "- Extend your dataset class to read annotation files or CSV with bounding boxes/captions.\n",
    "- The DataLoader can pass these along in each batch for downstream usage.\n",
    "- Keep the data structure consistent in `collate_fn`.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Implement a **lazy-loading** DataLoader that reads large images from disk only when needed, then caches a certain number of them in memory. This approach suits extremely large image datasets that can’t fit fully in RAM.\n",
    "\n",
    "*Technical note:*\n",
    "- Use a caching mechanism (e.g., an LRU cache) or partial in-memory for frequently accessed images.\n",
    "- `__getitem__` only loads from disk if not in cache.\n",
    "- Speeds up repeated epochs on large sets.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Construct a **DataLoader with partial label** (multi-label scenario) for each image. For example, each image might have labels [“animal”, “brown”], [“vehicle”, “red”], etc. Return a multi-hot vector for each sample. The model can then handle multi-label classification.\n",
    "\n",
    "*Technical note:*\n",
    "- The dataset must store label vectors for each image.\n",
    "- Collate them so each batch has shape [batch_size, number_of_possible_labels].\n",
    "- Typically pairs well with a `BCEWithLogitsLoss`.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "For a **paired image translation** task (e.g., day→night), your DataLoader should return `(img_day, img_night)` pairs. Possibly from a folder structure that matches day and night images by filename. This is common for pix2pix-like training.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement a custom dataset class that looks up the matching counterpart for each day image in a “night” folder.\n",
    "- Return both images in a single sample.\n",
    "- Sorting filenames or storing pairs in a dictionary can help.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Develop a **DataLoader that merges two datasets** on the fly. For instance, 50% of the time sample from dataset A (CIFAR-10) and 50% from dataset B (SVHN). This helps training a network that sees mixed data from two domains.\n",
    "\n",
    "*Technical note:*\n",
    "- Inside `__getitem__`, randomly choose which dataset to pull from.\n",
    "- Keep separate indices for each dataset or unify them in a single index range.\n",
    "- Could be used for domain adaptation or multi-task learning.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Create a **DataLoader for super-resolution** tasks. For each high-resolution image in the dataset, downscale it by a factor (e.g., 4×) to produce a low-res input. Return `(lr_image, hr_image)`. Perfect for training a super-resolution CNN.\n",
    "\n",
    "*Technical note:*\n",
    "- Use something like `transforms.Resize` to produce the low-res version.\n",
    "- The dataset must store the high-res original for the ground-truth.\n",
    "- This approach is standard in SR research.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Implement a **DataLoader for style transfer**. Each batch item includes `(content_image, style_image)`. Randomly select pairs from a “content” folder and a “style” folder. The model can then learn to blend style and content in training.\n",
    "\n",
    "*Technical note:*\n",
    "- Partition content images and style images separately.\n",
    "- `__getitem__` picks one from each set randomly.\n",
    "- Good for quick prototyping of style transfer training loops.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Adopt a **DataLoader that yields partial or corrupted images** to implement inpainting tasks. For instance, zero out a random patch in the image to simulate missing data. The label or “target” is the original uncorrupted image.\n",
    "\n",
    "*Technical note:*\n",
    "- The dataset stores the full image.\n",
    "- On-the-fly transform removes a patch from the input, producing “corrupted_image.”\n",
    "- Return `(corrupted_image, full_image)` as `(input, target)`.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Use a **custom multi-resolution DataLoader** that returns the same image in multiple scales (e.g., 224×224 and 112×112). The model can have a branch for each scale, or you can feed the smaller scale in a different stage. This can enhance multi-scale feature learning.\n",
    "\n",
    "*Technical note:*\n",
    "- For each sample, generate two versions with `transforms.Resize(224)` and `transforms.Resize(112)`.\n",
    "- Return `(img_224, img_112, label)`.\n",
    "- Useful for architectures expecting multi-level resolution inputs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 3. Create a CNN model</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCCVfXk5xjYS"
   },
   "source": [
    "**Variant 1:**\n",
    "Develop a **MobileNetV2-inspired** CNN. Use depthwise separable convolutions for efficiency. Have a few inverted residual blocks. Conclude with a global average pooling and a linear layer for classification. This structure ensures a lightweight yet powerful model.\n",
    "\n",
    "*Technical note:*\n",
    "- Each “bottleneck” block reduces channels, applies depthwise conv, then expands back.\n",
    "- Use ReLU6 or standard ReLU as activation.\n",
    "- Output dimension depends on the dataset classes.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Build a **DenseNet-like** model with dense connections between convolutional layers. After each small block, concatenate feature maps and apply a transition layer. Final classification layer uses average pooling and a fully connected output.\n",
    "\n",
    "*Technical note:*\n",
    "- Each dense block: `Conv -> ReLU -> BN`, then concat with input.\n",
    "- A transition block can include `1x1 conv` + average pool to reduce dimensions.\n",
    "- Suitable for deeper architectures in PyTorch.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Implement a **UNet-inspired** model for segmentation but adapt it for classification by replacing the decoder with a classification head. The encoder uses downsampling conv blocks, and skip connections exist but feed into a final FC layer. Good for tasks requiring spatial detail in early layers.\n",
    "\n",
    "*Technical note:*\n",
    "- Typical UNet has symmetrical encoder-decoder; here the “decoder” is replaced with a simpler aggregator.\n",
    "- High-level skip connections might be concatenated or averaged.\n",
    "- The last output dimension is the number of classes.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Recreate `model_2` by including **Squeeze-and-Excitation (SE) blocks**. Each convolutional block is followed by an SE step that learns channel-wise attention. Combine them in your CNN layers, culminating in a final classification layer.\n",
    "\n",
    "*Technical note:*\n",
    "- SE block: global pooling → small FC → ReLU → second FC → Sigmoid → multiply with feature maps.\n",
    "- Improves channel attention for each conv block.\n",
    "- Helps highlight important channels for each feature map.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Construct a **ResNet-18** style network from scratch using basic blocks with 2-layer conv and skip connections. End with a fully connected layer for classification. This trains from initialization or can load partial pretrained weights if available.\n",
    "\n",
    "*Technical note:*\n",
    "- BasicBlock: (3×3 conv + BN + ReLU) × 2, then add skip input.\n",
    "- Four stages with increasing channel depth.\n",
    "- Use final average pooling and a linear layer for output.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Propose a **Vision Transformer (ViT)-inspired** model. Split each image into patches, project them linearly, then feed them through a small transformer encoder. Conclude with an MLP head for classification. This merges advanced transformer ideas in model_2.\n",
    "\n",
    "*Technical note:*\n",
    "- Each image patch is flattened, then embedded with a linear layer.\n",
    "- Encoder: multi-head self-attention + feedforward blocks.\n",
    "- Classification token or pool the final patch embeddings.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Create a **lightweight CNN** with only 3 convolutional layers, each followed by batch norm and ReLU. Use a final global average pooling. Suitable for resource-constrained devices. This minimal approach is easily comparable to the lecture’s baseline.\n",
    "\n",
    "*Technical note:*\n",
    "- Example: conv(3→16), conv(16→32), conv(32→64).\n",
    "- BN+ReLU after each conv.\n",
    "- Final layer: linear from 64 to number_of_classes.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Design a **dual-input CNN** (for example, grayscale + edges as separate input channels) within the same forward pass. Concatenate their feature maps mid-network, then proceed with classification. This can replicate multi-modal input in a single model.\n",
    "\n",
    "*Technical note:*\n",
    "- Input dimension might be 2 channels: [batch, 2, H, W].\n",
    "- Or process them separately in two conv branches, then merge.\n",
    "- Summation or concatenation merges the branches.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Implement a **Grouped Convolution** approach: in early layers, split channels into two groups for parallel conv. Then merge. This reduces parameter count and sometimes accelerates inference. Continue with standard conv layers after grouping.\n",
    "\n",
    "*Technical note:*\n",
    "- `nn.Conv2d(in_channels, out_channels, groups=2)` splits the channels.\n",
    "- Grouped conv is used in ShuffleNet or Xception-like architectures.\n",
    "- Ensure channel divisibility aligns with grouping.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Use a **feature pyramid network** idea: extract features at multiple scales from a backbone CNN (like a simplified ResNet). Combine them into a single representation for classification. This approach helps the model handle objects at different scales.\n",
    "\n",
    "*Technical note:*\n",
    "- Feature pyramid merges outputs from multiple resolution stages.\n",
    "- Possibly upsample lower-resolution features to match higher resolution.\n",
    "- Then do a final classification from the aggregated feature map.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Construct a **ShuffleNet** style model. Use channel shuffling between groups after grouped convolutions. Follow with BN/ReLU. End with average pooling and an FC layer. This aims for high efficiency with relatively few parameters.\n",
    "\n",
    "*Technical note:*\n",
    "- The shuffle operation ensures cross-group information exchange.\n",
    "- Reproduce the “shuffle” pattern in forward pass after group conv.\n",
    "- Commonly used in mobile device scenarios.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Implement a **Dilated CNN** for classification, using dilated convolutions in the later layers to capture a wider receptive field. This model can handle large context without added parameters. Finish with a global pooling or flatten + FC.\n",
    "\n",
    "*Technical note:*\n",
    "- Use conv2d with `dilation=2` or `dilation=4` in deeper layers.\n",
    "- Helps see more context around each pixel.\n",
    "- Manage kernel size carefully to avoid excessive memory usage.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Create a **ResNeXt** style block in place of standard residual blocks. Each block has grouped convolutions (e.g., group=32). Summation with the skip connection. This often yields better performance for the same depth.\n",
    "\n",
    "*Technical note:*\n",
    "- The cardinality (number of groups) is a key hyperparameter.\n",
    "- Use `nn.Conv2d(in_channels, out_channels, groups=32)` in each branch.\n",
    "- Then add skip input for the final output.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Implement a **multi-task** version of `model_2` that outputs both class prediction and an auxiliary regression (e.g., bounding box). The network has a shared backbone but two separate heads. Ideal if you want classification plus location info.\n",
    "\n",
    "*Technical note:*\n",
    "- Shared feature extractor with convolutional layers.\n",
    "- Head 1: linear for classification. Head 2: linear for bounding box coords.\n",
    "- The forward pass returns two outputs.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Build a **CapsNet** style classifier. Each conv block forms primary capsules. Then a dynamic routing mechanism aggregates them into classification capsules. This significantly differs from standard CNN but can be integrated into `model_2` logic.\n",
    "\n",
    "*Technical note:*\n",
    "- PrimaryCaps layer = a set of small conv filters that output “capsule” vectors.\n",
    "- Routing by agreement uses iterative refinement of capsule weights.\n",
    "- Output capsules match the number of classes.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Design a CNN that includes a **spatial transformer** module early on. The transformer learns to warp or align input images. After that module, feed the transformed output to standard conv layers and end with a classification head.\n",
    "\n",
    "*Technical note:*\n",
    "- `nn.Sequential(SpatialTransformer(), conv_block, ...)`\n",
    "- Spatial transformer often includes a localization network and a grid sampler.\n",
    "- Great for tasks where the object might be at varied positions.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Propose a **NASNet-like** model with repeated “normal cells” and “reduction cells.” Although manual, mimic the NASNet approach with skip or parallel conv branches. The final aggregator is a global average pool + FC layer.\n",
    "\n",
    "*Technical note:*\n",
    "- Each “cell” is an arrangement of conv branches with merges.\n",
    "- Reduction cells reduce spatial size with stride 2.\n",
    "- Normal cells keep the same spatial dimension.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Implement a **self-attention** block within a CNN. After certain conv layers, apply a self-attention mechanism to highlight important spatial features. Then continue with standard CNN layers. This merges CNN with transformer-like attention.\n",
    "\n",
    "*Technical note:*\n",
    "- Flatten the feature map and compute attention scores for each location.\n",
    "- Weighted sum of feature vectors is the attention output.\n",
    "- Improves modeling of global dependencies in images.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Create a **GAN discriminator**-like architecture for classification, repurposing typical discriminator blocks (Conv→LeakyReLU→Conv→LeakyReLU→...). Then finalize with a Sigmoid or linear output for the classification. This reuses style from generative tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- Discriminator blocks usually have fewer pooling layers, but multiple strided convs.\n",
    "- Keep an eye on dimension reduction as you go deeper.\n",
    "- Output is 1 for binary or multiple for multi-class.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Use a **hybrid CNN-RNN** approach: a CNN processes the image row by row, then a small RNN scans across the row embeddings. Conclude with a classification layer. This is unconventional but can experiment with capturing row-wise dependencies.\n",
    "\n",
    "*Technical note:*\n",
    "- Flatten each row’s conv features into a sequence dimension for the RNN.\n",
    "- LSTM or GRU can handle the temporal dimension (each row).\n",
    "- Potentially interesting for text-like structural images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 4. Train the model on the corresponding dataset</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_3zUr7xlhy"
   },
   "source": [
    "**Variant 1:**\n",
    "Use a **one-cycle learning rate policy**. Set an initial small LR, gradually increase to a maximum mid-epoch, then decay. This often yields faster convergence. Track training loss each iteration to confirm it matches the cyclical LR pattern.\n",
    "\n",
    "*Technical note:*\n",
    "- `torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=...)` is built-in.\n",
    "- The schedule needs a total_step or epoch × iteration count.\n",
    "- Monitor if it helps skip local minima.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Train with **gradient accumulation** to effectively use a large batch size. Accumulate gradients over 4 sub-batches, then update. This helps if GPU memory is limited but you want large-batch benefits like stable gradient estimates.\n",
    "\n",
    "*Technical note:*\n",
    "- `for i, (data, target) in enumerate(train_loader): ... loss.backward()`\n",
    "- If `(i+1) % accumulate_steps == 0: optimizer.step(); optimizer.zero_grad()`\n",
    "- Keep consistent averaging of the loss for each sub-batch.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Apply **label smoothing**: instead of one-hot labels, use 0.9 for the true class and 0.1 distributed among others. This can reduce overconfidence. Evaluate if your validation accuracy or calibration improves.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement label smoothing manually or use PyTorch’s built-in cross-entropy with label_smoothing param.\n",
    "- Typically helps large datasets with overfitting or confident misclassifications.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Perform **mixup** training. For each mini-batch, randomly mix pairs of images and their labels. The model learns from interpolated samples. This often boosts robustness and reduces overfitting.\n",
    "\n",
    "*Technical note:*\n",
    "- For images x1, x2, labels y1, y2, do x_m = λx1 + (1−λ)x2, y_m = λy1 + (1−λ)y2.\n",
    "- Train with MSE or cross-entropy adapted for mixed labels.\n",
    "- Parameter λ ~ Beta(α, α).\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Use **SAM (Sharpness-Aware Minimization)** optimizer for training. This technique updates weights in a way that favors flatter minima. Evaluate if generalization improves compared to standard SGD or Adam.\n",
    "\n",
    "*Technical note:*\n",
    "- SAM requires a two-step process each update: compute grads, move weights in grad direction, recalc grads, then finalize update.\n",
    "- PyTorch libraries or code snippets for SAM exist externally.\n",
    "- Keep an eye on extra compute overhead.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Integrate **selective backprop**: skip backward pass for easy samples (where the loss is below a threshold) so the model focuses on hard examples. Evaluate if it speeds up or improves final accuracy. Keep standard forward pass for all, but selectively do `.backward()`.\n",
    "\n",
    "*Technical note:*\n",
    "- If `loss_item < threshold: skip backward`.\n",
    "- The threshold can dynamically adjust each epoch or remain fixed.\n",
    "- Good for large data with many trivial examples.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Adopt **stochastic depth** training. Randomly skip entire residual blocks in each forward pass with some probability. This is known to regularize deeper networks like ResNets, making them behave like an ensemble of shallower subnets.\n",
    "\n",
    "*Technical note:*\n",
    "- For each residual block, with probability p, forward = identity skip only.\n",
    "- Grad is zero for that block if it’s skipped.\n",
    "- Raises the model’s effective depth variability.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Perform **multi-task training** for classification plus an auxiliary segmentation or bounding box regression. Combine losses in a weighted sum. See if the shared features help classification. Evaluate both tasks’ metrics.\n",
    "\n",
    "*Technical note:*\n",
    "- Suppose total_loss = α * classification_loss + β * bounding_box_loss.\n",
    "- Adjust α, β to balance tasks.\n",
    "- Proper data structure if some images don’t have bounding boxes or segmentation labels.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Use a **frozen backbone** for the first 10 epochs (like a pretrained ResNet). Only train the new classification head. Then unfreeze the backbone for the next 10 epochs to fine-tune all layers. This two-stage approach is standard in transfer learning.\n",
    "\n",
    "*Technical note:*\n",
    "- Set `param.requires_grad = False` for backbone initially.\n",
    "- After initial stage, set True for all.\n",
    "- Helps avoid large gradient updates on pretrained weights early on.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Train with a **cosine annealing learning rate** schedule over 50 epochs. The LR decreases following a cosine curve from initial LR to a small min LR by the final epoch. This scheduling often stabilizes training for CNNs.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)`.\n",
    "- Validate performance after each epoch; the LR will be automatically adjusted.\n",
    "- Tends to yield smooth convergence.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Employ **distributed data parallel** training across multiple GPUs or multiple nodes. Synchronously update the model parameters. Track speedup vs. single-GPU. Confirm that final accuracy matches single-GPU baseline, assuming correct synchronization.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torch.distributed.init_process_group(...)` then `DistributedDataParallel(model)`.\n",
    "- Partition data across processes with `DistributedSampler`.\n",
    "- Keep an eye on setup complexities (master address, rank, etc.).\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Train with an **adversarial training** strategy. Generate small adversarial perturbations (FGSM or PGD) on training samples and train on these adversarially perturbed images. This can improve the model’s robustness.\n",
    "\n",
    "*Technical note:*\n",
    "- FGSM: x_adv = x + ε sign(∇_x loss).\n",
    "- Recompute the gradient for the perturbed input.\n",
    "- Greatly increases training time but yields more robust classification.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Implement a **knowledge distillation** training loop. Use a larger teacher model (already trained) to provide soft targets for the smaller student. Combine the student’s cross-entropy with the teacher’s KL divergence at a chosen temperature.\n",
    "\n",
    "*Technical note:*\n",
    "- `loss = α * CrossEntropy(student, hard_labels) + β * KLDiv(student, teacher_softmax)`.\n",
    "- Temperature > 1 for teacher’s output softening.\n",
    "- The student sees both the dataset labels and teacher’s distribution.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Train with a **reinforcement of correct predictions** approach: if the model is confident and correct on a sample, reduce its loss weight next time. If the model is uncertain or wrong, keep/increase the weight. This is a dynamic weighting scheme.\n",
    "\n",
    "*Technical note:*\n",
    "- Maintain a difficulty or correctness score for each sample.\n",
    "- Gradually adjust each sample’s effective loss weight over epochs.\n",
    "- Similar to boosting algorithms in classical ML.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Use an **auxiliary classifier** in the middle of the network. For instance, at an intermediate layer, branch out a small classifier that predicts the same label. Add its loss to the main classification loss. This can help gradients flow earlier in deep networks (like GoogLeNet’s Inception).\n",
    "\n",
    "*Technical note:*\n",
    "- mid_output = mid_classifier(mid_features), compute mid_loss.\n",
    "- final_output = main_classifier(final_features), compute main_loss.\n",
    "- total_loss = main_loss + λ * mid_loss.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Integrate a **cutout** augmentation approach. In each training image, randomly mask out a square region (e.g., 16×16) by setting it to zero or a mean color. Combine it with random crops and flips. Then proceed with normal training.\n",
    "\n",
    "*Technical note:*\n",
    "- For each input image in the batch, pick a random location, set a square patch to 0.\n",
    "- This helps the model rely on other visual cues, not just one region.\n",
    "- Implement via a custom transform or inside the training loop.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "Use **longer training** with a **warm restarts** schedule. For example, every 10 epochs, reset the LR to a higher value, then decay over subsequent epochs. Compare if this cyclical approach helps avoid local minima for your CNN.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)` for example.\n",
    "- The LR resets each T_0, but the period grows by T_mult each time.\n",
    "- Plot LR vs. epoch to confirm.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Train with **fp16 mixed precision** to reduce memory usage and speed up on GPU. Keep careful track of numerical stability. Compare final accuracy with a full fp32 baseline. Mixed precision can accelerate large CNNs significantly.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `torch.cuda.amp.autocast()` plus `GradScaler`.\n",
    "- If instability arises, fallback to fp32 for certain layers or set `GradScaler` growth intervals.\n",
    "- Typically no accuracy drop if done carefully.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Use **checkpoint averaging**: train the model for 100 epochs, but every 10 epochs, save a checkpoint. At the end, average the weights from these 10 checkpoints. Evaluate if the ensemble effect yields better accuracy than the final single checkpoint.\n",
    "\n",
    "*Technical note:*\n",
    "- Implement a function to load each checkpoint’s state_dict, accumulate them, and divide by the number of checkpoints.\n",
    "- This is simpler than SWA but similar in concept.\n",
    "- Usually stabilizes final performance.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Train your model with a **large-scale hyperparameter search** across learning rates, momentums, or weight decays. Automate this via a script that tries multiple combinations (grid or random). Evaluate each run’s validation accuracy and pick the best. This helps find an optimal training recipe.\n",
    "\n",
    "*Technical note:*\n",
    "- Could integrate libraries like Optuna or Ray Tune for hyperparameter optimization.\n",
    "- Each hyperparam set is a separate training run.\n",
    "- Summarize final val accuracy and pick top config.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.945799Z",
     "start_time": "2024-03-19T11:08:10.931711Z"
    },
    "id": "jSo6vVWFbNLD"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red; font-size:1.5em;\">Task 5. Make predictions</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1CsHhPpxp1w"
   },
   "source": [
    "**Variant 1:**\n",
    "Generate **class activation maps** (CAM) for each test image. Visualize which regions contributed most to the predicted class. Overlay the heatmap on the original image. This clarifies model interpretability in classification tasks.\n",
    "\n",
    "*Technical note:*\n",
    "- Forward pass with `model`.\n",
    "- Retrieve feature maps from the final conv layer, multiply by weights from the classifier.\n",
    "- Upsample to original image resolution, then color-map overlay.\n",
    "\n",
    "---\n",
    "**Variant 2:**\n",
    "Implement a **batch-inference** pipeline that processes the test set in large batches. Evaluate speed vs. single-image inference. Then measure accuracy across the entire set. This is standard for quick predictions at scale.\n",
    "\n",
    "*Technical note:*\n",
    "- `model.eval()`, then loop over test_loader with large batch_size.\n",
    "- Use `torch.no_grad()` or `torch.inference_mode()`.\n",
    "- Compare total time for different batch sizes.\n",
    "\n",
    "---\n",
    "**Variant 3:**\n",
    "Compute **per-class metrics** (precision, recall, F1) for multi-class classification. Summarize them in a table or confusion matrix to see which classes are hardest. Possibly plot them in a bar chart for clarity.\n",
    "\n",
    "*Technical note:*\n",
    "- After predictions, gather them in arrays, use `sklearn.metrics.classification_report`.\n",
    "- For multi-class, the report includes precision, recall, f1 for each label.\n",
    "- Helps interpret class imbalances in performance.\n",
    "\n",
    "---\n",
    "**Variant 4:**\n",
    "Perform **MC Dropout** during inference by keeping dropout layers active. Run multiple forward passes (e.g., 30 times) for the same image. Compute mean and variance of predictions. This yields an uncertainty estimate.\n",
    "\n",
    "*Technical note:*\n",
    "- `model.train()` for the dropout effect, but do not do a backward pass.\n",
    "- A loop: `for _ in range(n_samples): preds.append(model(x))`.\n",
    "- Compute standard deviation across those predictions.\n",
    "\n",
    "---\n",
    "**Variant 5:**\n",
    "Implement a **TTA (Test-Time Augmentation)** approach: for each test image, apply random flips, crops, or color jitters. Average the model’s outputs across these variants. Compare if TTA improves final accuracy.\n",
    "\n",
    "*Technical note:*\n",
    "- e.g., do 5 random augmentations + original image, sum probabilities, then divide by 6.\n",
    "- Common in Kaggle competitions.\n",
    "- Usually yields a small accuracy boost.\n",
    "\n",
    "---\n",
    "**Variant 6:**\n",
    "Predict with a **sliding window** approach for large images or segmentation tasks. Split the test image into overlapping patches, pass each patch through the model, then merge predictions. Good for high-resolution inputs.\n",
    "\n",
    "*Technical note:*\n",
    "- For classification, might treat each patch as local ROI. For segmentation, piece together patch outputs.\n",
    "- Overlap can reduce edge artifacts.\n",
    "- Potentially large memory usage, so watch out for efficiency.\n",
    "\n",
    "---\n",
    "**Variant 7:**\n",
    "Perform **out-of-distribution detection**. Supply images from a different domain, then measure the model’s confidence. If the predicted probability is low or the feature embedding is far from known classes, label it as OOD. This tests model robustness.\n",
    "\n",
    "*Technical note:*\n",
    "- Possibly measure max softmax probability or distance to training set embeddings.\n",
    "- Evaluate how the model behaves on classes not in training.\n",
    "- Helps see if the system confuses unknowns with known classes.\n",
    "\n",
    "---\n",
    "**Variant 8:**\n",
    "Use a **threshold calibration** approach after predictions. Instead of the default 0.5 for binary classification, find the best threshold via validation F1 or any chosen metric. Then apply that threshold to the test set predictions.\n",
    "\n",
    "*Technical note:*\n",
    "- Sort predicted probabilities on the validation set.\n",
    "- Evaluate metric at each possible threshold.\n",
    "- The threshold that yields best metric is used for final test predictions.\n",
    "\n",
    "---\n",
    "**Variant 9:**\n",
    "Visualize predictions in an **embedding space**. Pass each test image through the final layer before classification, collect the embeddings. Then apply TSNE or UMAP to reduce to 2D. Color points by predicted label. This can show cluster separations.\n",
    "\n",
    "*Technical note:*\n",
    "- `model.forward(..., return_embedding=True)` or extract from penultimate layer.\n",
    "- Use scikit-learn’s `TSNE(n_components=2)` or UMAP.\n",
    "- Plot with matplotlib, color-coded by predicted class.\n",
    "\n",
    "---\n",
    "**Variant 10:**\n",
    "Perform **multi-crop testing**: for each test image, extract multiple overlapping crops (e.g., 5 or 10) at corners and center. Average the model outputs across these crops for a final prediction. Compare if it improves performance over single-crop.\n",
    "\n",
    "*Technical note:*\n",
    "- Typically done with `transforms.FiveCrop` or custom logic.\n",
    "- Each crop is fed to `model.eval()`.\n",
    "- The final class is the one with the highest average probability.\n",
    "\n",
    "---\n",
    "**Variant 11:**\n",
    "Generate a **model ensemble** for prediction. Combine 3–5 trained networks (potentially with different seeds). For each test image, gather each model’s softmax probabilities and average them to get the final predicted class.\n",
    "\n",
    "*Technical note:*\n",
    "- Each model is in `eval()` mode, do a forward pass.\n",
    "- Probability-level averaging is common.\n",
    "- Usually yields higher accuracy but more inference cost.\n",
    "\n",
    "---\n",
    "**Variant 12:**\n",
    "Implement **confidence calibration** plotting: produce a reliability diagram showing predicted probability vs. actual accuracy in that probability bin. If the model is well-calibrated, it should align with the diagonal. Then compute ECE or MCE.\n",
    "\n",
    "*Technical note:*\n",
    "- Use `p, y` across test set, bin them, measure average p vs. fraction of positives.\n",
    "- Plot the difference from diagonal.\n",
    "- Tools like `torchmetrics` or scikit-learn can help.\n",
    "\n",
    "---\n",
    "**Variant 13:**\n",
    "Provide **per-instance prediction explanations** using LIME or SHAP. After the model predicts for an image, approximate local feature importance. Then highlight which pixels (or super-pixels) contributed most to the classification decision.\n",
    "\n",
    "*Technical note:*\n",
    "- LIME runs perturbations around the input to see changes in prediction.\n",
    "- SHAP can integrate gradients or approximate local surrogate models.\n",
    "- Often used for interpretability with end users.\n",
    "\n",
    "---\n",
    "**Variant 14:**\n",
    "Test the model on **adversarially perturbed images**. Evaluate how predictions differ from the standard images. Plot side-by-side results. This reveals how robust (or brittle) your model is under small input perturbations.\n",
    "\n",
    "*Technical note:*\n",
    "- Use an FGSM or PGD approach to create adversarial examples.\n",
    "- Compare predicted labels on clean vs. adversarial images.\n",
    "- Summarize success rate of attacks or degrade in accuracy.\n",
    "\n",
    "---\n",
    "**Variant 15:**\n",
    "Compute the **logits** for every test sample, then apply a **softmax** temperature scaling at inference (T > 1 or < 1). Observe how it changes predicted confidence. This is a post-training calibration trick.\n",
    "\n",
    "*Technical note:*\n",
    "- If `logits = model(x)`, then `softmax(logits / T)` for a chosen T.\n",
    "- T>1 flattens the distribution, T<1 sharpens it.\n",
    "- Evaluate if calibration or accuracy is improved.\n",
    "\n",
    "---\n",
    "**Variant 16:**\n",
    "Perform a **class-based error analysis**: gather all test samples the model got wrong for each class. Inspect them or generate a mini-HTML gallery. This helps see if there’s a pattern, e.g., certain backgrounds or angles that cause confusion.\n",
    "\n",
    "*Technical note:*\n",
    "- Compare `pred_label != true_label`.\n",
    "- Organize misclassified samples in folders or a webpage for manual inspection.\n",
    "- Potentially reveal data distribution issues or consistent failure modes.\n",
    "\n",
    "---\n",
    "**Variant 17:**\n",
    "For a multi-label classification, compute predictions as multiple independent sigmoid outputs. Then apply a chosen threshold for each label or a global threshold. Summarize the average precision for each label. Show examples of partial labeling success.\n",
    "\n",
    "*Technical note:*\n",
    "- `model` returns logit vector. Sigmoid for each dimension.\n",
    "- Compare each dimension’s thresholded output to ground truth.\n",
    "- Summarize with per-label F1 or average precision.\n",
    "\n",
    "---\n",
    "**Variant 18:**\n",
    "Evaluate **speed vs. accuracy** trade-offs by dynamically adjusting image resolution at inference time. Predict at 224×224, then at 160×160, etc., measuring any drop in accuracy. Plot a curve of resolution vs. inference speed and accuracy.\n",
    "\n",
    "*Technical note:*\n",
    "- Downsample test images to different sizes before feeding the model.\n",
    "- Typically, lower resolution = faster inference, but lower accuracy.\n",
    "- Summarize results to find an optimal compromise for deployment.\n",
    "\n",
    "---\n",
    "**Variant 19:**\n",
    "Run a **cross-dataset evaluation**: train on your original dataset, then test on a similar but distinct dataset (e.g., train on CIFAR-10, test on STL-10). Compare how the model generalizes. This clarifies domain transfer ability.\n",
    "\n",
    "*Technical note:*\n",
    "- Directly use `model.eval()` on images from the new dataset with same label set if possible.\n",
    "- If labels differ, adapt the evaluation metrics or classes.\n",
    "- Typically results in accuracy drop, quantifying domain gap.\n",
    "\n",
    "---\n",
    "**Variant 20:**\n",
    "Implement a **two-stage prediction** for large label sets. First, predict a super-class or coarse category. If needed, apply a second specialized classifier for the sub-category. Compare if this hierarchical approach improves accuracy or speed.\n",
    "\n",
    "*Technical note:*\n",
    "- Stage 1: coarse classification (e.g., animal vs. vehicle).\n",
    "- Stage 2: specialized model for the sub-group (e.g., dog vs. cat).\n",
    "- Potentially skip second stage for confident predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.7\"></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_pytorch_computer_vision_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
