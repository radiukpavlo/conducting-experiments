{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratory work 4.</center></h1>\n",
    "<h2><center>PyTorch Computer Vision Exercises</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Last name and First name\n",
    "\n",
    "**Варіант:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Load the dataset.](#4.1)\n",
    "2. [Task 2. Turn the loaded dataset into `torch.utils.data.DataLoader`.](#4.2)\n",
    "3. [Task 3. ecreate `model_2` used in our lecture notebook.](#4.3)\n",
    "4. [Task 4. Train the model on the corresponding dataset.](#4.4)\n",
    "5. [Task 5. Make predictions.](#4.5)\n",
    "6. [Task 6. Fine-tune the models for the corresponding datasets.](#4.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.801709Z",
     "start_time": "2024-03-19T11:08:09.248915Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DNwZLMbCzJLk",
    "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Exercises require PyTorch > 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# TODO: Setup device agnostic code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 1. Load the dataset</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvf-3pODxXYI"
   },
   "source": [
    "**For variants 1-3:** Load the CIFAR-100 dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the loaded CIFAR-100 dataset.\n",
    "* Apply data augmentation techniques like random cropping, horizontal flipping, and normalization to increase the diversity of the training set.\n",
    "* Split the dataset into training, validation, and test sets.\n",
    "\n",
    "**For variants 4-6:** Load the FashionMNIST dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the loaded FashionMNIST dataset.\n",
    "* Apply custom transformations such as resizing, grayscale conversion, and tensor conversion.\n",
    "* Create a subset of the dataset for faster experimentation.\n",
    "\n",
    "**For variants 7-9:** Load the STL-10 dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the STL-10 dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for semi-supervised image classification tasks.\n",
    "* Preprocess the images by resizing and normalizing them.\n",
    "* Split the dataset into labeled, unlabeled, and test sets as provided in the dataset structure.\n",
    "\n",
    "**For variants 10-12:** Load the SBU Captioned Photo dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the loaded Load the SBU Captioned Photo dataset.\n",
    "* Preprocess the images by resizing them and applying normalization.\n",
    "* Split the dataset into training, validation, and test sets based on the annotations provided.\n",
    "\n",
    "**For variants 13-15:** Load the CIFAR-10 dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the loaded CIFAR-10 dataset.\n",
    "* Apply data augmentation techniques like random cropping, horizontal flipping, and normalization to increase the diversity of the training set.\n",
    "* Split the dataset into training, validation, and test sets.\n",
    "\n",
    "**For variants 16-18:** Load the SVHN (Street View House Numbers) dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the SVHN dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for digit recognition tasks.\n",
    "* Preprocess the images by converting them to grayscale and normalizing them.\n",
    "* Split the dataset into training, extra, and test sets as provided in the dataset structure.\n",
    "\n",
    "**For variants 19-21:** Load the COCO (Common Objects in Context) dataset using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the COCO dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for object detection and segmentation tasks.\n",
    "* Preprocess the images by resizing them and applying normalization.\n",
    "* Split the dataset into training, validation, and test sets based on the annotations provided.\n",
    "\n",
    "**For variants 22-24:** Load the Oxford-IIIT Pet dataset for pet breed classification using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the Oxford-IIIT Pet dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for pet breed classification tasks.\n",
    "* Preprocess the images by resizing and normalizing them.\n",
    "* Split the dataset into training, validation, and test sets based on the breed labels provided.\n",
    "\n",
    "**For variants 25-27:** Load the CelebA (Celebrities Attributes) dataset for facial attribute recognition using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the CelebA dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for facial attribute recognition tasks.\n",
    "* Preprocess the images by cropping the faces and normalizing them.\n",
    "* Split the dataset into training, validation, and test sets based on the attribute labels provided.\n",
    "\n",
    "**For variants 28-30:** Load the LSUN (Large-scale Scene Understanding) dataset for scene classification using [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).\n",
    "* Use the LSUN dataset from [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html) for scene classification tasks.\n",
    "* Preprocess the images by resizing and normalizing them.\n",
    "* Split the dataset into training, validation, and test sets based on the scene categories provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.881709Z",
     "start_time": "2024-03-19T11:08:10.867709Z"
    },
    "id": "SHjeuN81bHza"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 2. Turn the loaded dataset into [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPDzW0wxhi3"
   },
   "source": [
    "**For variants 1-3:** Dataloaders for CIFAR-100 with augmented data.\n",
    "* Create PyTorch `DataLoader` instances for the training, validation, and test sets of the CIFAR-100 dataset.\n",
    "* For the training DataLoader, incorporate data augmentation techniques such as random cropping and horizontal flipping to increase the diversity of the training data. Use the [`torchvision.datasets`](https://pytorch.org/vision/main/datasets.html).transforms` module to apply these transformations.\n",
    "* Set appropriate batch sizes for the DataLoaders, considering the balance between computational efficiency and memory constraints. A common choice for CIFAR-100 is a batch size of 128 for training and 256 for testing.\n",
    "* Enable shuffling for the training DataLoader to ensure that the model sees different permutations of the data during training, which can help improve generalization.\n",
    "* Consider using multiple workers (`num_workers` parameter) in the DataLoader to parallelize data loading and preprocessing, thereby reducing the time spent on data loading during training.\n",
    "\n",
    "**For variants 4-6:** Custom transformations for FashionMNIST dataloaders.\n",
    "* For the FashionMNIST dataset, create DataLoaders with custom transformations such as resizing and grayscale conversion. Use the `torchvision.transforms.Compose` method to chain these transformations together.\n",
    "* Since FashionMNIST images are smaller (28x28), consider using a batch size 64 or 128 to prevent overloading the GPU memory.\n",
    "* Apply tensor conversion to all DataLoaders to convert the images into PyTorch tensors, which are required for training the model.\n",
    "* Create a subset of the training DataLoader for faster experimentation. This can be achieved by using the `torch.utils.data.Subset` class to select a smaller portion of the dataset for initial experiments.\n",
    "\n",
    "**For variants 7-9:** Semi-supervised learning with STL-10 dataloaders.\n",
    "* For the STL-10 dataset, create separate DataLoaders for the labeled, unlabeled, and test sets. The unlabeled set can be used for semi-supervised learning techniques.\n",
    "* Apply resizing and normalization transformations to all DataLoaders. For the unlabeled DataLoader, consider using more aggressive data augmentation techniques to generate diverse unlabeled examples for semi-supervised learning.\n",
    "* Use a smaller batch size for the labeled DataLoader (e.g., 64) and a larger batch size for the unlabeled DataLoader (e.g., 256) to efficiently utilize a large amount of unlabeled data.\n",
    "* Implement a custom sampling strategy for the unlabeled DataLoader that balances the exploration of the unlabeled data with the exploitation of the labeled data during training.\n",
    "\n",
    "**For variants 10-12:** Dataloaders for SBU Captioned Photo dataset with annotations.\n",
    "* Create DataLoaders for the SBU Captioned Photo dataset, applying resizing and normalization transformations to the images.\n",
    "* Since this dataset might have variable-sized images, ensure that the resizing transformation brings all images to a consistent size, which is crucial for batch processing.\n",
    "* Use a batch size that is appropriate for the size of the images and the available computational resources. For example, a batch size of 32 or 64 might be suitable for this dataset.\n",
    "* If the dataset includes captions or other annotations, consider using a custom collate function in the DataLoader to handle the batching of variable-length textual data along with the images.\n",
    "\n",
    "**For variants 13-15:** CIFAR-10 dataloaders with data augmentation.\n",
    "* Create DataLoaders for the CIFAR-10 dataset with data augmentation techniques like random cropping and horizontal flipping for the training DataLoader.\n",
    "* Use a standard batch size of 128 for training and 256 for testing, which is commonly used for CIFAR-10.\n",
    "* Ensure that the images are normalized using the mean and standard deviation specific to the CIFAR-10 dataset.\n",
    "\n",
    "**For variants 16-18:** Grayscale SVHN dataloaders with split sets.\n",
    "* For the SVHN dataset, create separate DataLoaders for the training, extra, and test sets. The extra set can be used for additional training data or for semi-supervised learning approaches.\n",
    "* Apply grayscale conversion and normalization to the images. Since SVHN is a digit recognition dataset, converting the images to grayscale can reduce the computational complexity without significantly impacting performance.\n",
    "* Use a moderate batch size, such as 128, for the training and extra DataLoaders, and a larger batch size, such as 256, for the test DataLoader.\n",
    "\n",
    "**For variants 19-21:** Object detection dataloaders for COCO dataset.\n",
    "* Create DataLoaders for the COCO dataset with resizing and normalization transformations. Since COCO is used for object detection, ensure that the resizing transformation maintains the aspect ratio of the images to preserve the bounding box annotations.\n",
    "* Use a batch size that accommodates the variable sizes of the images and the complexity of the object detection task. A smaller batch size, such as 16 or 32, might be necessary due to the memory requirements of storing bounding boxes and masks.\n",
    "* Implement a custom collate function to handle the batching of images with their corresponding annotations, including bounding boxes and class labels.\n",
    "\n",
    "**For variants 22-24:** Pet breed classification dataloaders for Oxford-IIIT Pet dataset.\n",
    "* Create DataLoaders for the Oxford-IIIT Pet dataset with resizing and normalization transformations. Since this dataset is used for fine-grained classification, ensure that the resizing transformation maintains the quality of the images.\n",
    "* Use a batch size that balances the computational efficiency with the need for fine-grained feature extraction. A batch size of 64 or 128 might be suitable for this dataset.\n",
    "* If the dataset includes annotations such as segmentation masks or bounding boxes, consider using a custom collate function to handle the batching of these annotations along with the images.\n",
    "\n",
    "**For variants 25-27:** Facial attribute recognition dataloaders for CelebA dataset.\n",
    "* Create DataLoaders for the CelebA dataset with transformations such as cropping the faces and normalizing the images. The cropping transformation should focus on the central region of the image where the face is typically located.\n",
    "* Use a moderate batch size, such as 64 or 128, to balance the computational efficiency with the need for detailed feature extraction for attribute recognition.\n",
    "* If the dataset includes multiple attribute labels for each image, ensure that the DataLoader correctly handles the batching of these multi-label annotations.\n",
    "\n",
    "**For variants 28-30:** Scene classification dataloaders for LSUN dataset.\n",
    "* Create DataLoaders for the LSUN dataset with resizing and normalization transformations. Since LSUN is used for scene classification, ensure that the resizing transformation maintains the aspect ratio of the images to preserve the scene context.\n",
    "* Use a batch size that is appropriate for the size of the images and the complexity of the scene classification task. A batch size of 32 or 64 might be suitable for this dataset.\n",
    "* If the dataset includes additional annotations such as room layout or object locations, consider using a custom collate function to handle the batching of these annotations along with the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.913710Z",
     "start_time": "2024-03-19T11:08:10.899710Z"
    },
    "id": "ALA6MPcFbJXQ"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 3. Recreate `model_2` used in our [lecture notebook](https://github.com/radiukpavlo/conducting-experiments/blob/main/01_notebooks/ce_04_pytorch_computer_vision.ipynb)</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCCVfXk5xjYS"
   },
   "source": [
    "**For variants 1-3:** Model for CIFAR-100 classification.\n",
    "* Design a convolutional neural network (CNN) suitable for the CIFAR-100 dataset, which has 100 classes. The model might include several convolutional layers with increasing filter sizes, followed by max-pooling layers to reduce spatial dimensions.\n",
    "* Include batch normalization layers after each convolutional layer to stabilize training and accelerate convergence.\n",
    "* Use dropout layers after each pooling layer to prevent overfitting, given the complexity of the dataset.\n",
    "* The final layers of the model should include fully connected layers that culminate in an output layer with 100 units, corresponding to the number of classes in CIFAR-100, and a softmax activation function to produce class probabilities.\n",
    "\n",
    "**For variants 4-6:** Model for FashionMNIST classification.\n",
    "* Develop a simpler CNN for the FashionMNIST dataset, which has 10 classes and consists of grayscale images. Start with a few convolutional layers with a small number of filters, followed by max-pooling layers.\n",
    "* Include ReLU activation functions after each convolutional layer to introduce non-linearity.\n",
    "* The final part of the model should have fully connected layers leading to an output layer with 10 units and a softmax activation function.\n",
    "* Since FashionMNIST is a less complex dataset, a simpler architecture with fewer layers and parameters might be sufficient for achieving good performance.\n",
    "\n",
    "**For variants 7-9:** Semi-supervised model for STL-10.\n",
    "* Design a CNN for the STL-10 dataset that can leverage the labeled data. The architecture could be similar to that used in [lecture notebook 04](https://github.com/radiukpavlo/conducting-experiments/blob/main/01_notebooks/ce_04_pytorch_computer_vision.ipynb), but with modifications to accommodate semi-supervised learning.\n",
    "* Implement a consistency regularization technique, such as consistency loss between the predictions of augmented versions of the same unlabeled image, to make use of the unlabeled data.\n",
    "* The output layer should have 10 units with a softmax activation function, corresponding to the 10 classes in STL-10.\n",
    "\n",
    "**For variants 10-12:** Model for SBU Captioned Photo captioning.\n",
    "* Create a model that combines CNN and recurrent neural network (RNN) architectures for image captioning on the SBU Captioned Photo dataset. The CNN part can extract visual features from the images, while the RNN part can generate captions based on these features.\n",
    "* Use a pre-trained CNN, such as ResNet or VGG, as the feature extractor, followed by an LSTM or GRU to generate captions word by word.\n",
    "* The output of the RNN should be connected to a fully connected layer with a softmax activation function to predict the next word in the caption.\n",
    "\n",
    "**For variants 13-15:** Model for CIFAR-10 classification.\n",
    "* Develop a CNN for the CIFAR-10 dataset with adjustments to account for the fewer number of classes. The model might have fewer layers and parameters.\n",
    "* Include data augmentation techniques in the training pipeline, such as random cropping and horizontal flipping, to improve generalization.\n",
    "* The output layer should have 10 units with a softmax activation function for the 10 classes in CIFAR-10.\n",
    "\n",
    "**For variants 16-18:** Model for SVHN digit recognition.\n",
    "* Design a CNN for the SVHN dataset, which is focused on digit recognition. The model could start with convolutional layers with a small number of filters, given the simplicity of the task, followed by max-pooling layers.\n",
    "* Include dropout layers to prevent overfitting, especially since the dataset contains a large number of training examples.\n",
    "* The output layer should have 10 units with a softmax activation function, corresponding to the 10 digit classes.\n",
    "\n",
    "**For variants 19-21:** Model for COCO object detection.\n",
    "* Create a model for object detection on the COCO dataset. This could involve using a pre-trained CNN, such as ResNet or VGG, as a backbone for feature extraction, combined with a region proposal network (RPN) for detecting object bounding boxes.\n",
    "* Implement a two-stage detector like Faster R-CNN or a one-stage detector like YOLO or SSD, depending on the desired trade-off between accuracy and speed.\n",
    "* The model should output bounding boxes along with class probabilities for each detected object.\n",
    "\n",
    "**For variants 22-24:** Model for Oxford-IIIT Pet breed classification.\n",
    "* Develop a CNN for pet breed classification on the Oxford-IIIT Pet dataset. Start with a pre-trained CNN, such as ResNet or VGG, to leverage transfer learning from ImageNet.\n",
    "* Fine-tune the pre-trained model by replacing the final fully connected layer with a new layer that has as many units as there are pet breeds in the dataset.\n",
    "* Include data augmentation techniques, such as random cropping and rotation, to improve the model's robustness to variations in pet images.\n",
    "\n",
    "**For variants 25-27:** Model for CelebA facial attribute recognition.\n",
    "* Design a CNN for multi-label classification on the CelebA dataset, where each image can have multiple attributes. Use a pre-trained CNN for feature extraction, followed by fully connected layers.\n",
    "* The output layer should have a sigmoid activation function for each attribute, as this is a multi-label classification task.\n",
    "* Implement techniques to handle class imbalance, such as weighted loss functions, given that some facial attributes might be less common than others.\n",
    "\n",
    "**For variants 28-30:** Model for LSUN scene classification.\n",
    "* Develop a CNN for scene classification on the LSUN dataset. Use a pre-trained CNN, such as ResNet or VGG, as the backbone for feature extraction.\n",
    "* Replace the final fully connected layer of the pre-trained model with a new layer that has as many units as there are scene categories in the LSUN dataset.\n",
    "* Include data augmentation techniques, such as random cropping and horizontal flipping, to improve the model's performance on diverse scene images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.929710Z",
     "start_time": "2024-03-19T11:08:10.915710Z"
    },
    "id": "5IKNF22XbKYS"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 4. Train the model on the corresponding dataset</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_3zUr7xlhy"
   },
   "source": [
    "**For variants 1-3:** Train the defined model for CIFAR-100 classification.\n",
    "* Initialize the model designed for CIFAR-100 and transfer it to the appropriate device (CPU or GPU).\n",
    "* Choose a suitable optimizer, such as Adam or SGD, with an appropriate learning rate (e.g., 0.001 for Adam).\n",
    "* Use cross-entropy loss as the loss function since it is a multi-class classification task.\n",
    "* Implement a training loop that iterates over the training DataLoader, computes the loss, and updates the model parameters.\n",
    "* Include a validation step after each epoch to monitor the model's performance on the validation set. Use metrics such as accuracy to evaluate the model.\n",
    "* Apply learning rate scheduling or early stopping based on the validation performance to optimize the training process.\n",
    "* Save the best model based on validation accuracy for later evaluation on the test set.\n",
    "\n",
    "**For variants 4-6:** Train the defined model for FashionMNIST classification.\n",
    "* Initialize the model designed for FashionMNIST and transfer it to the appropriate device.\n",
    "* Choose a suitable optimizer, such as Adam, with a learning rate tailored to the simplicity of the dataset (e.g., 0.001).\n",
    "* Use cross-entropy loss as the loss function for this classification task.\n",
    "* Implement a training loop that processes batches of images and labels from the FashionMNIST DataLoader, computes the loss, and updates the model parameters.\n",
    "* Validate the model after each epoch using the validation DataLoader and track metrics such as accuracy.\n",
    "* Consider using a learning rate scheduler to adjust the learning rate during training based on the validation performance.\n",
    "* Save the model with the highest validation accuracy for evaluation on the test set.\n",
    "\n",
    "**For variants 7-9:** Training a semi-supervised model for STL-10.\n",
    "* Initialize the semi-supervised model designed for STL-10 and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as Adam, with a learning rate that balances exploration and exploitation (e.g., 0.0005).\n",
    "* Use a combination of supervised loss (cross-entropy) for labeled data and unsupervised loss (consistency loss) for unlabeled data.\n",
    "* Implement a training loop that alternates between processing batches of labeled and unlabeled data, computing the respective losses, and updating the model parameters.\n",
    "* Validate the model using the labeled validation set and track metrics such as accuracy.\n",
    "* Adjust the balance between supervised and unsupervised loss during training to effectively leverage the unlabeled data.\n",
    "* Save the best-performing model based on validation accuracy for testing.\n",
    "\n",
    "**For variants 10-12:** Train the defined model for SBU Captioned Photo captioning.\n",
    "* Initialize the image captioning model combining CNN and RNN architectures and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as Adam, with a learning rate suitable for the complexity of the task (e.g., 0.0001).\n",
    "* Use a loss function that combines image feature extraction loss (e.g., mean squared error) and caption generation loss (e.g., cross-entropy for word prediction).\n",
    "* Implement a training loop that processes batches of images and corresponding captions, computes the combined loss, and updates the model parameters.\n",
    "* Validate the model after each epoch using a validation set and track metrics such as BLEU score for caption quality.\n",
    "* Apply techniques such as teacher forcing or scheduled sampling to improve the caption generation during training.\n",
    "* Save the model with the highest performance on the validation set for later evaluation.\n",
    "\n",
    "**For variants 13-15:** Train the defined model for CIFAR-10 classification.\n",
    "* Initialize the model designed for CIFAR-10 and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as SGD with momentum, with a learning rate suitable for the dataset (e.g., 0.01 with a momentum of 0.9).\n",
    "* Use cross-entropy loss as the loss function for this classification task.\n",
    "* Implement a training loop that iterates over the training DataLoader, computes the loss, and updates the model parameters.\n",
    "* Validate the model after each epoch using the validation DataLoader and track accuracy.\n",
    "* Consider using techniques such as learning rate decay or early stopping based on the validation performance.\n",
    "* Save the model with the highest validation accuracy for testing on the test set.\n",
    "\n",
    "**For variants 16-18:** Train the defined model for SVHN digit recognition.\n",
    "* Initialize the model designed for SVHN and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as Adam, with a learning rate adapted to the dataset's characteristics (e.g., 0.001).\n",
    "* Use cross-entropy loss as the loss function for digit recognition.\n",
    "* Implement a training loop that processes batches of images and labels, computes the loss, and updates the model parameters.\n",
    "* Validate the model periodically using the validation set and track metrics such as accuracy.\n",
    "* Apply data augmentation techniques during training, such as random rotations or translations, to improve the model's robustness.\n",
    "* Save the best model based on validation accuracy for evaluation on the test set.\n",
    "\n",
    "**For variants 19-21:** Train the defined model for COCO object detection.\n",
    "* Initialize the object detection model designed for the COCO dataset and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as SGD with momentum, with a learning rate suitable for the complexity of the task (e.g., 0.002 with a momentum of 0.9).\n",
    "* Use a loss function that combines object localization loss (e.g., smooth L1 loss) and object classification loss (e.g., cross-entropy).\n",
    "* Implement a training loop that iterates over batches of images and annotations, computes the combined loss, and updates the model parameters.\n",
    "* Validate the model using the validation set and track metrics such as mean average precision (mAP) for object detection.\n",
    "* Apply techniques such as gradient clipping or learning rate scheduling to stabilize and optimize the training process.\n",
    "* Save the model with the best performance on the validation set for further evaluation and testing.\n",
    "\n",
    "**For variants 22-24:** Train the defined model for Oxford-IIIT Pet breed classification.\n",
    "* Initialize the model designed for pet breed classification and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as Adam, with a learning rate tailored to the fine-grained nature of the task (e.g., 0.0001).\n",
    "* Use cross-entropy loss as the loss function for this multi-class classification task.\n",
    "* Implement a training loop that processes batches of images and labels, computes the loss, and updates the model parameters.\n",
    "* Validate the model after each epoch using the validation set and track accuracy.\n",
    "* Consider using transfer learning techniques, such as fine-tuning a pre-trained model, to leverage learned features from a similar domain.\n",
    "* Save the best-performing model based on validation accuracy for testing on the test set.\n",
    "\n",
    "**For variants 25-27:** Train the defined model for CelebA facial attribute recognition.\n",
    "* Initialize the multi-label classification model designed for CelebA and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as Adam, with a learning rate suitable for the multi-label nature of the task (e.g., 0.0005).\n",
    "* Use binary cross-entropy loss as the loss function, as each attribute is treated as an independent binary classification problem.\n",
    "* Implement a training loop that iterates over batches of images and attribute labels, computes the loss, and updates the model parameters.\n",
    "* Validate the model using the validation set and track metrics such as average precision or F1 score for each attribute.\n",
    "* Apply class balancing techniques, such as weighted loss, to address the imbalance in attribute prevalence.\n",
    "* Save the model with the best performance on the validation set for further evaluation.\n",
    "\n",
    "**For variants 28-30:** Train the defined model for LSUN scene classification.\n",
    "* Initialize the scene classification model designed for the LSUN dataset and transfer it to the appropriate device.\n",
    "* Choose an optimizer, such as SGD with momentum, with a learning rate appropriate for the task (e.g., 0.01 with a momentum of 0.9).\n",
    "* Use cross-entropy loss as the loss function for this multi-class classification task.\n",
    "* Implement a training loop that processes batches of images and labels, computes the loss, and updates the model parameters.\n",
    "* Validate the model after each epoch using the validation set and track accuracy.\n",
    "* Consider using data augmentation techniques, such as random cropping and horizontal flipping, to improve the model's generalization.\n",
    "* Save the best-performing model based on validation accuracy for evaluation on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.945799Z",
     "start_time": "2024-03-19T11:08:10.931711Z"
    },
    "id": "jSo6vVWFbNLD"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 5. Make predictions</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1CsHhPpxp1w"
   },
   "source": [
    "**For variants 1-3:** Make predictions for CIFAR-100 classification.\n",
    "* Load the best model trained on the CIFAR-100 dataset and transfer it to the appropriate device.\n",
    "* Use the test DataLoader to iterate over the test set and collect the model's predictions. Ensure that the model is in evaluation mode to disable dropout and batch normalization effects.\n",
    "* Compare the predicted classes with the true labels and calculate the overall accuracy of the model on the test set.\n",
    "* For a more detailed analysis, compute the precision, recall, and F1-score for each class to understand the model's performance across different categories.\n",
    "* Visualize some test images along with their true and predicted labels to qualitatively assess the model's predictions.\n",
    "\n",
    "**For variants 4-6:** Make predictions for FashionMNIST classification.\n",
    "* Load the trained model for FashionMNIST and set it to evaluation mode.\n",
    "* Iterate over the test DataLoader, passing batches of test images through the model to obtain predictions.\n",
    "* Calculate the accuracy of the model by comparing the predicted labels with the true labels from the test set.\n",
    "* Analyze the confusion matrix to identify classes that the model might be confusing with each other.\n",
    "* Visualize some test images with their corresponding predicted and true labels to qualitatively evaluate the model's performance.\n",
    "\n",
    "**For variants 7-9:** Make predictions for semi-supervised STL-10 classification.\n",
    "* Load the trained semi-supervised model for STL-10 and set it to evaluation mode.\n",
    "* Use the labeled test DataLoader to obtain predictions for the test set. Since STL-10 also includes unlabeled data, ensure that only labeled data is used for evaluation.\n",
    "* Calculate the model's accuracy on the test set and consider other metrics such as precision and recall for a comprehensive evaluation.\n",
    "* Explore the model's performance on both labeled and unlabeled data to assess its ability to leverage unlabeled data for improved performance.\n",
    "\n",
    "**For variants 10-12:** Generating captions for SBU Captioned Photo dataset.\n",
    "* Load the trained image captioning model for the SBU Captioned Photo dataset.\n",
    "* For each test image, use the model to generate a caption. This might involve feeding the image through the CNN to obtain features, which are then used by the RNN to generate a sequence of words.\n",
    "* Evaluate the quality of the generated captions using metrics such as BLEU, METEOR, or CIDEr, which compare the generated captions with reference captions provided in the dataset.\n",
    "* Visualize some test images along with their generated captions and reference captions to qualitatively assess the model's captioning ability.\n",
    "\n",
    "**For variants 13-15:** Make predictions for CIFAR-10 classification.\n",
    "* Load the trained model for CIFAR-10 classification and set it to evaluation mode.\n",
    "* Use the test DataLoader to iterate over the test set, obtaining predictions from the model.\n",
    "* Calculate the accuracy of the model on the CIFAR-10 test set and consider computing class-specific metrics for a detailed analysis.\n",
    "* Visualize some test images with their predicted and true labels to get a sense of the model's prediction quality.\n",
    "\n",
    "**For variants 16-18:** Make predictions for SVHN digit recognition.\n",
    "* Load the trained model for SVHN digit recognition and set it to evaluation mode.\n",
    "* Iterate over the test DataLoader, passing batches of test images through the model to obtain digit predictions.\n",
    "* Calculate the model's accuracy on the test set and consider analyzing the confusion matrix to understand common prediction errors.\n",
    "* Visualize some test images with their corresponding predicted and true digit labels to qualitatively evaluate the model's performance.\n",
    "\n",
    "**For variants 19-21:** Make predictions for COCO object detection.\n",
    "* Load the trained object detection model for the COCO dataset and set it to evaluation mode.\n",
    "* Use the test DataLoader to process test images, obtaining predicted bounding boxes and class labels for each object in the image.\n",
    "* Evaluate the model's performance using mean average precision (mAP) over different IoU (Intersection over Union) thresholds, which is a standard metric for object detection tasks.\n",
    "* Visualize some test images with their predicted bounding boxes and class labels overlaid on the image to qualitatively assess the model's detection ability.\n",
    "\n",
    "**For variants 22-24:** Make predictions for Oxford-IIIT Pet breed classification.\n",
    "* Load the trained model for pet breed classification and set it to evaluation mode.\n",
    "* Iterate over the test DataLoader, obtaining predictions for each batch of test images.\n",
    "* Calculate the accuracy of the model on the test set and consider computing metrics such as precision, recall, and F1-score for each pet breed.\n",
    "* Visualize some test images with their predicted and true breed labels to qualitatively evaluate the model's classification performance.\n",
    "\n",
    "**For variants 25-27:** Make predictions for CelebA facial attribute recognition.\n",
    "* Load the trained multi-label classification model for CelebA and set it to evaluation mode.\n",
    "* Use the test DataLoader to obtain predictions for facial attributes in the test set.\n",
    "* Evaluate the model's performance using average precision or F1-score for each attribute, considering the multi-label nature of the task.\n",
    "* Visualize some test images with their predicted attributes and compare them with the true attributes to qualitatively assess the model's performance.\n",
    "\n",
    "**For variants 28-30:** Make predictions for LSUN scene classification.\n",
    "* Load the trained model for LSUN scene classification and set it to evaluation mode.\n",
    "* Iterate over the test DataLoader, obtaining scene class predictions for each test image.\n",
    "* Calculate the accuracy of the model on the test set and consider analyzing the confusion matrix to identify any challenging scene categories.\n",
    "* Visualize some test images with their predicted and true scene labels to qualitatively evaluate the model's classification ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.977768Z",
     "start_time": "2024-03-19T11:08:10.962768Z"
    },
    "id": "vSrXiT_AbQ6e"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 6. Fine-tune the models for the corresponding datasets</span>\n",
    "\n",
    "[Go back to the content](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj6bDhoWxt2y"
   },
   "source": [
    "**For variants 1-3:** Fine-tune a model for CIFAR-100 classification.\n",
    "* Load a pre-trained model, such as ResNet or VGG, and replace the final fully connected layer with a new layer that has 100 output units for CIFAR-100 classification.\n",
    "* Freeze the initial layers of the model and only fine-tune the last few layers and the newly added layer to adapt the model to the CIFAR-100 dataset.\n",
    "* Use a lower learning rate (e.g., 0.0001) for the fine-tuning process to prevent large updates that could destabilize the pre-trained weights.\n",
    "* Train the model on the CIFAR-100 training set with data augmentation techniques like random cropping and horizontal flipping.\n",
    "* Validate the fine-tuned model on the CIFAR-100 validation set and adjust the fine-tuning process based on the validation performance.\n",
    "\n",
    "**For variants 4-6:** Fine-tune a model for FashionMNIST classification.\n",
    "* Start with a pre-trained CNN model and modify the final layer to suit the 10 classes of the FashionMNIST dataset.\n",
    "* Fine-tune only the last few layers of the model to adapt it to the grayscale images and fashion items of the dataset.\n",
    "* Use a small learning rate for fine-tuning and a suitable optimizer like Adam or SGD with momentum.\n",
    "* Employ data augmentation techniques such as resizing and random rotations to improve the model's generalization.\n",
    "* Monitor the model's performance on a validation set during fine-tuning and make adjustments to the fine-tuning process as necessary.\n",
    "\n",
    "**For variants 7-9:** Fine-tuning a semi-supervised model for STL-10.\n",
    "* Use a pre-trained model as the backbone and add a new classification head suitable for the 10 classes of STL-10.\n",
    "* Fine-tune the model using both labeled and unlabeled data from the STL-10 dataset. For unlabeled data, use techniques like pseudo-labeling or consistency regularization to leverage the additional information.\n",
    "* Set a lower learning rate for the pre-trained layers and a higher learning rate for the new classification head to encourage learning new features specific to STL-10.\n",
    "* Evaluate the fine-tuned model on the labeled test set of STL-10 to assess its performance in a semi-supervised setting.\n",
    "\n",
    "**For variants 10-12:** Fine-tune a model for SBU Captioned Photo captioning.\n",
    "* Start with a pre-trained CNN for image feature extraction (e.g., ResNet) and a pre-trained RNN (e.g., LSTM) for text generation.\n",
    "* Replace the final layers of both the CNN and RNN to suit the specific requirements of the SBU Captioned Photo dataset, such as the vocabulary size and image dimensions.\n",
    "* Fine-tune the combined model on the SBU Captioned Photo training set, using the image-caption pairs to learn the mapping between visual features and textual descriptions.\n",
    "* Use a suitable loss function, such as cross-entropy loss for caption generation, and a learning rate that allows for gradual adaptation of the pre-trained weights.\n",
    "* Validate the fine-tuned model on the validation set and adjust the fine-tuning strategy based on the caption quality and other relevant metrics.\n",
    "\n",
    "**For variants 13-15:** Fine-tune a model for CIFAR-10 classification.\n",
    "* Utilize a pre-trained CNN model and modify its final layer to classify the 10 classes of CIFAR-10.\n",
    "* Fine-tune the last few layers of the model to adapt it to the CIFAR-10 dataset, while keeping the initial layers frozen to retain the learned features.\n",
    "* Employ a small learning rate for fine-tuning to ensure that the pre-trained weights are not drastically altered.\n",
    "* Apply data augmentation techniques such as random cropping and horizontal flipping to enhance the model's robustness.\n",
    "* Monitor the fine-tuned model's performance on a validation set and make necessary adjustments to the fine-tuning process.\n",
    "\n",
    "**For variants 16-18:** Fine-tune a model for SVHN digit recognition.\n",
    "* Use a pre-trained CNN model and adjust the output layer to predict the 10 digit classes of the SVHN dataset.\n",
    "* Fine-tune the model using the SVHN training set, focusing on adapting the model to recognize digits in natural scene images.\n",
    "* Employ a reduced learning rate for fine-tuning to prevent overfitting and to preserve the generalizability of the pre-trained model.\n",
    "* Consider using grayscale conversion as a pre-processing step to match the input format of the SVHN dataset.\n",
    "* Evaluate the fine-tuned model on the SVHN test set to assess its digit recognition performance.\n",
    "\n",
    "**For variants 19-21:** Fine-tune a model for COCO object detection.\n",
    "* Start with a pre-trained object detection model, such as Faster R-CNN or YOLO, and modify it to detect objects in the COCO dataset.\n",
    "* Fine-tune the model on the COCO training set, focusing on learning to detect a wide range of objects in various contexts.\n",
    "* Use a learning rate that allows for gradual adaptation of the model to the new task while preserving the pre-trained features.\n",
    "* Employ data augmentation techniques like random resizing and flipping to improve the model's ability to generalize to different object sizes and orientations.\n",
    "* Validate the fine-tuned model on the COCO validation set and use metrics like mean average precision (mAP) to evaluate its performance.\n",
    "\n",
    "**For variants 22-24:** Fine-tune a model for Oxford-IIIT Pet breed classification.\n",
    "* Utilize a pre-trained CNN, such as ResNet or VGG, and customize the output layer to classify the pet breeds in the Oxford-IIIT Pet dataset.\n",
    "* Fine-tune the model on the pet breed classification task, focusing on adapting the pre-trained features to recognize fine-grained differences between breeds.\n",
    "* Set a lower learning rate for fine-tuning to ensure that the pre-trained weights are fine-tuned without significant deviations.\n",
    "* Apply data augmentation techniques, such as random cropping and rotation, to enhance the model's ability to recognize breeds in various poses and orientations.\n",
    "* Monitor the fine-tuned model's performance on the validation set and adjust the fine-tuning strategy based on the classification accuracy.\n",
    "\n",
    "**For variants 25-27:** Fine-tune a model for CelebA facial attribute recognition.\n",
    "* Start with a pre-trained CNN model and adjust the output layer to predict multiple facial attributes in the CelebA dataset.\n",
    "* Fine-tune the model using the CelebA training set, focusing on learning to recognize various facial attributes simultaneously.\n",
    "* Use a small learning rate for fine-tuning to prevent large updates that could destabilize the pre-trained weights.\n",
    "* Consider using a multitask learning approach, where each facial attribute is treated as a separate task, to improve the model's ability to learn correlated attributes.\n",
    "* Evaluate the fine-tuned model on the validation set using metrics such as average precision or F1-score for each attribute.\n",
    "\n",
    "**For variants 28-30:** Fine-tune a model for LSUN scene classification.\n",
    "* Utilize a pre-trained CNN, such as ResNet or VGG, and modify the output layer to classify scenes in the LSUN dataset.\n",
    "* Fine-tune the model on the LSUN training set, focusing on adapting the model to recognize various indoor and outdoor scenes.\n",
    "* Employ a reduced learning rate for fine-tuning to ensure that the pre-trained features are adapted without drastic changes.\n",
    "* Apply data augmentation techniques, such as random cropping and horizontal flipping, to improve the model's ability to generalize to different scene configurations.\n",
    "* Monitor the fine-tuned model's performance on the validation set and adjust the fine-tuning process based on the scene classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T11:08:10.992906Z",
     "start_time": "2024-03-19T11:08:10.979770Z"
    },
    "id": "leCTsqtSbR5P"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.7\"></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_pytorch_computer_vision_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
