{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratory work 5.</center></h1>\n",
    "<h2><center>PyTorch Custom Datasets Exercises</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performed:** Last name and First name\n",
    "\n",
    "**Variant:** #__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Task 1. Preparing data](#5.1)\n",
    "2. [Task 2. Creating a model](#5.2)\n",
    "3. [Task 3. Training and testing loops](#5.3)\n",
    "4. [Task 4. Conducting experiments with hyperparameters](#5.4)\n",
    "5. [Task 5. Conducting experiments with the model's layers](#5.5)\n",
    "6. [Task 6. Conducting experiments with the data](#5.6)\n",
    "7. [Task 7. Making predictions](#5.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DNwZLMbCzJLk",
    "outputId": "1a7b73e2-ec4b-41b0-b4da-c3216a8a29ac"
   },
   "outputs": [],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Exercises require PyTorch > 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-90Qvx9gtrLj"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 1. Preparing data</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For all variants:** Recreate the data loading functions we built in sections 1-4 of [notebook 05](https://github.com/radiukpavlo/conducting-experiments/blob/main/01_notebooks/ce_05_pytorch_custom_datasets.ipynb). By this time, you should have had the trained and tested `DataLoader`'s ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZkCPJBR3lw4"
   },
   "outputs": [],
   "source": [
    "# 1. Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYmhAX7J52VX"
   },
   "outputs": [],
   "source": [
    "# 2. Become one with the data\n",
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"Walks through dir_path returning file counts of its contents.\"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A9ZmOn-7Jhh"
   },
   "outputs": [],
   "source": [
    "# Setup train and testing paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51ywNKkN7WOl"
   },
   "outputs": [],
   "source": [
    "# Visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe4LoASC9sQ-"
   },
   "outputs": [],
   "source": [
    "# Do the image visualization with matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AU4FGYC_KBz"
   },
   "source": [
    "We've got some images in our folders.\n",
    "\n",
    "Now we need to make them compatible with PyTorch by:\n",
    "1. Transform the data into tensors.\n",
    "2. Turn the tensor data into a `torch.utils.data.Dataset` and later a `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbGMaYGT-vwq"
   },
   "outputs": [],
   "source": [
    "# 3.1 Transforming data with torchvision.transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnvUSYYW_ohN"
   },
   "outputs": [],
   "source": [
    "# Write transform for turning images into tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp8I2cpMAxcT"
   },
   "outputs": [],
   "source": [
    "# Write a function to plot transformed images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKgfqPArChVR"
   },
   "source": [
    "### Load image data using `ImageFolder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OFgwQF1CkOu"
   },
   "outputs": [],
   "source": [
    "# Use ImageFolder to create dataset(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbT0fhXHEQyJ",
    "outputId": "ad2b0da9-285c-493d-c362-a6b00e5d2197"
   },
   "outputs": [],
   "source": [
    "# Get class names as a list\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCcWk7NDEay1",
    "outputId": "88718fd6-c6b2-4132-9383-0a64a0a7bcee"
   },
   "outputs": [],
   "source": [
    "# Can also get class names as a dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7H7bX4HEgie",
    "outputId": "2ff1fdd2-f990-461d-c2e9-27435e7744c9"
   },
   "outputs": [],
   "source": [
    "# Check the lengths of each dataset\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nskNr5YCEoRl",
    "outputId": "cd825b1d-095f-4080-86ce-7d17aafef7e8"
   },
   "outputs": [],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8vJxmxAFqw6",
    "outputId": "e5a8bd88-1b05-4109-de5a-8183a85c7872"
   },
   "outputs": [],
   "source": [
    "# How many batches of images are in our data loaders?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 2. Creating a model</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Enhance the efficiency of TinyVGG by integrating depthwise separable convolutions.\n",
    "* Replace standard convolutional layers (`torch.nn.Conv2d`) in TinyVGG with depthwise separable convolutions (`torch.nn.Conv2d` for depthwise and pointwise convolutions). This reduces the computational load and model size by decomposing convolutions into spatial and depth components.\n",
    "* It is beneficial for deploying models on devices with limited computational resources, such as mobile phones or IoT devices, without substantially sacrificing accuracy.\n",
    "* Configure the model parameters to adjust the depth multiplier and the pointwise convolution to ensure it fits the needs of the food classification task.\n",
    "\n",
    "**For variants 4-6:** Improve training stability and accelerate convergence using batch normalization layers.\n",
    "* Insert a `torch.nn.BatchNorm2d` layer after each convolutional layer in the TinyVGG architecture. Batch normalization normalizes the activations of the previous layer at each batch, maintaining a mean output close to 0 and an output standard deviation close to 1.\n",
    "* This adjustment helps in reducing internal covariate shift, which can speed up training and lead to higher overall accuracy.\n",
    "* Ensure that hyperparameters such as momentum and epsilon are tuned for optimal performance specific to the dataset.\n",
    "\n",
    "**For variants 7-9:** Incorporate dropout layers to prevent overfitting.\n",
    "* Add `torch.nn.Dropout` layers following each activation layer in the TinyVGG setup. Dropout randomly zeros some elements of the input tensor with probability p during training, acting as a form of regularization.\n",
    "* This approach is especially useful when dealing with a small or highly similar dataset, where the model is at a higher risk of overfitting.\n",
    "* Adjust the dropout rate based on the validation set performance to find the right balance between learning and regularization.\n",
    "\n",
    "**For variants 10-12:** Implement residual connections within the TinyVGG to enable training of deeper networks by mitigating the vanishing gradient problem.\n",
    "\n",
    "* Add residual connections by incorporating skip connections that add the input of a convolution block to its output, facilitating deeper architectures without degradation in training performance.\n",
    "* Use `torch.nn.Identity` or direct tensor addition to implement these connections.\n",
    "* This setup allows for the potential increase in model depth, enhancing its ability to learn more complex features without suffering from training difficulties.\n",
    "\n",
    "**For variants 13-15:** Expand the receptive field using dilated convolutions to capture a broader context without increasing the number of parameters.\n",
    "* Replace some of the standard convolution layers in TinyVGG with dilated convolutions (`torch.nn.Conv2d` with a dilation parameter greater than 1). This helps in increasing the receptive field of the model, allowing it to incorporate context from a larger area of the input.\n",
    "* Dilated convolutions are particularly useful for images where understanding broader spatial relationships is beneficial for classification accuracy.\n",
    "* Carefully adjust dilation rates and layer configurations to optimize performance without causing gridding artifacts.\n",
    "\n",
    "**For variants 16-18:** Integrate attention mechanisms to focus the model more on relevant parts of the image.\n",
    "* Embed attention modules such as Squeeze-and-Excitation (SE) blocks within the TinyVGG structure. These blocks reweight channel-wise features by explicitly modeling interdependencies between channels, enhancing the representational power of the network.\n",
    "* Such attention mechanisms can help the model to focus on more informative features and improve the classification of complex food images where certain features are more discriminative than others.\n",
    "* Experiment with the placement and configuration of SE blocks to maximize their effect without overwhelming the network's capacity.\n",
    "\n",
    "**For variants 19-21:** Aggregate features from multiple scales to improve model robustness and accuracy.\n",
    "* Modify TinyVGG to incorporate feature aggregation from different layers, using techniques like feature pyramids or concatenated outputs from different stages of the network.\n",
    "* This allows the model to leverage both low-level details and high-level semantic information, which is crucial for accurately classifying food items that can vary significantly in appearance at different scales.\n",
    "* Implement custom forward methods to handle multiscale feature aggregation and ensure that the feature dimensions are compatible for concatenation or merging.\n",
    "\n",
    "**For variants 22-24:** Explore the impact of different activation functions on the performance of the TinyVGG model.\n",
    "* Experiment with advanced activation functions beyond ReLU, such as LeakyReLU, PReLU, or Swish, which may offer advantages in certain scenarios by providing non-linearities that can improve learning dynamics.\n",
    "* Integrate these activation functions into the TinyVGG model by replacing all ReLU layers. Each activation function has unique characteristics: LeakyReLU allows a small, non-zero gradient when the unit is not active, potentially helping with the dying ReLU problem; PReLU introduces learnable parameters that allow it to adapt its shape during training; Swish, being a smooth function, often provides benefits in deeper networks.\n",
    "* Evaluate the impact of these changes through experimentation, analyzing how they affect convergence speed and overall accuracy on the food classification task.\n",
    "\n",
    "**For variants 25-27:** Modify TinyVGG to prioritize inference speed, suitable for real-time applications.\n",
    "* Apply techniques like layer fusion, precision reduction (e.g., using half-precision floats), and pruning to reduce the computational cost and model size. This makes the model more suitable for deployment in environments where computational resources or latency is a concern, such as mobile apps.\n",
    "* Consider integrating platform-specific optimizations if the deployment target is known, using libraries like ONNX or TensorRT, which can provide significant speedups by optimizing network graphs.\n",
    "* Thoroughly test the optimized model to ensure that these speed enhancements do not unduly compromise classification accuracy.\n",
    "\n",
    "**For variants 28-30:** Combine multiple TinyVGG models in an ensemble to improve robustness and accuracy.\n",
    "* Train multiple TinyVGG models with variations in initialization, data shuffling, and hyperparameters to create a diverse set of classifiers.\n",
    "* Use ensemble techniques such as averaging, voting, or stacking to combine the outputs of these models. Averaging can reduce variance in predictions; voting can be used for a robust majority rule in classification; stacking uses another model to learn the optimal combination of classifiers.\n",
    "* This approach often results in better performance than any single model, especially in complex classification tasks like food recognition, where different models might learn to focus on different aspects of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 3. Training and testing loops</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Implement dynamic adjustment of the learning rate during training to improve convergence and avoid overshooting minima.\n",
    "* Use a learning rate scheduler from `torch.optim.lr_scheduler` to adjust the learning rate as training progresses. Start with a higher learning rate to quickly converge towards the general vicinity of the optimum, and then decrease it progressively to fine-tune the model’s parameters without overshooting.\n",
    "* Apply `StepLR` to decrease the learning rate by a certain factor every few epochs, or use `ExponentialLR` for a steady exponential decay, or `ReduceLROnPlateau` to reduce the learning rate when the validation loss plateaus, indicating that the model might benefit from more subtle updates.\n",
    "\n",
    "**For variants 4-6:** Integrate an early stopping mechanism to halt training when the model begins to overfit.\n",
    "* Monitor the model's performance on a validation set at the end of each epoch. If the validation loss fails to improve or starts to increase over several consecutive epochs, terminate training early. This prevents the model from learning noise and non-generalizable patterns present in the training data.\n",
    "* Implement early stopping using a custom function that tracks the best validation loss observed during training and counts the number of epochs since it last improved. If this count exceeds a predefined threshold (patience), stop the training process.\n",
    "\n",
    "**For variants 7-9:** Apply gradient clipping during training to prevent the exploding gradient problem, which can lead to destabilized learning processes.\n",
    "* Use `torch.nn.utils.clip_grad_norm_` or `torch.nn.utils.clip_grad_value_` to clip the gradients during backpropagation. This keeps them within a manageable range and prevents the gradients from growing too large, which can cause the model parameters to oscillate wildly or diverge.\n",
    "* Gradient clipping is particularly useful in training deep networks or networks with recurrent layers, where gradients can grow exponentially through time or depth.\n",
    "* Incorporate this modification into the training loop, applying the clipping right after computing gradients (`loss.backward()`) and before updating the model parameters (`optimizer.step()`). Experiment with different clipping thresholds to find a balance that minimizes the impact on the natural training dynamics of the model while preventing instability.\n",
    "\n",
    "**For variants 10-12:** Utilize mixed precision training to speed up the training process and reduce memory usage while maintaining the model's performance.\n",
    "* Leverage PyTorch’s `torch.cuda.amp` for automatic mixed precision (AMP). This module allows certain parts of the model to use lower precision (float16) calculations, which can be processed faster on compatible hardware, while maintaining critical parts of the model in higher precision (float32) to preserve accuracy.\n",
    "* Use `amp.GradScaler` to manage the scaling of the loss value to prevent issues with small gradients that can underflow when using float16.\n",
    "* Integrate AMP into the training loop by wrapping the forward and backward passes in an `amp.autocast()` context manager to enable/disable automatic casting for specific layers or operations. This strategy helps in achieving faster training times and reducing GPU memory consumption without significant loss in model accuracy or training stability.\n",
    "\n",
    "**For variants 13-15:** Utilize adaptive gradient algorithms to dynamically adjust learning rates at the parameter level, improving convergence speeds and model robustness.\n",
    "* Implement an optimizer like Adam or RMSprop, which adjusts the learning rate for each parameter based on estimates of the first and second moments of the gradients. This method helps in handling sparse gradients and different scales of parameters effectively.\n",
    "* Set up Adam with specific hyperparameters such as `beta1`, `beta2`, and `epsilon`. These control the decay rates of the moving averages and the term added to improve numerical stability, respectively.\n",
    "* Integrate this optimizer into your training loop. Monitor the effect on training dynamics, specifically looking at how quickly and smoothly the model converges compared to using standard SGD. Adjust the learning rate and other parameters based on empirical results.\n",
    "\n",
    "**For variants 16-18:** Gradually increase the complexity of training data, simulating a learning \"curriculum\" to help the model learn more effectively.\n",
    "* Start with simpler or smaller subsets of the training data and gradually introduce more complex or larger batches as the training progresses. This can be implemented by sorting the training data by some measure of complexity (e.g., image resolution, the presence of noise, etc.) or by modifying the data loader to emit progressively more challenging examples.\n",
    "* Use a scheduling mechanism to increase complexity, such as increasing the size of the input data or the number of classes the model needs to predict after certain epochs.\n",
    "* Experiment with different metrics for defining complexity and schedules for introducing new challenges to optimize training outcomes.\n",
    "\n",
    "**For variants 19-21:** Improve generalization by averaging multiple points along the trajectory of SGD, capturing a wider \"ensemble\" of models.\n",
    "* Integrate SWA by replacing the conventional training loop's final phase with a process where the model weights are periodically averaged. This typically begins after the model has initially converged using standard training methods.\n",
    "* Implement SWA by using a custom or available SWA optimizer in PyTorch, which manages the averaging process automatically. Adjust the frequency of updates and the number of cycles based on validation performance.\n",
    "* SWA often leads to better generalization and more stable predictions, as it smooths out sharp minima in the loss landscape that are sensitive to small perturbations in inputs or parameters.\n",
    "\n",
    "**For variants 22-24:** Tailor the loss function to address specific characteristics or challenges of the dataset, such as class imbalance or outliers.\n",
    "* Develop and integrate custom loss functions that can better reflect the importance of certain examples or balance the influence of different classes. For instance, use a weighted cross-entropy loss where weights are inversely proportional to class frequencies.\n",
    "* Include mechanisms to handle outliers, such as using a robust loss function like Huber loss, which is less sensitive to outliers than squared error loss.\n",
    "* Test different configurations of the loss function to identify the best setup for balancing learning across the diverse elements of the dataset, thereby enhancing model accuracy and robustness.\n",
    "\n",
    "**For variants 25-27:** Implement checkpointing to save the model at various stages during training, allowing recovery and fine-tuning from specific states.\n",
    "* Set up a checkpointing system that periodically saves the model's state, including the weights, optimizer state, and current epoch number. This is crucial for long training sessions or when using expensive computational resources, as it allows training to resume from the last checkpoint in case of a failure.\n",
    "* Optionally, use model snapshots at different points (e.g., every 5 epochs) to evaluate how the model's performance evolves over time and to choose the best model based on validation metrics rather than just the last or best epoch.\n",
    "* Use PyTorch’s `torch.save` and `torch.load` for efficient management of checkpoints.\n",
    "\n",
    "**For variants 28-30:** Adjust the batch size dynamically during training to find an optimal balance between learning stability and computational efficiency.\n",
    "* Start with a smaller batch size and increase it as training progresses. This can help in stabilizing the initial learning phase when the model is more sensitive to noisy gradients, then scale up to exploit computational efficiencies of larger batch sizes once the training stabilizes.\n",
    "* Implement a schedule or a performance-based trigger for adjusting the batch size, such as increasing the batch size when the training loss decreases consistently or plateaus. This approach can help in optimizing the use of GPU memory and computational power throughout the training process.\n",
    "* Monitor the impact of dynamic batch sizing on training speed and model accuracy. Larger batches provide more stable but potentially less accurate gradient estimates, while smaller batches can lead to noisier updates but might escape suboptimal local minima more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnUox1qayDes"
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  \n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader and data batches\n",
    " \n",
    "    # Send data to target device\n",
    "\n",
    "    # 1. Forward pass\n",
    "    \n",
    "    # 2. Calculate and accumulate loss\n",
    "    \n",
    "\n",
    "    # 3. Optimizer zero grad \n",
    "    \n",
    "\n",
    "    # 4. Loss backward \n",
    "    \n",
    "\n",
    "    # 5. Optimizer step\n",
    "    \n",
    "\n",
    "    # Calculate and accumualte accuracy metric across all batches\n",
    "   \n",
    "\n",
    "  # Adjust metrics to get average loss and average accuracy per batch\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7_EVPpHNKUP"
   },
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "  \n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Setup the test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  \n",
    "    # Loop through DataLoader batches\n",
    "    \n",
    "      # Send data to target device\n",
    "      \n",
    "\n",
    "      # 1. Forward pass\n",
    "      \n",
    "\n",
    "      # 2. Calculuate and accumulate loss\n",
    "\n",
    "\n",
    "      # Calculate and accumulate accuracy\n",
    "\n",
    "    \n",
    "  # Adjust metrics to get average loss and accuracy per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXxTIh9tOh68"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "  \n",
    "  # Create results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "             \"test_loss\": [],\n",
    "             \"test_acc\": []}\n",
    "\n",
    "  # Loop through the training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    # Train step\n",
    "    train_loss, train_acc = train_step(model=model, \n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer)\n",
    "    # Test step\n",
    "    test_loss, test_acc = test_step(model=model, \n",
    "                                    dataloader=test_dataloader,\n",
    "                                    loss_fn=loss_fn)\n",
    "    \n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Update the results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the results dictionary\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 4. Conducting experiments with hyperparameters</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Explore the effects of varying learning rates and the addition of weight decay in the optimization process.\n",
    "* Conduct experiments by training the same model with different learning rates, such as 0.01, 0.001, and 0.0001, combined with weight decay values of 0, 0.0001, and 0.001. The Adam optimizer is used for this purpose, as it handles sparse gradients and adaptive learning rate well.\n",
    "* Weight decay adds a regularization term to the loss function that helps in reducing overfitting by penalizing large weights. Experimenting with weight decay can show its effect on the generalization ability of the model.\n",
    "* Use metrics such as validation loss and accuracy to assess the impact of different learning rates and weight decay configurations. Plotting these metrics over epochs will help visualize trends and identify the best configurations for balance between speed of convergence and model accuracy.\n",
    "\n",
    "**For variants 4-6:** Investigate the impact of different batch sizes on model training dynamics and performance.\n",
    "* Train your model with varying batch sizes, such as 32, 64, and 128, to understand how they affect the learning process, computational demand, and model performance. Larger batch sizes provide more accurate estimates of the gradient but require more memory and computational power.\n",
    "* Monitor changes in training and validation loss, as well as accuracy across different batch sizes. Additionally, observe the GPU utilization and training time per epoch to evaluate the computational trade-offs.\n",
    "* This experiment will help in finding an optimal batch size that balances between efficient use of computational resources and model performance, especially important in scenarios where resources are limited or costs are a concern.\n",
    "\n",
    "**For variants 7-9:** Examine how different activation functions influence model training and accuracy.\n",
    "* Modify the model to use various activation functions such as ReLU, LeakyReLU, and ELU. Each of these functions has different properties; for example, LeakyReLU allows a small gradient when the unit is inactive, which can help mitigate the dying ReLU problem.\n",
    "* Train the model using each activation function and compare performance metrics like training loss, validation loss, and final accuracy. It's crucial to observe not just the performance but also how quickly each model converges and its behavior during training (e.g., whether it exhibits more stable or erratic loss reductions).\n",
    "* Analyze the results to determine which activation function performs best with your specific model architecture and dataset. This experiment can reveal insights into the non-linear dynamics of your model and how they affect learning and generalization.\n",
    "\n",
    "**For variants 10-12:** Compare different optimizers to see how they impact model performance and training speed.\n",
    "* Set up experiments to train the model using different optimizers such as SGD, Adam, and RMSprop. Each optimizer has distinct mechanisms, for instance, SGD maintains a constant learning rate and benefits from manual tuning, while Adam and RMSprop adaptively adjust the learning rates based on running averages of recent gradients.\n",
    "* Evaluate how each optimizer influences the rate of convergence, stability of training, and final model accuracy. Also, consider factors like the ease of reaching a satisfactory solution and the sensitivity to initial conditions or hyperparameter settings.\n",
    "* Collecting data on training duration, epochs needed to converge, and performance on a held-out validation set will provide a comprehensive view of the strengths and weaknesses of each optimizer.\n",
    "\n",
    "**For variants 13-15:** Experiment with different optimization algorithms and learning rates.\n",
    "\n",
    "Instead of using `torch.optim.Adam()`, try training the model from Task 2 with the following optimizers:\n",
    "* `torch.optim.SGD()` with a learning rate of 0.01\n",
    "* `torch.optim.RMSprop()` with a learning rate of 0.001\n",
    "* `torch.optim.Adagrad()` with a learning rate of 0.01\n",
    "\n",
    "Train the model for 20 epochs with each optimizer and observe the impact on the training and testing loss and accuracy. Additionally, try varying the learning rate for each optimizer (e.g., 0.001, 0.005, 0.01) and note the differences in performance.\n",
    "* Import the required optimization algorithms from `torch.optim`.\n",
    "* Create a list of tuples containing the optimizer instances and corresponding learning rates.\n",
    "* Iterate over the list, training the model for 20 epochs with each optimizer and learning rate combination.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which optimizer and learning rate combination works best for your model.\n",
    "\n",
    "**For variants 16-18:** Experiment with different loss functions.\n",
    "\n",
    "Instead of using `nn.CrossEntropyLoss()`, try training the model from Task 2 with the following loss functions:\n",
    "* `nn.NLLLoss()` (Negative Log Likelihood Loss)\n",
    "* `nn.BCELoss()` (Binary Cross-Entropy Loss)\n",
    "* `nn.MSELoss()` (Mean Squared Error Loss)\n",
    "\n",
    "Train the model for 20 epochs with each loss function and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required loss functions from `torch.nn`.\n",
    "* Create a list containing the loss function instances.\n",
    "* Iterate over the list, training the model for 20 epochs with each loss function.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which loss function works best for your model and dataset.\n",
    "\n",
    "**For variants 19-21:** Experiment with different regularization techniques to prevent overfitting.\n",
    "\n",
    "Try incorporating the following regularization methods into your model from Task 2:\n",
    "* L2 regularization (weight decay)\n",
    "* Dropout\n",
    "* Early stopping\n",
    "\n",
    "Train the model for 50 epochs with each regularization technique and observe the impact on the training and testing loss and accuracy.\n",
    "* Implement L2 regularization by adding a `weight_decay` parameter to your optimizer.\n",
    "* Incorporate dropout layers into your model by adding `nn.Dropout()` layers after the linear layers.\n",
    "* Implement early stopping by monitoring the validation loss and stopping the training when it starts to increase.\n",
    "* Train the model for 50 epochs with each regularization technique.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which regularization technique works best for preventing overfitting in your model.\n",
    "\n",
    "**For variants 22-24:** Experiment with different batch sizes and data augmentation techniques.\n",
    "\n",
    "Try training the model from Task 2 with the following configurations:\n",
    "* Batch size of 32 without data augmentation\n",
    "* Batch size of 64 without data augmentation\n",
    "* Batch size of 32 with data augmentation (e.g., random flips, rotations, and crops)\n",
    "* Batch size of 64 with data augmentation (e.g., random flips, rotations, and crops)\n",
    "\n",
    "Train the model for 20 epochs with each configuration and observe the impact on the training and testing loss and accuracy.\n",
    "* Create different `DataLoader` instances with various batch sizes.\n",
    "* Implement data augmentation techniques using `torchvision.transforms`.\n",
    "* Create a list containing the different configurations (batch size and data augmentation).\n",
    "* Iterate over the list, training the model for 20 epochs with each configuration.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which batch size and data augmentation combination works best for your model and dataset.\n",
    "\n",
    "**For variants 25-27:** Experiment with different activation functions and weight initialization techniques.\n",
    "\n",
    "Try training the model from Task 2 with the following configurations:\n",
    "* ReLU activation function and Xavier weight initialization\n",
    "* Leaky ReLU activation function and Kaiming weight initialization\n",
    "* ELU activation function and Xavier normal weight initialization\n",
    "\n",
    "Train the model for 20 epochs with each configuration and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required activation functions from `torch.nn`.\n",
    "* Implement the different weight initialization techniques using `torch.nn.init`.\n",
    "* Create a list containing the different configurations (activation function and weight initialization).\n",
    "* Iterate over the list, modifying the model accordingly and training it for 20 epochs with each configuration.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which activation function and weight initialization combination works best for your model and dataset.\n",
    "\n",
    "**For variants 28-30:** Experiment with different learning rate scheduling techniques.\n",
    "\n",
    "Instead of using a fixed learning rate, try training the model from Task 2 with the following learning rate schedulers:\n",
    "* `torch.optim.lr_scheduler.StepLR()`: Decays the learning rate by a factor every specified number of epochs.\n",
    "* `torch.optim.lr_scheduler.ReduceLROnPlateau()`: Decays the learning rate when the validation loss plateaus.\n",
    "* `torch.optim.lr_scheduler.CosineAnnealingLR()`: Decays the learning rate following a cosine annealing schedule.\n",
    "\n",
    "Train the model for 50 epochs with each learning rate scheduler and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required learning rate scheduler from `torch.optim.lr_scheduler`.\n",
    "* Create an instance of the optimizer (e.g., `torch.optim.Adam()`).\n",
    "* Create an instance of the learning rate scheduler, passing the optimizer as an argument.\n",
    "* In the training loop, update the learning rate after each epoch using the scheduler's `step()` method.\n",
    "* Record the training and testing loss and accuracy for each experiment.\n",
    "* Analyze and compare the results to determine which learning rate scheduling technique works best for your model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model might be starting to overfit towards the end (performing far better on the training data than on the testing data).\n",
    "\n",
    "In order to fix this, we'd have to introduce ways of preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 5. Conducting experiments with the model's layers</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Experiment with different types of convolutional layers.\n",
    "\n",
    "Instead of using the standard convolutional layers in your model from Task 2, try replacing them with the following types of convolutional layers:\n",
    "* Depthwise Separable Convolutions\n",
    "* Dilated Convolutions\n",
    "* Transposed Convolutions (for upsampling)\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required convolutional layers from `torch.nn`.\n",
    "* Modify the existing model architecture by replacing the standard convolutional layers with the new types of convolutional layers.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different convolutional layer types.\n",
    "\n",
    "**For variants 4-6:** Experiment with different types of pooling layers.\n",
    "\n",
    "Instead of using the standard max pooling layers in your model from Task 2, try replacing them with the following types of pooling layers:\n",
    "* Average Pooling\n",
    "* Adaptive Max Pooling\n",
    "* Adaptive Average Pooling\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required pooling layers from `torch.nn`.\n",
    "* Modify the existing model architecture by replacing the max pooling layers with the new types of pooling layers.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different pooling layer types.\n",
    "\n",
    "**For variants 7-9:** Experiment with different types of normalization layers.\n",
    "\n",
    "Instead of using batch normalization in your model from Task 2, try replacing it with the following types of normalization layers:\n",
    "* Layer Normalization\n",
    "* Instance Normalization\n",
    "* Group Normalization\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required normalization layers from `torch.nn`.\n",
    "* Modify the existing model architecture by replacing the batch normalization layers with the new types of normalization layers.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different normalization layer types.\n",
    "\n",
    "**For variants 10-12:** Experiment with different types of attention mechanisms.\n",
    "\n",
    "Instead of using standard convolutional and fully connected layers in your model from Task 2, try incorporating the following attention mechanisms:\n",
    "* Self-Attention\n",
    "* Squeeze-and-Excitation Attention\n",
    "* Convolutional Block Attention Module (CBAM)\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required attention modules from PyTorch or implement them manually.\n",
    "* Modify the existing model architecture by incorporating the attention mechanisms at appropriate locations.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different attention mechanisms.\n",
    "\n",
    "**For variants 13-15:** Experiment with different types of activation functions.\n",
    "\n",
    "Instead of using the ReLU activation function in your model from Task 2, try replacing it with the following activation functions:\n",
    "* Leaky ReLU\n",
    "* ELU (Exponential Linear Unit)\n",
    "* Swish\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required activation functions from `torch.nn`.\n",
    "* Modify the existing model architecture by replacing the ReLU activation function with the new activation functions.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different activation functions.\n",
    "\n",
    "**For variants 16-18:** Experiment with different types of recurrent layers.\n",
    "\n",
    "Instead of using a feed-forward neural network architecture in your model from Task 2, try incorporating the following recurrent layer types:\n",
    "* Long Short-Term Memory (LSTM)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Bidirectional LSTM\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required recurrent layers from `torch.nn`.\n",
    "* Modify the existing model architecture by replacing the feed-forward layers with the recurrent layer types.\n",
    "* Preprocess the input data to be suitable for the recurrent layers (e.g., sequences of images or text).\n",
    "* Train the modified model for 20 epochs using the preprocessed data and appropriate hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different recurrent layer types.\n",
    "\n",
    "**For variants 19-21:** Experiment with different types of skip connections.\n",
    "\n",
    "Instead of using a standard feed-forward architecture in your model from Task 2, try incorporating the following skip connection types:\n",
    "* Residual Connections (as in ResNet)\n",
    "* Dense Connections (as in DenseNet)\n",
    "* Inception Modules (as in GoogLeNet/Inception)\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required modules or implement the skip connection types manually.\n",
    "* Modify the existing model architecture by incorporating the skip connection types at appropriate locations.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different skip connection types.\n",
    "\n",
    "**For variants 22-24:** Experiment with different types of multi-head attention layers.\n",
    "\n",
    "Instead of using standard attention mechanisms in your model from Task 2 (if applicable), try incorporating the following multi-head attention layer types:\n",
    "* Scaled Dot-Product Attention\n",
    "* Multi-Head Attention (as in Transformers)\n",
    "* Convolutional Multi-Head Attention\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required multi-head attention layers from PyTorch or implement them manually.\n",
    "* Modify the existing model architecture by incorporating the multi-head attention layer types at appropriate locations.\n",
    "* Train the modified model for 20 epochs using the same data and hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different multi-head attention layer types.\n",
    "\n",
    "**For variants 25-27:** Experiment with different types of generative layers.\n",
    "Instead of using a discriminative model architecture in your model from Task 2, try incorporating the following generative layer types:\n",
    "* Variational Autoencoder (VAE)\n",
    "* Generative Adversarial Network (GAN)\n",
    "* Autoregressive Layers (as in PixelCNN or WaveNet)\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy (if applicable).\n",
    "* Import the required generative layers from PyTorch or implement them manually.\n",
    "* Modify the existing model architecture by incorporating the generative layer types at appropriate locations.\n",
    "* Preprocess the data to be suitable for the generative model (e.g., images or text sequences).\n",
    "* Train the modified model for 20 epochs using the preprocessed data and appropriate hyperparameters.\n",
    "* Record the training and testing loss and accuracy (if applicable).\n",
    "* Analyze and compare the results with the original model to determine the impact of the different generative layer types.\n",
    "\n",
    "**For variants 28-30:** Experiment with different types of transformer layers.\n",
    "\n",
    "Instead of using standard convolutional or recurrent layers in your model from Task 2, try incorporating the following transformer layer types:\n",
    "* Encoder-Decoder Transformer (as in Machine Translation models)\n",
    "* Vision Transformer (ViT)\n",
    "* Perceiver (a unified transformer architecture for various data modalities)\n",
    "\n",
    "Train the modified model for 20 epochs and observe the impact on the training and testing loss and accuracy.\n",
    "* Import the required transformer layers from PyTorch or implement them manually.\n",
    "* Modify the existing model architecture by incorporating the transformer layer types at appropriate locations.\n",
    "* Preprocess the data to be suitable for the transformer layers (e.g., sequences of images or text).\n",
    "* Train the modified model for 20 epochs using the preprocessed data and appropriate hyperparameters.\n",
    "* Record the training and testing loss and accuracy.\n",
    "* Analyze and compare the results with the original model to determine the impact of the different transformer layer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THYGHbxyTfzM"
   },
   "source": [
    "It looks like the model might be overfitting, even when changing the number of hidden units.\n",
    "\n",
    "To fix this, we'd have to look at ways to prevent overfitting with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 6. Conducting experiments with the data</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Examine the impact of different data augmentation techniques on model robustness and accuracy.\n",
    "* Implement various data augmentation strategies using PyTorch's `torchvision.transforms` module. Common techniques include random rotations, horizontal flipping, vertical flipping, random crops, color jitters (adjusting brightness, contrast, and saturation), and adding noise.\n",
    "* Integrate these transformations into the data loading pipeline so that each image is randomly transformed during training but remains unchanged during validation and testing. This can be achieved by defining separate transform chains for training and testing datasets using `transforms.Compose`.\n",
    "* Train the model using the augmented dataset for a fixed number of epochs and compare the performance (accuracy and loss metrics) against the same model trained on non-augmented data. Analyze how different augmentation techniques influence the learning process, convergence behavior, and final model accuracy.\n",
    "\n",
    "**For variants 4-6:** Investigate the effects of different scaling and normalization techniques on model training dynamics and performance.\n",
    "* Experiment with various feature scaling and normalization methods, such as Min-Max scaling, Z-score normalization (standardization), and L2 normalization. These techniques adjust the range and distribution of feature values, which can significantly impact the convergence rate and stability of gradient descent algorithms.\n",
    "* Use PyTorch’s `transforms.Normalize` for image data to normalize pixel values based on the mean and standard deviation of the channels across the training set.\n",
    "* Train your model with each method and monitor how each affects the speed of convergence during training, the stability of training (variance in loss over epochs), and the accuracy on a validation set.\n",
    "* Determine the optimal pre-processing strategy for your dataset, which is crucial for models, especially those sensitive to the scale of input data like neural networks.\n",
    "\n",
    "**For variants 7-9:** Assess the impact of adding synthetic data to the training dataset on model performance.\n",
    "* Use generative techniques, such as a simple Generative Adversarial Network (GAN), to create additional synthetic training data. For the food classification task, generate images that resemble the existing data categories but with variations not present in the original dataset.\n",
    "* Integrate this synthetic data into the training set to see if it helps in improving model robustness and filling gaps in the data distribution, especially for underrepresented classes or features.\n",
    "* Train the model on a mix of real and synthetic data and compare the results with training solely on real data. Evaluate the model based on its accuracy, recall, and precision on a balanced validation set.\n",
    "* Analyze whether the synthetic data provides a meaningful diversity that aids the model during training, particularly checking for overfitting or underfitting scenarios.\n",
    "\n",
    "**For variants 10-12:** Explore the effects of training the model on different subsets of the data, focusing on diversity and representation.\n",
    "* Create multiple subsets of the original dataset based on different criteria, such as data recency, image quality, and the presence of specific features or classes. For example, train separate models on high-resolution images versus low-resolution images.\n",
    "* Train the model on these different subsets to determine how each subset's characteristics influence learning dynamics, model bias, and performance on a general test set.\n",
    "* This experiment can reveal dependencies on specific data characteristics and help in designing more effective data collection and preprocessing strategies to improve model performance and fairness.\n",
    "\n",
    "**For variants 13-15:** Test the model's robustness against noisy data and its ability to generalize under less ideal conditions.\n",
    "* Introduce various types of noise to the training data, such as Gaussian noise, salt-and-pepper noise, or occlusions (e.g., random black boxes on images). This can simulate real-world imperfections in data that the model might encounter.\n",
    "* Train the model with increasing levels of noise and monitor how its performance on validation data changes with noise intensity.\n",
    "* Test the model's resilience to data corruption and its ability to extract useful information from degraded inputs. It's particularly valuable in applications where data quality cannot be consistently guaranteed.\n",
    "* Evaluate not just overall accuracy, but also how sensitivity and specificity change with noise levels. This will help understand if the model is still reliable for certain classes more than others when the data quality deteriorates.\n",
    "\n",
    "**For variants 16-18:** Investigate the effects of class imbalance on model performance and explore strategies to mitigate its impact.\n",
    "* Simulate varying degrees of class imbalance by artificially reducing the representation of certain classes in the training data. For instance, reduce the number of images for one or more classes by 50%, 75%, and 90%.\n",
    "* Train the model on these imbalanced datasets and observe how the imbalance affects model accuracy, precision, recall, and F1 score, particularly for the underrepresented classes.\n",
    "* Implement and compare various techniques to handle imbalance, such as oversampling the minority class, undersampling the majority class, or using class-weighted or balanced loss functions like weighted cross-entropy.\n",
    "\n",
    "**For variants 19-21:** Examine the impact of augmenting training data with external datasets on model generalization.\n",
    "* Augment the original Food101 dataset with additional data from similar but external sources, such as images from open datasets like ImageNet or specialized food datasets that might include dishes not represented in the original dataset.\n",
    "* Ensure that the external data is preprocessed and formatted to match the original dataset in terms of image size, scaling, and color channels.\n",
    "* Train the model on this augmented dataset and evaluate changes in its ability to generalize across a broader range of food categories, particularly looking at performance metrics on a separate, diverse test set.\n",
    "\n",
    "**For variants 22-24:** Determine how variations in image quality affect model training and performance.\n",
    "* Create multiple versions of your dataset with different levels of image quality. This could involve altering image resolution, adding noise, and varying lighting conditions.\n",
    "* Use PyTorch's `torchvision.transforms` to simulate these quality variations dynamically during data loading, which allows for a scalable approach to manipulating image properties.\n",
    "* Train your model on these datasets and monitor how changes in image quality impact the training speed, convergence behavior, and accuracy on a validation set.\n",
    "\n",
    "**For variants 25-27:** Explore the effect of the order in which data is presented to the model during training.\n",
    "* Implement two data feeding strategies: sequential, where the dataset is sorted based on certain criteria like class labels or image complexity before training, and random, where data order is shuffled for each epoch.\n",
    "* Utilize PyTorch's DataLoader with parameters to control shuffling and batch sampling to facilitate these strategies.\n",
    "* Evaluate how each strategy affects the learning dynamics and model performance, particularly focusing on whether a particular ordering leads to faster learning or better generalization.\n",
    "* Test the hypothesis that certain sequences of data presentation might prime the model more effectively, potentially leading to better or faster learning outcomes.\n",
    "\n",
    "**For variants 28-30:** Assess the impact of advanced feature engineering and data transformation techniques on model performance.\n",
    "* Beyond basic image preprocessing, apply advanced feature engineering techniques such as PCA for dimensionality reduction, edge detection filters, or Fourier transforms to transform the input data.\n",
    "* Integrate these transformations into the PyTorch data pipeline using custom `torchvision.transforms` functions or by modifying the dataset class to preprocess data before training.\n",
    "* Train the model on this transformed dataset and compare the results with training on the original data. Look for changes in accuracy, training efficiency, and model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tWfa7Y0yCkX"
   },
   "outputs": [],
   "source": [
    "# Download 20% data for Pizza/Steak/Sushi from GitHub\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi_20_percent\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it... \n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# Download pizza, steak, sushi data\n",
    "with open(data_path / \"pizza_steak_sushi_20_percent.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/radiukpavlo/applied-math-packages/blob/main/data/pizza_steak_sushi_20_percent.zip\")\n",
    "    print(\"Downloading pizza, steak, sushi 20% data...\")\n",
    "    f.write(request.content)\n",
    "\n",
    "# Unzip pizza, steak, sushi data\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi_20_percent.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping pizza, steak, sushi 20% data...\") \n",
    "    zip_ref.extractall(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrFK2ScnVg4q"
   },
   "outputs": [],
   "source": [
    "# See how many images we have\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhlWd-z-Vk22"
   },
   "source": [
    "Excellent, we now have double the training and testing images... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNzXRfO1Tt1Q"
   },
   "outputs": [],
   "source": [
    "# Create the train and test paths\n",
    "train_data_20_percent_path = image_path / \"train\"\n",
    "test_data_20_percent_path = image_path / \"test\"\n",
    "\n",
    "train_data_20_percent_path, test_data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1_xU3FQUPkN"
   },
   "outputs": [],
   "source": [
    "# Turn the 20 percent datapaths into Datasets and DataLoaders\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "  transforms.Resize((64, 64)),                                     \n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "\n",
    "\n",
    "# Create dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuJ9YpRCVXRm"
   },
   "outputs": [],
   "source": [
    "# Train a model with increased amount of data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1em;\"> Task 7. Making predictions</span>\n",
    "\n",
    "[Go back to the content](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For variants 1-3:** Utilize an ensemble of models to improve prediction accuracy and robustness.\n",
    "* Instead of relying on a single model, train several models (potentially with different architectures or hyperparameters) on the same dataset. For instance, train variations of your main model with different layers, activation functions, or trained with different data augmentation strategies.\n",
    "* Use a voting system among the models in the ensemble. Each model provides a prediction, and the final output is decided by majority vote. Alternatively, use a weighted approach where models that showed higher accuracy on the validation set have more influence on the final prediction.\n",
    "* Evaluate the ensemble's performance by testing it on a new set of images that were not part of the training or validation datasets. Compare its accuracy and reliability against the single-model approach.\n",
    "\n",
    "**For variants 4-6:** Implement a system for making real-time predictions as new image data is streamed to the model.\n",
    "* Set up a simulated streaming environment where new images (e.g., from a live camera feed at a restaurant) are continuously fed into the model. This could involve integrating PyTorch with a web application or a database that periodically updates with new images.\n",
    "* Use PyTorch's DataLoader to handle incoming data streams effectively. Ensure that the images are preprocessed and normalized in the same way as the training data before they are passed to the model for prediction.\n",
    "* Implement performance metrics to evaluate the model's prediction speed and accuracy in real-time. This might include measuring the latency between receiving an image and outputting a prediction, as well as tracking the model's accuracy over time as more data is processed.\n",
    "\n",
    "**For variants 7-9:** Adapt the trained model to make predictions on images from a different but related domain.\n",
    "* Suppose the original model is trained on high-quality images of food. To adapt this model to work with lower-quality images (e.g., images from social media), implement techniques like fine-tuning or domain adaptation.\n",
    "* Explore advanced domain adaptation techniques that can bridge the gap between different data distributions, such as feature-level adaptation, where the internal representations of images from both domains are encouraged to be similar.\n",
    "* Evaluate the adapted model on a validation set from the new domain to measure how well it has adjusted to the new types of images. This evaluation helps in understanding the effectiveness of the adaptation process and whether further tuning or more sophisticated adaptation techniques are required.\n",
    "\n",
    "**For variants 10-12:** Implement methods to estimate and communicate the uncertainty in the model's predictions.\n",
    "* Incorporate Bayesian methods or Monte Carlo dropout into the prediction process to estimate the confidence of the model in its output. This involves modifying the model to include dropout layers during both training and testing, and running multiple forward passes with dropout enabled to get a distribution of outputs.\n",
    "* Use these distributions to calculate confidence intervals or probabilities that reflect how certain the model is about its predictions. For example, a high variance in the outputs could indicate low confidence, prompting the system to request additional data or human verification.\n",
    "* Integrate this uncertainty estimation into the prediction output, providing end-users or downstream systems with not just a classification result, but also a measure of reliability. This can be crucial in applications where decisions based on these predictions carry significant consequences.\n",
    "\n",
    "**For variants 13-15:** Test the model's robustness to adversarial examples and implement strategies to mitigate potential vulnerabilities.\n",
    "* Generate adversarial examples using techniques like FGSM (Fast Gradient Sign Method) or more sophisticated methods available in libraries like Foolbox. These examples are designed to fool the model into making incorrect predictions, highlighting potential vulnerabilities in its training.\n",
    "* Test the model's ability to correctly classify these adversarially perturbed images. Analyze the types of errors it makes, which could indicate specific weaknesses in the model’s understanding of the input data.\n",
    "* Implement defensive strategies against such adversarial attacks. These could include adversarial training, where the model is trained on a mix of normal and adversarial examples to improve its robustness, or using model architectures that are inherently more robust to adversarial perturbations.\n",
    "* Evaluate the model’s performance after incorporating these defenses, particularly focusing on maintaining or improving accuracy on both standard and adversarially modified test sets.\n",
    "\n",
    "**For variants 16-18:** Use a fusion of different models to enhance prediction accuracy and reliability.\n",
    "* Develop a system where multiple models trained independently (e.g., using different architectures or subsets of data) contribute to a final decision. This could involve simple techniques like model averaging or more complex strategies like stacking, where a new model learns how to best combine the outputs of the individual models.\n",
    "* Implement this by training several models on the same dataset, or different segments of it, ensuring diversity in the learning process. Each model might specialize in different features of the data, improving overall predictive performance when combined.\n",
    "* For prediction, input the same image into all models and aggregate their predictions. Depending on the approach, this could mean taking the majority vote (voting ensemble) or weighting predictions based on the confidence or historical accuracy of each model (weighted average).\n",
    "* Evaluate this system by comparing its accuracy, precision, and recall against those of individual models. Check if the ensemble reduces overfitting and provides more stable and reliable predictions across a diverse set of test images.\n",
    "\n",
    "**For variants 19-21:** Implement attention mechanisms to improve model predictions by focusing on relevant parts of the image.\n",
    "* Modify the existing model architecture to include attention layers, which help the model focus on areas of the image that are more relevant for making a prediction. This is particularly useful for complex images where not all parts of the image are equally informative.\n",
    "* Techniques like the Self-Attention mechanism or Transformer models can be incorporated into traditional CNN architectures to enhance their ability to localize and emphasize important features without the need for additional input or segmentation maps.\n",
    "* Train the modified model on the dataset, ensuring that the attention mechanism is properly integrated and contributing positively to the model's learning process.\n",
    "* For prediction, visualize the attention maps generated by the model to understand which parts of the image are being focused on. This not only aids in prediction but also provides insights into the model's decision-making process, which can be crucial for applications requiring high levels of trust and interpretability.\n",
    "\n",
    "**For variants 22-24:** Implement dynamic thresholding to handle uncertain predictions more effectively.\n",
    "* Instead of using a fixed threshold (e.g., 0.5 in binary classification) to decide the class of an image, use a dynamic threshold that adjusts based on the confidence of the model’s predictions or the specific requirements of the application.\n",
    "* Develop a method to calculate the threshold dynamically, perhaps based on the distribution of prediction probabilities seen in the validation data. For example, the threshold could be set at the 90th percentile of confidence scores for predicted classes.\n",
    "* Integrate this dynamic thresholding into the prediction pipeline, applying it to decide the final class labels for new images.\n",
    "* Evaluate how this approach affects the number of uncertain predictions, the overall accuracy, and the balance between precision and recall.\n",
    "\n",
    "**For variants 25-27:** Establish a real-time feedback system for continuously improving the model based on new predictions and user feedback.\n",
    "* Set up a mechanism where users can provide immediate feedback on the model’s predictions (e.g., correct/incorrect). Use this feedback to dynamically adjust the model, either through direct model updates or by periodically retraining the model with the new data.\n",
    "* Implement a lightweight version of the model for quick updates and integrate an online learning protocol where the model can update its parameters in real-time based on user feedback.\n",
    "* Monitor and analyze the impact of this continuous learning approach on model performance over time, especially how quickly the model adapts to new patterns or corrections in its predictions.\n",
    "\n",
    "**For variants 28-30:** Quantify the uncertainty in the model’s predictions to better assess risks and confidences.\n",
    "* Convert the existing model into a Bayesian framework where instead of having fixed weights, the model maintains a distribution over possible weights, reflecting uncertainty in its predictions.\n",
    "* Use techniques like Monte Carlo Dropout to approximate Bayesian inference, performing multiple forward passes through the network with dropout enabled at inference time to generate a distribution of outputs for each input image.\n",
    "* Analyze the variance and other statistical properties of the outputs to estimate the uncertainty associated with each prediction.\n",
    "* Implement this uncertainty quantification as part of the prediction output, providing end-users with not just a categorical result but also a measure of confidence in that result.\n",
    "* Evaluate how effectively this Bayesian approach mitigates overconfident errors, improves decision-making processes, and aligns with the operational needs of applications where understanding uncertainty is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1X-33t0vT20"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO06uacGGDzw0TkyZ8mqgSU",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "04_pytorch_custom_datasets_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
